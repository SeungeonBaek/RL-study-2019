ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content/value_function_approximation.html
/*ì´ì›…ì›ë‹˜ Git*/

# Off-Policy Control

## Importance Sampling

  ì§€ê¸ˆê¹Œì§€ Monte-Carlo Controlê³¼ Temporal-Difference Controlì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ì€ ë‘ ë°©ë²• ëª¨ë‘ on-policy reinforcement learningì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìƒˆë¡œìš´ ê°œë…ì„ í•˜ë‚˜ ì•Œê³  ê°ˆ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

## 1. On-Policy vs Off-Policy

  ë‹¤ì‹œ Sarsaì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ Sarsaì—ì„œëŠ”

  - Choose A  from S  using Policy derived from Q
  - Choose A' from S' using Policy derived from Q

  actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë‘ ë¶€ë¶„ì´ ë§ìŠµë‹ˆë‹¤. ë³´ë©´ ë‘˜ ë‹¤ ê³µí†µì ìœ¼ë¡œ "using Policy derived from Q"ê°€ ì‚¬ìš©ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > An on-policy TD control algorithm
      Initialize Q(s,a), s âˆˆ S, ğ‘ âˆˆ ğ´, arbitrarily, and Q(terminal-state,âˆ™) = 0
      Repeat (for each episode):
        Initialize S
        Choose A from S using policy derived from Q(e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
        Repeat (for each step of episode):
          Take action A, observe R, S'
          Choose A' from S' using policy derived from Q(e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
          Q(S,A) <- Q(S,A) + ğ›¼ * (R + ğ›¾ * Q(S',A') - Q(S,A))
          S <- S'; A <- A';
      until S is terminal

  ì´ ë§ì€ ì¦‰ í˜„ì¬ policyë¡œ ì›€ì§ì´ë©´ì„œ ê·¸ policyë¥¼ í‰ê°€í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë³´ë©´ ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ì›€ì§ì¸ Q functionìœ¼ë¡œ í˜„ì¬ì˜ Q functionì„ updateí•©ë‹ˆë‹¤. ë”°ë¼ì„œ í˜„ì¬ policyìœ„ì—ì„œ control(prediction + policy improvement)ì„ í•˜ê¸° ë•Œë¬¸ì— on-policyë¼ê³  ìƒê°í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.

  í•˜ì§€ë§Œ on-policyì—ëŠ” í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë°”ë¡œ íƒí—˜ì˜ ë¬¸ì œì…ë‹ˆë‹¤. í˜„ì¬ ì•Œê³ ìˆëŠ” ì •ë³´ì— ëŒ€í•´ greedyë¡œ policyë¥¼ ì •í•´ë²„ë¦¬ë©´ optimalì— ê°€ì§€ ëª» í•  í™•ë¥ ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì—ì´ì „íŠ¸ëŠ” í•­ìƒ íƒí—˜ì´ í•„ìš”í•©ë‹ˆë‹¤.

  ë”°ë¼ì„œ on-policyì²˜ëŸ¼ ì›€ì§ì´ëŠ” policyì™€ í•™ìŠµí•˜ëŠ” policyê°€ ê°™ì€ ê²ƒì´ ì•„ë‹ˆê³  ì´ ë‘ê°œì˜ policyë¥¼ ë¶„ë¦¬ì‹œí‚¨ ê²ƒì´ off-policyì…ë‹ˆë‹¤. SilverëŠ” ìˆ˜ì—…ì—ì„œ Off-policyë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•©ë‹ˆë‹¤.

    > Off-policy definition
      Evaluate target policy ğœ‹(a|s) to compute v_ğœ‹(s) or q_ğœ‹(s,a)
      While following behaviour policy ğœ‡(a|s)
        {S_1, A_1, R_2, ... , S_T} ~ ğœ‡
      Why is this important?
        Learn from observing humans or other agents
        Re-use experience generated from old policies ğœ‹_1, ğœ‹_2, ..., ğœ‹_(t-1)
        Learn about optimal policy while following exploratory policy
        Learn about multiple policies while following one policy

  Off-policyëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.
  - ë‹¤ë¥¸ agentë‚˜ ì‚¬ëŒì„ ê´€ì°°í•˜ê³  ê·¸ë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
  - ì´ì „ì˜ policyë“¤ì„ ì¬í™œìš©í•˜ì—¬ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
  - íƒí—˜ì„ ê³„ì† í•˜ë©´ì„œë„ optimalí•œ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. (Q-learning)
  - í•˜ë‚˜ì˜ policyë¥¼ ë”°ë¥´ë©´ì„œ ì—¬ëŸ¬ê°œì˜ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

## 2. Importance Sampling

  ìœ„ì—ì„œ Off-policy learningì´ ì–´ë–¤ ê²ƒì¸ì§€ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ policyë¡œ ë¶€í„° í˜„ì¬ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê·¼ê±°ê°€ ë¬´ì—‡ì¼ê¹Œìš”? "importance sampling"ì´ë¼ëŠ” ê°œë…ì€ ì›ë˜ í†µê³„í•™ì—ì„œ ì‚¬ìš©í•˜ë˜ ê°œë…ìœ¼ë¡œ ì•„ë˜ì™€ íŠ¹ì •í•œ ë¶„í¬ì˜ ê°’ë“¤ì„ ì¶”ì •í•˜ëŠ” ê¸°ë²•ì¤‘ì˜ í•˜ë‚˜ì…ë‹ˆë‹¤.

    > Importance sampling
      In statics, importance sampling is a general technique for estimating properties of a particular distribution,
      while only having samples generated from a different distribution than the distribution of interset

  ì–´ë–¤ ê°’ì„ ì¶”ì •í•˜ëŠ”ë° ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì€ ê·¸ëƒ¥ randomí•˜ê²Œ ì°ì–´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¯¸ ì €í¬ê°€ ë°°ì› ë‹¤ì‹œí”¼ ì´ëŸ¬í•œ processë£Œ í‘œí˜„í•˜ëŠ” ë§ì€ "monte-carlo"ë¡œì„œ Monte-Carlo estimationì´ë¼ê³  í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ê²Œ íƒìƒ‰í•˜ê¸°ë„ í•˜ê³  ì–´ë– í•œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì•Œì•„ì„œ ê·¸ ìœ„ì£¼ë¡œ íƒìƒ‰ì„ í•˜ë©´ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê°’ì„ ì¶”ì •í•  ìˆ˜ ìˆê³  ê·¸ëŸ¬í•œ ì•„ì´ë””ì–´ê°€ ë°”ë¡œ "Importance Sampling"ì…ë‹ˆë‹¤.

  importance samplingì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. pì™€ që¼ëŠ” distributionì´ ìˆì„ ë•Œ që¼ëŠ” distributionì—ì„œ ì‹¤ì œë¡œ ì§„í–‰ì„ í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  pë¡œ ì¶”ì •í•˜ëŠ” ê²ƒì²˜ëŸ¼ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œë„ policyê°€ ë‹¤ë¥´ë©´ stateì˜ distributionì€ ë‹¬ë¼ì§€ê²Œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ distributionì„ í†µí•´ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê°œë…ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ë‹¤ë¥¸ policyë¥¼ í†µí•´ì„œ ì–»ì–´ì§„ sampleì„ ì´ìš©í•˜ì—¬ Q ê°’ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¼ì¢…ì˜ trickì´ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

  ìœ„ì˜ ë‚´ìš©ì„ David Silverêµìˆ˜ë‹˜ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ëª…í•©ë‹ˆë‹¤. f(X)ë¼ëŠ” í•¨ìˆ˜ë¥¼ value functionì´ë¼ê³  ìƒê°í•˜ê³  ê°•í™”í•™ìŠµì—ì„œëŠ” ì´ value function = expected future rewardë¥¼ ê³„ì† ì¶”ì •í•´ ë‚˜ê°€ëŠ”ë° P(x)ë¼ëŠ” í˜„ì¬ policyë¡œ í˜•ì„±ëœ distributionìœ¼ë¡œë¶€í„° í•™ìŠµì„ í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ Që¼ëŠ” distributionì„ ë”°ë¥´ë©´ì„œë„ ë˜‘ê°™ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ë° ë‹¨, ì•„ë˜ì™€ ê°™ì´ ê°„ë‹¨íˆ ì‹ì„ ë³€í˜•ì‹œì¼œì£¼ë©´ ë©ë‹ˆë‹¤. Q(x)ë¥¼ ê³±í•´ì£¼ê³  ë‚˜ëˆ ì£¼ë©´ ë©ë‹ˆë‹¤.

    > Importance Sampling
      Estimate the expectation of a different distribution
        E_(X~P)[f(x)] = Î£ P(X) * f(X)
                      = Î£ Q(X) * (P(X)/Q(X)) * f(X)
                      = E_(X~Q)[ (P(X)/Q(X)) * f(x)]

  Off-Policy ë˜í•œ MCì™€ TDë¡œ ê°ˆë¦½ë‹ˆë‹¤. Off-Policy MCëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  returnì„ ê³„ì‚°í•  ë•Œ ì•„ë˜ì™€ ê°™ì´ ì‹ì„ ë³€í˜•ì‹œì¼œì¤ë‹ˆë‹¤. ê° ìŠ¤í…ì— rewardë¥¼ ë°›ê²Œ ëœ ê²ƒì€ ğœ‡ë¼ëŠ” policyë¥¼ ë”°ë¼ì„œ ì–»ì—ˆë˜ ê²ƒì´ë¯€ë¡œ ë§¤ stepë§ˆë‹¤ ğœ‹/ğœ‡ë¥¼ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Monte-Carloì— Off-policyë¥¼ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì€ ê·¸ë¦¬ ì¢‹ì€ ì•„ì´ë””ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤.

    > Importance Sampling for Off-Policy Monte-Carlo method
      Use returns generated from ğœ‡ to evaluate ğœ‹
      Weight return G_t according to similarity between policies
      Multiply importance sampling corrections along whole episode
                    ğœ‹(A_t|S_t) * ğœ‹(A_(t+1)|S_(t+1)) ... ğœ‹(A_T|S_T)
       G^(ğœ‹/ğœ‡)_t = ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ * G_t
                    ğœ‡(A_t|S_t) * ğœ‡(A_(t+1)|S_(t+1)) ... ğœ‡(A_T|S_T)

      Update value towards corrected return
        V(S_t) <- V(S_t) + ğ›¼ * (G^(ğœ‹/ğœ‡)_t - V(S_t))

  Off-Policy TDì—ì„œëŠ” MC ë•Œì™€ëŠ” ë‹¬ë¦¬ Importance Samplingì„ 1-stepë§Œ ì§„í–‰í•˜ë©´ ë©ë‹ˆë‹¤.
  MCë•Œì™€ ë¹„êµí•˜ë©´ Varianceê°€ ë‚®ì•„ì§€ê¸°ëŠ” í–ˆì§€ë§Œ ì—¬ì „íˆ ì›ë˜ TDì— ë¹„í•˜ë©´ Importance samplingë•Œë¬¸ì— ë†’ì€ varianceë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. Off-policy learningì„ í•  ë–„ Importance samplingë§ê³  ë‹¤ë¥¸ ë°©ë²•ì„ ìƒê°í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ ì—¬ê¸°ì„œ, ìœ ëª…í•œ Q learning ì•Œê³ ë¦¬ì¦˜ì´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.

    > Importance Sampling for Off-Policy Temporal Difference method
      Use TD targets generated from ğœ‡ to evaluate ğœ‹
      Weight TD target R + ğ›¾ * V(S') by importance sampling
      Only need a single importance sampling correction
                                ( (ğœ‹(A_t|S_t)                                       )
        V(S_t) <- V(S_t) +  ğ›¼ * ( ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ * (R_(t+1) + ğ›¾ * V(S_(t+1) - V(S_t)) ))
                                ( ğœ‡(A_t|S_t))                                       )

***

## Q Lenarning

## 1. Q-Learning

  Off-Policy Learning ì•Œê³ ë¦¬ì¦˜ ì¤‘ì—ì„œ Off-policy MCì™€ Off-policy TDê°€ ìˆì§€ë§Œ Importance samplingë¬¸ì œ ë•Œë¬¸ì— ìƒˆë¡œìš´ ë°©ë²•ì´ í•„ìš”í•˜ë‹¤ê³  ë§í–ˆì—ˆìŠµë‹ˆë‹¤. Off-Policy learningì„ í•˜ëŠ”ë° ê°€ì¥ ì¢‹ì€ ì•Œê³ ë¦¬ì¦˜ì€ Q Learningì…ë‹ˆë‹¤. ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

  í˜„ì¬ state Sì—ì„œ actionì„ ì„ íƒí•˜ëŠ” ê²ƒì€ behaviour policyë¥¼ ë”°ë¼ì„œ ì„ íƒí•©ë‹ˆë‹¤. TDì—ì„œ updateí•  ë•ŒëŠ” one-stepì„ bootstrapí•˜ëŠ”ë° ì´ ë•Œ ë‹¤ìŒ stateì˜ actionì„ ì„ íƒí•˜ëŠ” ë°ëŠ” behaviour policyì™€ëŠ” ë‹¤ë¥¸ policy(alternative policy)ë¥¼ ì‚¬ìš©í•˜ë©´ Importance Samplingì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

  ì´ì „ì˜ Off-Policyì—ì„œëŠ” Value functionì„ ì‚¬ìš©í–ˆì—ˆëŠ”ë° ì—¬ê¸°ì„œëŠ” action-value functionì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë‹¤ìŒ actionê¹Œì§€ ì„ íƒì„ í•´ì•¼í•˜ëŠ”ë° ê·¸ ë•Œ ë‹¤ë¥¸ policyë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
    > Q-Learning
      We now consider off-policy learning of action-values Q(s,a)
      No importance sampling is required
      Next action is chosen using behaviour policy A_(t+1) ~ ğœ‡(âˆ™|S_t)
      But we consider alternative successor action A' ~ ğœ‹(âˆ™|S_t)
      And update Q(S_t, A_t) towards value of alternative action
        Q(S_t, A_t) <- Q(S_t, A_t) + ğ›¼ * (R_(t+1) + ğ›¾ * Q(S_(t+1), A') - Q(S_t, A_t))

***

## 2. Off-Policy control with Q-Learning

  ì´ Q learning ì•Œê³ ë¦¬ì¦˜ ì¤‘ì—ì„œ ê°€ì¥ ìœ ëª…í•œ ê²ƒì´ ì•„ë˜ì…ë‹ˆë‹¤.
  - Behaviour policyë¡œëŠ” Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ w.r.t. Q(s,a)
  - Target policy(alternative policy)ë¡œëŠ” ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ w.r.t Q(s,a)

  ìœ„ì™€ ê°™ì´ behavior policyì™€ target policyë¥¼ íƒí•œ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì´ì „ì— Off-policyì˜ ì¥ì ì´ exploratory policyë¥¼ ë”°ë¥´ë©´ì„œë„ optimal policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ê³  í–ˆëŠ”ë° ê·¸ê²Œ ë°”ë¡œ ì´ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. greedyí•œ policyë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ìˆ˜ë ´ì„ ë¹¨ë¦¬ í•˜ëŠ”ë° ì¶©ë¶„íˆ íƒí—˜ì„ í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— localì— ë¹ ì§€ê¸°ê°€ ì‰½ìŠµë‹ˆë‹¤.

  ê·¸ë ‡ê¸° ë•Œë¬¸ì—, íƒí—˜ì„ ìœ„í•´ Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ policyë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ìˆ˜ë ´ì†ë„ê°€ ëŠë ¤ì ¸ì„œ í•™ìŠµì†ë„ê°€ ëŠë ¤ì§€ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ Ïµ(epsilon)ì„ ì‹œê°„ì— ë”°ë¼ decayê¸°í‚¤ëŠ” ë°©ë²•ê³¼ ì•„ë˜ì™€ ê°™ì´ Q learningì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

    > Off-Policy Control with Q-Learning
      We now allow both behaviour and target policies to improve
      The target policy ğœ‹ is greedy w.r.t. Q(s,a)
        ğœ‹(S_(t+1)) = {a') argmax{ Q(S_(t+1),a') }

      The behaviour policy mu is e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ w.r.t Q(s,a)
      The Q-learning target then simplifies:
         R_(t+1) + ğ›¾ * Q(S_(t+1), A')
       = R_(t+1) + ğ›¾ * Q(S_(t+1), {a'}argmax{ Q(S_(t+1),a') })
       = R_(t+1) + {a'}max {ğ›¾ * Q(S_(t+1), a')}

  ì•„ë˜ ì•Œê³ ë¦¬ì¦˜ì€ ì‚¬ì‹¤ Bellman Optimality Equationì„ ì‚¬ìš©í•œ Value iterationì„ ì´ìš©í•œ ê²ƒì…ë‹ˆë‹¤.
  Optimal valuefunctionë¼ë¦¬ì˜ ê´€ê³„ì‹ì„ ì´ìš©í•´ì„œ updateë¥¼ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë ‡ê²Œ updateë¥¼ í•  ë•Œ optimal action-value functionì— ìˆ˜ë ´í•˜ê²Œ ë©ë‹ˆë‹¤.

    > Q-Learning control Algorithm
      ì‹      : Q(S,A) <- Q(S,A) + ğ›¼ * (R_(t+1) + ğ›¾ * {a'}max {Q(S_(t+1), a')} - Q(S,A))
      Theorem : Q-learning control converges to the optimal action-value function, Q(s,a) -> q*(s,a)

      Pseudo code
        Initialize Q(s,a), s S, a A, arbitrarily, and Q(terminal-state,âˆ™) = 0
        Repeat (for each episode):
          Initialize S
          Repeat (for each step of episode):
            Choose A from S using policy derived from Q (e.g., Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
            Take action A, observe R, S'
            Q(S,A) <- Q(S,A) + ğ›¼ * [R + ğ›¾ * {a} max(Q(S',a)) - Q(S<A)]
            S <- S';
          until S is terminal

***

## 3. Sarsa vs Q-learning

  ì´ë ‡ê²Œ Q-learningì— ëŒ€í•´ì„œ ì‚´í´ë´ë„ ì§ê´€ì ìœ¼ë¡œ Q-learningì´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ ì˜ ì™€ë‹¿ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. Q-learningì„ ì´í•´í•˜ë ¤ë©´ SARSAì™€ ë¹„êµí•´ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. Suttonì´ ì´ ë‘ê°€ì§€ë¥¼ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì˜ˆì œë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. "Cliff Wlking"ì´ë¼ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

  ì´ ì˜ˆì œì—ì„œ ëª©í‘œëŠ” Së¼ëŠ” start stateì—ì„œ ì‹œì‘í•´ì„œ Goalê¹Œì§€ ê°€ëŠ” optimal pathë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¼ì— ë‚˜ì™€ìˆëŠ” Cliffì— ë¹ ì ¸ë²„ë¦¬ë©´ -100ì˜ rewardë¥¼ ë°›ê³  time-stepë§ˆë‹¤ rewardë¥¼ -1ì”© ë°›ëŠ” ë¬¸ì œë¼ì„œ ì ˆë²½ì— ë¹ ì§€ì§€ ì•Šê³  goalê¹Œì§€ ê°€ëŠ¥í•œ í•œ ë¹ ë¥´ê²Œ ë„ì°©í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

  ëˆˆìœ¼ë¡œ ë”± ë´ë„ ê·¸ë¦¼ì— ìˆëŠ” optimal pathê°€ ë‹µì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. SARSAì™€ Q-learning ëª¨ë‘ ë‹¤ $$\epsilon$$-greedyí•œ policyë¡œ ì›€ì§ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë”ëŸ¬ëŠ” Cliffì— ë¹ ì ¸ë²„ë¦¬ê¸°ë„ í•©ë‹ˆë‹¤. ì°¨ì´ëŠ” SARSAëŠ” on-policyë¼ì„œ ê·¸ë ‡ê²Œ Cliffì— ë¹ ì ¸ë²„ë¦¬ëŠ” ê²°ê³¼ë¡œ ì¸í•´ ê·¸ ì£¼ë³€ì˜ ìƒíƒœë“¤ì˜ valueë¥¼ ë‚®ë‹¤ê³  íŒë‹¨í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ Q-learingì˜ ê²½ìš°ì—ëŠ” ë¹„ë¡ $$\epsilon$$-greedyë¡œ ì¸í•´ Cliffì— ë¹ ì ¸ë²„ë¦´ì§€ë¼ë„ ìì‹ ì´ ì§ì ‘ ì²´í—˜í•œ ê·¸ ê²°ê³¼ê°€ ì•„ë‹ˆë¼ greedyí•œ policyë¡œ ì¸í•œ Q functionì„ ì´ìš©í•´ì„œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Cliff ê·¼ì²˜ì˜ ê¸¸ë„ Q-learningì€ optimal pathë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆì–´ì„œ ì´ ë¬¸ì œì˜ ê²½ìš° SARSAë³´ë‹¤ëŠ” Q-learningì´ ì í•©í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  SARSAì—ì„œ íƒí—˜ì„ ìœ„í•´ì„œ $$\epsilon$$-greedyë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ ê²°êµ­ì€ ê·¸ë¡œì¸í•´ì„œ ì •ì‘ ì—ì´ì „íŠ¸ê°€ optimalë¡œ ìˆ˜ë ´í•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒë“¤ì´ ë°œìƒí•œ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ Q-learningì˜ ë“±ì¥ ì´í›„ë¡œëŠ” ë§ì€ ë¬¸ì œì—ì„œ Q-learningì´ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ì—ˆê¸° ë•Œë¬¸ì— ê°•í™”í•™ìŠµì—ì„œ Q-learningì€ ê¸°ë³¸ì ì¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìë¦¬ë¥¼ ì¡ê²Œ ë©ë‹ˆë‹¤.








  asd
