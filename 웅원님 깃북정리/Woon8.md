/*ì´ì›…ì›ë‹˜ Git*/

# Q-Learning

## Importance Sampling

  ì§€ê¸ˆê¹Œì§€ Monte-Carlo Controlê³¼ Temporal-Difference Controlì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ì€ ë‘ ë°©ë²• ëª¨ë‘ on-policy reinforcement learningì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìƒˆë¡œìš´ ê°œë…ì„ í•˜ë‚˜ ì•Œê³  ê°ˆ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

## 1. On-Policy vs Off-Policy

  ë‹¤ì‹œ Sarsaì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ Sarsaì—ì„œëŠ”

  - Choose A  from S  using Policy derived from Q
  - Choose A' from S' using Policy derived from Q

  actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë‘ ë¶€ë¶„ì´ ë§ìŠµë‹ˆë‹¤. ë³´ë©´ ë‘˜ ë‹¤ ê³µí†µì ìœ¼ë¡œ "using Policy derived from Q"ê°€ ì‚¬ìš©ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > An on-policy TD control algorithm
      Initialize Q(s,a), s âˆˆ S, ğ‘ âˆˆ ğ´, arbitrarily, and Q(terminal-state,âˆ™) = 0
      Repeat (for each episode):
        Initialize S
        Choose A from S using policy derived from Q(e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
        Repeat (for each step of episode):
          Take action A, observe R, S'
          Choose A' from S' using policy derived from Q(e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
          Q(S,A) <- Q(S,A) + ğ›¼ * (R + ğ›¾ * Q(S',A') - Q(S,A))
          S <- S'; A <- A';
      until S is terminal

  ì´ ë§ì€ ì¦‰ í˜„ì¬ policyë¡œ ì›€ì§ì´ë©´ì„œ ê·¸ policyë¥¼ í‰ê°€í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë³´ë©´ ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ì›€ì§ì¸ Q functionìœ¼ë¡œ í˜„ì¬ì˜ Q functionì„ updateí•©ë‹ˆë‹¤. ë”°ë¼ì„œ í˜„ì¬ policyìœ„ì—ì„œ control(prediction + policy improvement)ì„ í•˜ê¸° ë•Œë¬¸ì— on-policyë¼ê³  ìƒê°í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.

  í•˜ì§€ë§Œ on-policyì—ëŠ” í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë°”ë¡œ íƒí—˜ì˜ ë¬¸ì œì…ë‹ˆë‹¤. í˜„ì¬ ì•Œê³ ìˆëŠ” ì •ë³´ì— ëŒ€í•´ greedyë¡œ policyë¥¼ ì •í•´ë²„ë¦¬ë©´ optimalì— ê°€ì§€ ëª» í•  í™•ë¥ ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì—ì´ì „íŠ¸ëŠ” í•­ìƒ íƒí—˜ì´ í•„ìš”í•©ë‹ˆë‹¤.

  ë”°ë¼ì„œ on-policyì²˜ëŸ¼ ì›€ì§ì´ëŠ” policyì™€ í•™ìŠµí•˜ëŠ” policyê°€ ê°™ì€ ê²ƒì´ ì•„ë‹ˆê³  ì´ ë‘ê°œì˜ policyë¥¼ ë¶„ë¦¬ì‹œí‚¨ ê²ƒì´ off-policyì…ë‹ˆë‹¤. SilverëŠ” ìˆ˜ì—…ì—ì„œ Off-policyë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•©ë‹ˆë‹¤.

    > Off-policy definition
      Evaluate target policy ğœ‹(a|s) to compute v_ğœ‹(s) or q_ğœ‹(s,a)
      While following behaviour policy ğœ‡(a|s)
        {S_1, A_1, R_2, ... , S_T} ~ ğœ‡
      Why is this important?
        Learn from observing humans or other agents
        Re-use experience generated from old policies ğœ‹_1, ğœ‹_2, ..., ğœ‹_(t-1)
        Learn about optimal policy while following exploratory policy
        Learn about multiple policies while following one policy

  Off-policyëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.
  - ë‹¤ë¥¸ agentë‚˜ ì‚¬ëŒì„ ê´€ì°°í•˜ê³  ê·¸ë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
  - ì´ì „ì˜ policyë“¤ì„ ì¬í™œìš©í•˜ì—¬ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
  - íƒí—˜ì„ ê³„ì† í•˜ë©´ì„œë„ optimalí•œ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. (Q-learning)
  - í•˜ë‚˜ì˜ policyë¥¼ ë”°ë¥´ë©´ì„œ ì—¬ëŸ¬ê°œì˜ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

## 2. Importance Sampling

  ìœ„ì—ì„œ Off-policy learningì´ ì–´ë–¤ ê²ƒì¸ì§€ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ policyë¡œ ë¶€í„° í˜„ì¬ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê·¼ê±°ê°€ ë¬´ì—‡ì¼ê¹Œìš”? "importance sampling"ì´ë¼ëŠ” ê°œë…ì€ ì›ë˜ í†µê³„í•™ì—ì„œ ì‚¬ìš©í•˜ë˜ ê°œë…ìœ¼ë¡œ ì•„ë˜ì™€ íŠ¹ì •í•œ ë¶„í¬ì˜ ê°’ë“¤ì„ ì¶”ì •í•˜ëŠ” ê¸°ë²•ì¤‘ì˜ í•˜ë‚˜ì…ë‹ˆë‹¤.

    > Importance sampling
      In statics, importance sampling is a general technique for estimating properties of a particular distribution,
      while only having samples generated from a different distribution than the distribution of interset

  ì–´ë–¤ ê°’ì„ ì¶”ì •í•˜ëŠ”ë° ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì€ ê·¸ëƒ¥ randomí•˜ê²Œ ì°ì–´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¯¸ ì €í¬ê°€ ë°°ì› ë‹¤ì‹œí”¼ ì´ëŸ¬í•œ processë£Œ í‘œí˜„í•˜ëŠ” ë§ì€ "monte-carlo"ë¡œì„œ Monte-Carlo estimationì´ë¼ê³  í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ê²Œ íƒìƒ‰í•˜ê¸°ë„ í•˜ê³  ì–´ë– í•œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì•Œì•„ì„œ ê·¸ ìœ„ì£¼ë¡œ íƒìƒ‰ì„ í•˜ë©´ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê°’ì„ ì¶”ì •í•  ìˆ˜ ìˆê³  ê·¸ëŸ¬í•œ ì•„ì´ë””ì–´ê°€ ë°”ë¡œ "Importance Sampling"ì…ë‹ˆë‹¤.

  importance samplingì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. pì™€ që¼ëŠ” distributionì´ ìˆì„ ë•Œ që¼ëŠ” distributionì—ì„œ ì‹¤ì œë¡œ ì§„í–‰ì„ í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  pë¡œ ì¶”ì •í•˜ëŠ” ê²ƒì²˜ëŸ¼ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œë„ policyê°€ ë‹¤ë¥´ë©´ stateì˜ distributionì€ ë‹¬ë¼ì§€ê²Œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ distributionì„ í†µí•´ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê°œë…ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ë‹¤ë¥¸ policyë¥¼ í†µí•´ì„œ ì–»ì–´ì§„ sampleì„ ì´ìš©í•˜ì—¬ Q ê°’ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¼ì¢…ì˜ trickì´ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

  ìœ„ì˜ ë‚´ìš©ì„ David Silverêµìˆ˜ë‹˜ì€ ì•„ë˜ì™€ ê°™ì´ ì„¤ëª…í•©ë‹ˆë‹¤. f(X)ë¼ëŠ” í•¨ìˆ˜ë¥¼ value functionì´ë¼ê³  ìƒê°í•˜ê³  ê°•í™”í•™ìŠµì—ì„œëŠ” ì´ value function = expected future rewardë¥¼ ê³„ì† ì¶”ì •í•´ ë‚˜ê°€ëŠ”ë° P(x)ë¼ëŠ” í˜„ì¬ policyë¡œ í˜•ì„±ëœ distributionìœ¼ë¡œë¶€í„° í•™ìŠµì„ í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ Që¼ëŠ” distributionì„ ë”°ë¥´ë©´ì„œë„ ë˜‘ê°™ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ë° ë‹¨, ì•„ë˜ì™€ ê°™ì´ ê°„ë‹¨íˆ ì‹ì„ ë³€í˜•ì‹œì¼œì£¼ë©´ ë©ë‹ˆë‹¤. Q(x)ë¥¼ ê³±í•´ì£¼ê³  ë‚˜ëˆ ì£¼ë©´ ë©ë‹ˆë‹¤.

    > Importance Sampling
      Estimate the expectation of a different distribution
        E_(X~P)[f(x)] = Î£ P(X) * f(X)
                      = Î£ Q(X) * (P(X)/Q(X)) * f(X)
                      = E_(X~Q)[ (P(X)/Q(X)) * f(x)]

  Off-Policy ë˜í•œ MCì™€ TDë¡œ ê°ˆë¦½ë‹ˆë‹¤. Off-policy MCëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  returnì„ ê³„ì‚°í•  ë•Œ ì•„ë˜ì™€ ê°™ì´ ì‹ì„ ë³€í˜•ì‹œì¼œì¤ë‹ˆë‹¤. ê° ìŠ¤í…ì— rewardë¥¼ ë°›ê²Œ ëœ ê²ƒì€ ğœ‡ë¼ëŠ” policyë¥¼ ë”°ë¼ì„œ ì–»ì—ˆë˜ ê²ƒì´ë¯€ë¡œ ë§¤ stepë§ˆë‹¤ ğœ‹/ğœ‡ë¥¼ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ Monte-Carloì— Off-policyë¥¼ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì€ ê·¸ë¦¬ ì¢‹ì€ ì•„ì´ë””ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤.

    > Importance Sampling for Off-Policy Monte-Carlo
      Use returns generated from ğœ‡ to evaluate ğœ‹
      Weight return G_t according to similarity between policies
      Multiply importance sampling corrections along whole episode
                    ğœ‹(A_t|S_t) * ğœ‹(A_(t+1)|S_(t+1)) ... ğœ‹(A_T|S_T)
       G^(ğœ‹/ğœ‡)_t = ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ * G_t
                    ğœ‡(A_t|S_t) * ğœ‡(A_(t+1)|S_(t+1)) ... ğœ‡(A_T|S_T)

      Update value towards corrected return
        V(S_t) <- V(S_t) + ğ›¼ * (G^(ğœ‹/ğœ‡)_t - V(S_t))




















  asd
