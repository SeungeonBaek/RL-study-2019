/*ì´ì›…ì›ë‹˜ Git*/

# Q-Learning

## Importance Sampling

  ì§€ê¸ˆê¹Œì§€ Monte-Carlo Controlê³¼ Temporal-Difference Controlì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ì€ ë‘ ë°©ë²• ëª¨ë‘ on-policy reinforcement learningì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìƒˆë¡œìš´ ê°œë…ì„ í•˜ë‚˜ ì•Œê³  ê°ˆ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

## 1. On-Policy vs Off-Policy

  ë‹¤ì‹œ Sarsaì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ Sarsaì—ì„œëŠ”

  - Choose A  from S  using Policy derived from Q
  - Choose A' from S' using Policy derived from Q

  actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë‘ ë¶€ë¶„ì´ ë§ìŠµë‹ˆë‹¤. ë³´ë©´ ë‘˜ ë‹¤ ê³µí†µì ìœ¼ë¡œ "using Policy derived from Q"ê°€ ì‚¬ìš©ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > An on-policy TD control algorithm
      Initialize Q(s,a), s âˆˆ S, ğ‘ âˆˆ ğ´, arbitrarily, and Q(terminal-state,âˆ™) = 0
      Repeat (for each episode):
        Initialize S
        Choose A from S using policy derived from Q(e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
        Repeat (for each step of episode):
          Take action A, observe R, S'
          Choose A' from S' using policy derived from Q(e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
          Q(S,A) <- Q(S,A) + ğ›¼ * (R + ğ›¾ * Q(S',A') - Q(S,A))
          S <- S'; A <- A';
      until S is terminal

  ì´ ë§ì€ ì¦‰ í˜„ì¬ policyë¡œ ì›€ì§ì´ë©´ì„œ ê·¸ policyë¥¼ í‰ê°€í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë³´ë©´ ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ì›€ì§ì¸ Q functionìœ¼ë¡œ í˜„ì¬ì˜ Q functionì„ updateí•©ë‹ˆë‹¤. ë”°ë¼ì„œ í˜„ì¬ policyìœ„ì—ì„œ control(prediction + policy improvement)ì„ í•˜ê¸° ë•Œë¬¸ì— on-policyë¼ê³  ìƒê°í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.

  í•˜ì§€ë§Œ on-policyì—ëŠ” í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë°”ë¡œ íƒí—˜ì˜ ë¬¸ì œì…ë‹ˆë‹¤. í˜„ì¬ ì•Œê³ ìˆëŠ” ì •ë³´ì— ëŒ€í•´ greedyë¡œ policyë¥¼ ì •í•´ë²„ë¦¬ë©´ optimalì— ê°€ì§€ ëª» í•  í™•ë¥ ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì—ì´ì „íŠ¸ëŠ” í•­ìƒ íƒí—˜ì´ í•„ìš”í•©ë‹ˆë‹¤.

  ë”°ë¼ì„œ on-policyì²˜ëŸ¼ ì›€ì§ì´ëŠ” policyì™€ í•™ìŠµí•˜ëŠ” policyê°€ ê°™ì€ ê²ƒì´ ì•„ë‹ˆê³  ì´ ë‘ê°œì˜ policyë¥¼ ë¶„ë¦¬ì‹œí‚¨ ê²ƒì´ off-policyì…ë‹ˆë‹¤. SilverëŠ” ìˆ˜ì—…ì—ì„œ Off-policyë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•©ë‹ˆë‹¤.

    > Off-policy definition
      Evaluate target policy ğœ‹(a|s) to compute v_ğœ‹(s) or q_ğœ‹(s,a)
      While following behaviour policy ğœ‡(a|s)
        {S_1, A_1, R_2, ... , S_T} ~ ğœ‡
      Why is this important?
        Learn from observing humans or other agents
        Re-use experience generated from old policies ğœ‹_1, ğœ‹_2, ..., ğœ‹_(t-1)
        Learn about optimal policy while following exploratory policy
        Learn about multiple policies while following one policy
