ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content/value_function_approximation.html
/*ì´ì›…ì›ë‹˜ Git*/

# Dynamic programming

  - 5~7ì¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

  ìœ„ì˜ ì„¸ê°€ì§€ ë°©ë²•ì€ ê°ê° ì¥ì ê³¼ ë‹¨ì ì„ ë¶„ëª…íˆ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

  Dynamic programmingì˜ ê²½ìš°, ìˆ˜í•™ì ìœ¼ë¡œ ì˜ ì„¤ê³„ëœ ëª¨ë¸ì´ê¸´ í•˜ì§€ë§Œ, ì™„ì „í•˜ê³  ì •í™•í•œ ëª¨ë¸ì´ í•„ìˆ˜ì ì´ë¼ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

  Monte Carlo methodsì˜ ê²½ìš° simpleí•œ ì»¨ì…‰ì„ ì§€ë‹ˆê³  ìˆê³ , ëª¨ë¸ì„ í•„ìš”ë¡œ í•˜ì§€ëŠ” ì•Šì§€ë§Œ, step-by-step í•™ìŠµì—ëŠ” ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

  Temporal-difference methodsì˜ ê²½ìš°, modelì´ í•„ìš”í•˜ì§€ ì•Šê³ , fully incrementalí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ë³µì¡í•˜ê³  ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.

  ê°ê°ì˜ ë°©ë²•ë“¤ì€ íš¨ìœ¨ê³¼ ìˆ˜ë ´ì†ë„ ì¸¡ë©´ì—ì„œ ì—¬ëŸ¬ê°€ì§€ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.

  ë˜í•œ Dynamic programmingì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    > The term dynamic programming (DP) refers to a collection of algorithm that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP)


## 1.Policy iteration

  - Planning vs Learning

  Planningê³¼ Learningì˜ ì°¨ì´ë¥¼ ë¨¼ì € ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
  Planningì´ë€ environmentì˜ modelì„ ì•Œê³ ì„œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì´ê³ , Learningì´ë€ environmentì˜ modelì„ ëª¨ë¥´ì§€ë§Œ ìƒí˜¸ì‘ìš©ì„ í†µí•´ì„œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.

  Dynamic programmingì€ planningìœ¼ë¡œì„œ Environmentì˜ model(reward, state transition matrix)ì— ëŒ€í•´ì„œ ì•ˆë‹¤ëŠ” ì „ì œì—ì„œ, Bellman equationì„ í†µí•´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì„ ë§í•©ë‹ˆë‹¤.


    > Two fundamental problems in sequential decision making
      - Reinforcement Learning
        The environment is initially unknown
        The agent interacts with the environment
        The agent improves its policy
      - Planning
        A model of the environment is known
        The agent performs computations with its model (without any external interaction)
        The agent improves its policy
        a.k.a deliberation, reasoning, introspection, pondering, thought, search

  - Prediction & control

  Dynamic programmingì€ ë‹¤ìŒì˜ ë‘ step (1) Prediction (2) Control ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

  Predictionì´ë€ í˜„ì¬ optimal í•˜ì§€ ì•Šì€ ì–´ë–¤ policyì— ëŒ€í•´ì„œ value functionì„ êµ¬í•˜ëŠ” ê³¼ì •ì´ë©°, í˜„ì¬ì˜ value functionì„ í† ëŒ€ë¡œ ë” ë‚˜ì€ policyë¥¼ êµ¬í•˜ê³  ì´ì™€ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ optimal policyë¥¼ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  - Policy evaluation

  Policy evaluationì€ perediction ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒìœ¼ë¡œì„œ í˜„ì¬ ì£¼ì–´ì§„ policyì— ëŒ€í•œ true value functionì„ êµ¬í•˜ëŠ” ê²ƒì´ê³ , Bellman equationì„ ì‚¬ìš© í•œë‹¤.

  í˜„ì¬ policyë¥¼ ê°€ì§€ê³  true value functionì„ êµ¬í•˜ëŠ” ê²ƒì€ one-step backupìœ¼ë¡œ êµ¬í•©ë‹ˆë‹¤.

    > Problem : evaluate a given policy ğœ‹
      - Solution : iterative application of Bellman expectation backup
      - v_1 -> v_2 -> ... -> v_ğœ‹
      - Using synchronous backup
        At each iteration k+1
        For all states
        Update v_k+1(s) from v_k(s')
        where s' is a successor state of s

  ì´ì „ì˜ Bellman equationê³¼ ë‹¤ë¥¸ ì ì€, value functionì— kë¼ëŠ” iteration ìˆ«ìê°€ ë¶™ì€ ê²ƒì…ë‹ˆë‹¤.

  ğ‘£_(ğ‘˜+1)(ğ‘ ) = {ğ‘ âˆˆ ğ´} âˆ‘ã€–ğœ‹(ğ‘â”‚ğ‘ ) âˆ— (ğ‘…(ğ‘ ,ğ‘) + ğ›¾ âˆ— {ğ‘  âˆˆ ğ‘†} Î£ P(ssâ€²,a) âˆ— v_k(sâ€²)ã€—

  í˜„ì¬ ìƒíƒœì˜ value functionì„ updateí•˜ëŠ”ë° rewardì™€ next stateë“¤ì˜ value functionì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì „ì²´ MDPì˜ ëª¨ë“  stateì— ëŒ€í•´ì„œ ë™ì‹œì— í•œ ë²ˆì”© Bellman equationì„ ê³„ì‚°í•´ì„œ update í•¨ìœ¼ë¡œì„œ kê°€ í•˜ë‚˜ì”© ì˜¬ë¼ê°€ê²Œ ë©ë‹ˆë‹¤. ì°¨ë¡€ì°¨ë¡€ stateë³„ë¡œ êµ¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , í•œ ë²ˆì— ê³„ì‚°í•´ì„œ í•œ ë²ˆì— value functionì„ updateí•©ë‹ˆë‹¤.

  - Policy improvement

  í•´ë‹¹ policyì— ëŒ€í•œ ì°¸ ê°’ì„ ì–»ì—ˆìœ¼ë©´, ì´ì œ policyë¥¼ ë” ë‚˜ì€ policyë¡œ update í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë˜ì•¼ ì ì  optimal policyì— ê°€ê¹Œì›Œì§ˆ ê²ƒì…ë‹ˆë‹¤.
  ê·¸ëŸ¬í•œ ê³¼ì •ì„ policy improvementë¼ê³  í•©ë‹ˆë‹¤. improveí•˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” greedy improvementê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ stateì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ value functionì„ ê°€ì§„ stateë¡œ ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¦‰, maxë¥¼ ì·¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. (argmax)

    > Improve the policy by acting greedily with respect to v_ğœ‹
      ğœ‹' = greedy(v_ğœ‹)

  ìœ„ì™€ ê°™ì´ evaluationì„ í†µí•´ êµ¬í•œ value functionì„ í† ëŒ€ë¡œ ì—¬ëŸ¬ ë²ˆ improveë¥¼ í•˜ê²Œë˜ë©´ optimal policyë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°˜ë³µë˜ëŠ” ê³¼ì •ì„ Policy iterationì´ë¼ê³  í•©ë‹ˆë‹¤.

***

## 2.Value iteration

  Value iterationì´ Policy iterationê³¼ ë‹¤ë¥¸ ì ì€ Bellman Expectation equationì´ ì•„ë‹ˆê³ , Bellman Optimality equationì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. Bellman Optimality equationì€ optimal value functionë“¤ ì‚¬ì´ì˜ ê´€ê³„ ì‹ì…ë‹ˆë‹¤.

  Value iterationì˜ êµ¬í˜„ì€ ë‹¨ìˆœíˆ ì´ ê´€ê³„ì‹ì„ itrativeí•˜ê²Œ ë³€í™˜ì‹œì¼œì£¼ë©´ ë©ë‹ˆë‹¤.

  Policy itrationì˜ ê²½ìš°ì—ëŠ” evaluationí•  ë•Œ ìˆ˜ë§ì€ ê³„ì‚°ì„ í•´ì¤˜ì•¼ í•˜ëŠ” ë‹¨ì ì´ ìˆì—ˆëŠ”ë°, ê·¸ evaluationì„ ë‹¨ í•œ ë²ˆë§Œ í•˜ëŠ” ê²ƒì´ value iterationì…ë‹ˆë‹¤. ë”°ë¼ì„œ í˜„ì¬ value functionì„ ê³„ì‚°í•˜ê³  updateí•  ë•Œ maxë¥¼ ì·¨í•¨ìœ¼ë¡œì„œ greedyí•˜ê²Œ improveí•˜ëŠ” íš¨ê³¼ë¥¼ ì¤ë‹ˆë‹¤. ë”°ë¼ì„œ í•œ ë²ˆì˜ evaluation + improvement = value iterationì´ ë©ë‹ˆë‹¤.

    > ğ‘£_(ğ‘˜+1)(ğ‘ ) = {ğ‘ âˆˆ ğ´} maxâ¡ã€–( ğ‘…(ğ‘ ,ğ‘) + ğ›¾ âˆ— {s âˆˆ S} Î£ (ğ‘ƒ(ğ‘ ğ‘ â€²,ğ‘) âˆ— ğ‘£_ğ‘˜(ğ‘ â€²)ã€—

***

## 3. Sample Backup

  ì²˜ìŒì— ì–¸ê¸‰í–ˆë‹¤ì‹œí”¼ DPëŠ” MDP ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹¤ ê°€ì§€ê³  ìˆì–´ì•¼ optimal policyë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ DPëŠ” full-width backup(í•œ ë²ˆ updateí•  ë•Œ ê°€ëŠ¥í•œ ëª¨ë“  successor stateì˜ value functionì„ í†µí•´ updateí•˜ëŠ” ë°©ë²•)ì„ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ë‹¨ í•œë²ˆì˜ backupì„ í•˜ëŠ” ë°ë„ ë§ì€ ê³„ì‚°ì„ í•´ì•¼í•©ë‹ˆë‹¤.

  ë˜í•œ state ìˆ«ìê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ê³„ì‚°ëŸ‰ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì—, MDPê°€ ìƒë‹¹íˆ í¬ê±°ë‚˜ MDPì— ëŒ€í•´ì„œ ë‹¤ ì•Œì§€ ëª»í•  ë•Œì—ëŠ” DPë¥¼ ì ìš©ì‹œí‚¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

    > - DP usese full-width backups

      - For each backup (sync or async)
        Every successor state action is considered
        Using knowledge of the MDP transition and reward function

      - DP is effective for medium-sized problems (millions of states)

      - For large problems DP suffers Bellman's curse of dimensionality

      - Even one backup can be too expensive

  ì´ë•Œ ë“±ì¥í•˜ëŠ” ê°œë…ì´ Sample backupì…ë‹ˆë‹¤. ì¦‰, ëª¨ë“  ê°€ëŠ¥í•œ successor stateì™€ actionì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , Samplingì„ í†µí•´ì„œ í•œ ê¸¸ë§Œ ê°€ë³´ê³  ê·¸ ì •ë³´ë¥¼ í† ëŒ€ë¡œ value functionì„ ì—…ë°ì´íŠ¸ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ì´ë ‡ê²Œ í•  ê²½ìš°, ê³„ì‚°ì´ íš¨ìœ¨ì ì´ë¼ëŠ” ì¥ì ë„ ìˆì§€ë§Œ, "Model-free"ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤. ì¦‰, DPì˜ ë°©ë²•ëŒ€ë¡œ optimal í•œ í•´ë¥¼ ì°¾ìœ¼ë ¤ë©´ ë§¤ iterationë§ˆë‹¤ Reward functionê³¼ state transition matrixë¥¼ ì•Œì•„ì•¼ í•˜ëŠ”ë° sample backupì˜ ê²½ìš°ì—ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ <S, A, R, S'>ì„ training setìœ¼ë¡œ ì‹¤ì œ ë‚˜ì˜¨ rewardì™€ sample transitionìœ¼ë¡œì„œ ê·¸ ë‘ê°œë¥¼ ëŒ€ì²´í•˜ê²Œ ë©ë‹ˆë‹¤.

  ë”°ë¼ì„œ MDPë¼ëŠ” modelì„ ëª°ë¼ë„ optimal policyë¥¼ êµ¬í•  ìˆ˜ ìˆê²Œ ë˜ê³ , "Learning"ì´ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  sampleì´ë¼ë©´ ê°œë…ì´ ì¢€ ì–´ë µê²Œ ë‹¤ê°€ì˜¬ ìˆ˜ë„ ìˆëŠ”ë° ì²˜ìŒì— ê°•í™”í•™ìŠµì´ trial & errorë¡œ í•™ìŠµí•œë‹¤ê³  í–ˆë˜ ê²ƒì„ ë– ì˜¬ë ¤ ë³´ë©´, sampleì´ë¼ëŠ” ê²ƒì´ tryê°€ ëœë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.

  ì¦‰, ë„ë©”ì¸ì— ëŒ€í•´ ì˜ ëª¨ë¥´ë”ë¼ë„ ì¼ë‹¨ ê°€ë³´ë©´ì„œ ê²ªëŠ” experienceë¡œë¶€í„° ë¬¸ì œë¥¼ í’€ê¸° ì‹œì‘í•˜ê² ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ì´ë ‡ë“¯, DPë¥¼ samplingì„ í†µí•´ì„œ í’€ë©´ì„œë¶€í„° __"Reinforcement Learning"__ ì´ ì‹œì‘ë©ë‹ˆë‹¤.
