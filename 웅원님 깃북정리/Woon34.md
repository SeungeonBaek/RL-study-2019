/*ì´ì›…ì›ë‹˜ Git*/

# Bellman Equation

## 1. Bellman Equation

  Bellman Equationì€ Dynamic programmingì˜ ê¸°ë°˜ì´ ë˜ëŠ” ë°©ì •ì‹ ì…ë‹ˆë‹¤.

  ì´ì „ markdownì— ì •ë¦¬í•´ ë†“ì•˜ë“¯ì´, MDPì—ì„œ  Value functionì—ëŠ” state-value function v(s)ì™€ action-value function q(s,a) ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

  Value functionì˜ ì •ì˜ì—ì„œ, value functionì€ ë‹¤ìŒê³¼ ê°™ì´ í’€ì–´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - v(s) = E[G_t | S_t = s]

  = E[R_t+1 + ğ›¾ * R_t+2 + ğ›¾^2 * R_t+3 + ... | S_t = s]

  = E[R_t+1 + ğ›¾ * (R_t+2 + ğ›¾ * R_t+3 + ...) | S_t = s]

  = E[R_t+1 + ğ›¾ * G_t+1 | S_t = s]

    > E[R_t+1 + ğ›¾ * v(S_t+1) | S_t = s]

  ì´ ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ì‹ì€ ë§ˆì§€ë§‰ ì‹ìœ¼ë¡œ, ì´ ì‹ì€ í˜„ì¬ stateì™€ ë‹¤ìŒ stateì‚¬ì´ì˜ value functionì„ ê´€ê³„ì§“ëŠ” ì‹ì…ë‹ˆë‹¤.

  ì´ ì‹ì„ Bellman equationì´ë¼ê³  í•©ë‹ˆë‹¤.

  ì´ì™€ ë¹„ìŠ·í•˜ê²Œ action-value functionë„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > q(s,a) = E[R_t+1 + ğ›¾ * q(S_t+1, A_t+1) | S_t = s, A_t = a]

  ì´ë ‡ê²Œ ìœ„ì™€ê°™ì´ expectationì„ ì´ìš©í•œ í‘œí˜„ì€ ì¡°ê¸ˆ ì¶”ìƒì ì¼ ìˆ˜ ìˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  í˜„ì¬ stateì˜ value functionê³¼ ë‹¤ìŒ stateì˜ value functionì˜ ìƒê´€ê´€ê³„ ì‹ì„ êµ¬í•˜ë ¤ë©´ ê·¸ ì‚¬ì´ì— ìˆëŠ” state-action pairì— ëŒ€í•´ì„œ ê·¸ ê´€ê³„ë¥¼ ë‚˜ëˆ ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

  ê·¸ë ‡ê²Œ ë˜ë©´, v(s)ë¼ëŠ” ê°’ì€ ë‹¤ìŒê³¼ ê°™ì´, sì—ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ëª¨ë“  a ì— ëŒ€í•´ ğœ‹(ğ‘â”‚ğ‘ ) * q(s,a) í•©í•œ ê²ƒê³¼ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > ğ‘£_ğœ‹ (ğ‘ ) = {aâˆˆğ´} âˆ‘ã€–ğœ‹(ğ‘â”‚ğ‘ ) * ğ‘_ğœ‹ (ğ‘ ,ğ‘)ã€—

***

  q(s,a)ë¼ëŠ” ê²ƒì€ S_t = s, A_t = a ì¼ë•Œì˜ G_tì¸ë°, ì´ëŠ” R_t+1 + ğ›¾ * R_t+2 + ğ›¾^2 * R_t+3 + ... ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ìŠµë‹ˆë‹¤.

    > ğ‘_ğœ‹ (ğ‘ ,ğ‘)=ğ‘…(ğ‘ ,ğ‘) + ğ›¾ âˆ— {sâˆˆS}âˆ‘ã€–ğ‘ƒ(ğ‘ ğ‘ â€²,ğ‘)âˆ—ğ‘£_ğœ‹ (ğ‘ ^â€²)ã€—

        > R_t+1 = R(s,a) / ğ›¾ * R_t+2 + ğ›¾^2 * R_t+3 + ... ğ›¾ âˆ— {sâˆˆS}âˆ‘ã€–ğ‘ƒ(ğ‘ ğ‘ â€²,ğ‘)âˆ—ğ‘£_ğœ‹ (ğ‘ â€²)ã€—

***

  ì´ ë‘ê°œì˜ ì‹ì„ í•©ì¹˜ë©´, ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > ğ‘£_ğœ‹ (ğ‘ ) = {aâˆˆğ´}âˆ‘ã€–ğœ‹(ğ‘â”‚ğ‘ )âˆ—(ğ‘…(ğ‘ ,ğ‘)+ğ›¾âˆ—Î£ğ‘ƒ(ğ‘ ğ‘ â€²,ğ‘)âˆ—ğ‘£_ğœ‹ (ğ‘ ^â€² ))ã€—

  ì‹¤ì œ ê°•í™”í•™ìŠµìœ¼ë¡œëŠ” ë¬´ì—‡ì¸ê°€ë¥¼ í•™ìŠµ ì‹œí‚¬ ë•Œ rewardì™€ transition probability PëŠ” ë¯¸ë¦¬ ì•Œ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½í—˜ì„ í†µí•´ì„œ ì•Œì•„ê°€ì•¼ë§Œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ì´ëŸ¬í•œ ì •ë³´ë¥¼ ë‹¤ ì•Œë©´ MDPë¥¼ ëª¨ë‘ ì•ˆë‹¤ê³  í‘œí˜„í•˜ë©°, ì´ëŸ¬í•œ ì •ë³´ë“¤ì´ MDPì˜ modelì´ ë©ë‹ˆë‹¤.

  ê°•í™”í•™ìŠµì˜ í° íŠ¹ì§•ì€ ë°”ë¡œ MDPì˜ modelì„ ëª°ë¼ë„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ë”°ë¼ì„œ, reward functionê³¼ state transition probabilityë¥¼ ëª¨ë¥´ê³  í•™ìŠµí•˜ëŠ” ê°•í™”í•™ìŠµì—ì„œëŠ” Bellman equationìœ¼ë¡œëŠ” êµ¬í•  ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.

  - Bellman Equation for Q-function

  ê°™ì€ ì‹ì„ action value functionì— ëŒ€í•´ì„œ ì‘ì„±í•˜ê³  ê·¸ë¦¼ì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    > ğ‘_ğœ‹(ğ‘ ,ğ‘) = ğ‘…(ğ‘ ,ğ‘) + ğ›¾ âˆ— {sâˆˆğ‘†}âˆ‘ã€– ğ‘ƒ(ğ‘ ğ‘ ^â€²,ğ‘) {aâˆˆğ´}âˆ‘ ğœ‹(ğ‘â€²â”‚ğ‘ â€²) * ğ‘_ğœ‹(ğ‘ â€², ğ‘â€²) ã€—

***

  - Optimal value function

  Bellman optimality equationì„ ë³´ê¸° ì „ì— optimal value functionì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

  ê°•í™”í•™ìŠµì˜ ëª©ì ì´ accumulative future rewardë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” policyë¥¼ ì°¾ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤.

  optimal state-value functionì´ë€, í˜„ì¬ stateì—ì„œ policyì— ë”°ë¼ì„œ ì•ìœ¼ë¡œ ë°›ì„ rewardë“¤ì´ ë‹¬ë¼ì§€ëŠ”ë°, ê·¸ ì¤‘ì—ì„œ ì•ìœ¼ë¡œ ê°€ì¥ ë§ì€ rewardë¥¼ ë°›ì„ policyë¥¼ ë‹¤ëì„ ë•Œì˜ value functionì…ë‹ˆë‹¤.

  optimal action-value functionë„ ë§ˆì°¬ê°€ì§€ë¡œ, (s,a)ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ì˜ value functionì…ë‹ˆë‹¤.

  ì •ì˜ : The optimal state-value function v_*(s) is the maximum value function over  all policies

    > v_*(s) = max_ğœ‹(v_ğœ‹(s))

  The optimal action-value function q_*(s) is the maximum action-value function over all policies

    > q_*(s,a) = max_ğœ‹(q_ğœ‹(s,a))

    The optimal value function specifies the best possible performance in the MDP

  - An MDP is "solved" when we know the optimal value function

  ì¦‰, í˜„ì¬ environmentì—ì„œ ì·¨í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ë†’ì€ ê°’ì˜ reward ì´ í•©ì…ë‹ˆë‹¤.

  ìœ„ì˜ ë‘ ì‹ì¤‘ì—ì„œ ë‘ë²ˆì§¸ ì‹. ì¦‰, optimal action-value functionì˜ ê°’ì„ ì•ˆë‹¤ë©´ ë‹¨ìˆœíˆ q ê°’ì´ ë†’ì€ actionì„ ì„ íƒí•´ì£¼ë©´ ë˜ë¯€ë¡œ, ì´ ìµœì í™” ë¬¸ì œëŠ” í’€ë ¸ë‹¤ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ê°•í™”í•™ìŠµ ë¿ë§Œ ì•„ë‹ˆë¼, Dynamic programming ì—ì„œë„ ëª©í‘œê°€ ë˜ëŠ” optimal policyëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

  optimal policyëŠ” (s,a)ì—ì„œ action-value functionì´ ê°€ì¥ ë†’ì€ actionë§Œì„ ê³ ë¥´ê¸° ë•Œë¬¸ì— deterministicí•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  An optimal policy can be found by maximising over q_*(s,a)

    > ğœ‹_* (a|s) = 1 (if a = argmax q_*(s,a) ) or 0 (if otherwise)
