ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/

# Policy Gradient

## 1. Policy Gradient

  í˜„ì¬ ê°•í™”í•™ìŠµì—ì„œ ê°€ì¥ "hot"í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ë°”ë¡œ Policy Gradientì…ë‹ˆë‹¤.

  AlphaGoì˜ ì•Œê³ ë¦¬ì¦˜ë„ "Policy Gradient with Monte-Carlo Tree Search"ë¼ê³  í•©ë‹ˆë‹¤. ë˜í•œ ì‹¤ì œ ë¡œë´‡ì´ë‚˜ í—¬ë¦¬ì½¥í„°, ë“œë¡  ê°™ì€ ë„ë©”ì¸ì— ì ìš©í•˜ê¸° ì í•©í•œ ë°©ë²•ì´ë¼ê³  í•©ë‹ˆë‹¤.

  ì´ì›…ì›ë‹˜ì˜ ê²½ìš° ê°•í™”í•™ìŠµì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ê°€ ë“œë¡ ì˜ ììœ¨ì£¼í–‰ì´ì—ˆê¸° ë•Œë¬¸ì— Policy GradientëŠ” ìƒë‹¹íˆ í¥ë¯¸ë¡­ê²Œ ë‹¤ê°€ì™”ë‹¤ê³  í•©ë‹ˆë‹¤.

## 2. Value-based RL vs Policy-based RL

  ì§€ê¸ˆê¹Œì§€ ì €í¬ê°€ ë‹¤ë£¨ì–´ì™”ë˜ ë°©ë²•ë“¤ì€ ëª¨ë‘ "Value-based" ê°•í™”í•™ìŠµì…ë‹ˆë‹¤. ì¦‰, Që¼ëŠ” action-value functionì— ì´ˆì ì„ ë§ì¶”ì–´ì„œ Q functionì„ êµ¬í•˜ê³  ê·¸ê²ƒì„ í† ëŒ€ë¡œ policyë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

  ì´ì „ì— DQN ë˜í•œ Value-based RLìœ¼ë¡œì¨ DNNì„ ì´ìš©í•˜ì—¬ Q-functionì„ approximateí•˜ê³  policyëŠ” ê·¸ê²ƒì„ í†µí•´ ë§Œë“¤ì–´ì¡Œì„ ë¿ì´ì—ˆìŠµë‹ˆë‹¤.

    > Value-based reinforcement learning vs Policy-based reinforcement learning

      In the last lecture we approximated the values or action-value function using parameters ğœƒ,
        V_ğœƒ(s) â‰ˆ V_pi(s)
        Q_ğœƒ(s,a) â‰ˆ Q_pi(s,a)

      A policy was generated directly from the value function
        e.g. using Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦

  ê·¸ì™€ ë‹¬ë¦¬ Policy-based RLì€ Policy ìì²´ë¥¼ approximateí•´ì„œ function approximatorì—ì„œ ë‚˜ì˜¤ëŠ” ê²ƒì´ value functionì´ ì•„ë‹ˆê³  policyìì²´ê°€ ë‚˜ì˜µë‹ˆë‹¤.

  Policyìì²´ë¥¼ parameterizeí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë³´ë©´ evolutionary ì•Œê³ ë¦¬ì¦˜ì˜ ê°œë…ì— ë” ê°€ê¹ë‹¤ê³  í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì§€ê¸ˆê¹Œì§€ ì €í¬ê°€ ì‚´í´ë³´ì•˜ë“¯ì´ evolutionary ì•Œê³ ë¦¬ì¦˜ê³¼ ë‹¬ë¦¬ ê°•í™”í•™ìŠµì€ í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì´ ìˆìŠµë‹ˆë‹¤.
