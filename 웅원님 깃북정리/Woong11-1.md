ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/

# Policy Gradient

## 1. Policy Gradient

  í˜„ì¬ ê°•í™”í•™ìŠµì—ì„œ ê°€ì¥ "hot"í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ë°”ë¡œ Policy Gradientì…ë‹ˆë‹¤.

  AlphaGoì˜ ì•Œê³ ë¦¬ì¦˜ë„ "Policy Gradient with Monte-Carlo Tree Search"ë¼ê³  í•©ë‹ˆë‹¤. ë˜í•œ ì‹¤ì œ ë¡œë´‡ì´ë‚˜ í—¬ë¦¬ì½¥í„°, ë“œë¡  ê°™ì€ ë„ë©”ì¸ì— ì ìš©í•˜ê¸° ì í•©í•œ ë°©ë²•ì´ë¼ê³  í•©ë‹ˆë‹¤.

  ì´ì›…ì›ë‹˜ì˜ ê²½ìš° ê°•í™”í•™ìŠµì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ê°€ ë“œë¡ ì˜ ììœ¨ì£¼í–‰ì´ì—ˆê¸° ë•Œë¬¸ì— Policy GradientëŠ” ìƒë‹¹íˆ í¥ë¯¸ë¡­ê²Œ ë‹¤ê°€ì™”ë‹¤ê³  í•©ë‹ˆë‹¤.

***

## 2. Value-based RL vs Policy-based RL

  ì§€ê¸ˆê¹Œì§€ ì €í¬ê°€ ë‹¤ë£¨ì–´ì™”ë˜ ë°©ë²•ë“¤ì€ ëª¨ë‘ "Value-based" ê°•í™”í•™ìŠµì…ë‹ˆë‹¤. ì¦‰, Që¼ëŠ” action-value functionì— ì´ˆì ì„ ë§ì¶”ì–´ì„œ Q functionì„ êµ¬í•˜ê³  ê·¸ê²ƒì„ í† ëŒ€ë¡œ policyë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

  ì´ì „ì— DQN ë˜í•œ Value-based RLìœ¼ë¡œì¨ DNNì„ ì´ìš©í•˜ì—¬ Q-functionì„ approximateí•˜ê³  policyëŠ” ê·¸ê²ƒì„ í†µí•´ ë§Œë“¤ì–´ì¡Œì„ ë¿ì´ì—ˆìŠµë‹ˆë‹¤.

    > Value-based reinforcement learning vs Policy-based reinforcement learning

      In the last lecture we approximated the values or action-value function using parameters ğœƒ,
        V_ğœƒ(s)   â‰ˆ V_pi(s)
        Q_ğœƒ(s,a) â‰ˆ Q_pi(s,a)

      A policy was generated directly from the value function
        e.g. using Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦

  ê·¸ì™€ ë‹¬ë¦¬ Policy-based RLì€ Policy ìì²´ë¥¼ approximateí•´ì„œ function approximatorì—ì„œ ë‚˜ì˜¤ëŠ” ê²ƒì´ value functionì´ ì•„ë‹ˆê³  policyìì²´ê°€ ë‚˜ì˜µë‹ˆë‹¤.

  Policyìì²´ë¥¼ parameterizeí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë³´ë©´ evolutionary ì•Œê³ ë¦¬ì¦˜ì˜ ê°œë…ì— ë” ê°€ê¹ë‹¤ê³  í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì§€ê¸ˆê¹Œì§€ ì €í¬ê°€ ì‚´í´ë³´ì•˜ë“¯ì´ evolutionary ì•Œê³ ë¦¬ì¦˜ê³¼ ë‹¬ë¦¬ ê°•í™”í•™ìŠµì€ í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì´ ìˆìŠµë‹ˆë‹¤.

    > Value-based reinforcement learning vs Policy-based reinforcement learning
        In this lecture we will directly parametrise the policy
          ğœ‹_ğœƒ(s,a) = P[a|s, ğœƒ]

        We will focus again on model-free reinforcement learning

  Policy GraidientëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ê³¼ ë‹¨ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

  - ê¸°ì¡´ì˜ ë°©ë²•ì— ë¹„í•´ì„œ ìˆ˜ë ´ì´ ë” ì˜ë˜ë©° ê°€ëŠ¥í•œ actionì´ ì—¬ëŸ¬ê°œì´ê±°ë‚˜(high-dimension) actionìì²´ê°€ ì—°ì†ì ì¸ ê²½ìš°ì— íš¨ê³¼ì ì…ë‹ˆë‹¤. ì¦‰, ì‹¤ì œì˜ ë¡œë´‡ controlì— ì í•©í•©ë‹ˆë‹¤.
  - ë˜í•œ ê¸°ì¡´ì˜ ë°©ë²•ì€ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ optimalí•œ actionìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ”ë° policy gradientì—ì„œëŠ” stochasticí•œ policyë¥¼ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆë¥¼ ë“¤ë©´ ê°€ìœ„ë°”ìœ„ë³´)

  í•˜ì§€ë§Œ local optimumì— ë¹ ì§ˆ ìˆ˜ ìˆìœ¼ë©° policyì˜ evaluateí•˜ëŠ” ê³¼ì •ì´ ë¹„íš¨ìœ¨ì ì´ê³  varianceê°€ ë†’ìŠµë‹ˆë‹¤.

    > Policy-based rien forcement learning
    Advantage
      - Better convergence properties
      - Effective in high-dimensional or continuous action spaces
      - Can learn stochastic policies

    Disadvantage
      - Typically converge to a local rather than global optimum
      - Evaluating a policy is typically inefficient and high variance

  ê¸°ì¡´ ë°©ë²•ì˜ ë¬¸ì œë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Value-based RL ë°©ì‹ì—ëŠ” ë‘ ê°€ì§€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.

### Unstable
  Value-based RLì—ì„œëŠ” Value functionì„ ë°”íƒ•ìœ¼ë¡œ policyë¥¼ ê³„ì‚°í•˜ë¯€ë¡œ Value functionì´ ì¡°ê¸ˆë§Œ ë‹¬ë¼ì§€ë”ë¼ë„, Policyìì²´ëŠ” ì¢ŒíšŒì „ì—ì„œ ìš°íšŒì „ì„ í•˜ëŠ” ë“±ì˜ í¬ê²Œ ë³€í™”í•  ìˆ˜ ìˆëŠ” riskë¥¼ ì•ˆê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ riskê°€ ì „ì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ì— ë¶ˆì•ˆì •ì„±ì„ ë”í•´ì¤ë‹ˆë‹¤.
  í•˜ì§€ë§Œ Policyìì²´ê°€ í•¨ìˆ˜í™” ë˜ì–´ë²„ë¦´ ê²½ìš°, í•™ìŠµì„ í•˜ë©´ì„œ ì¡°ê¸ˆì”© ë³€í•˜ëŠ” value functionìœ¼ë¡œ ì¸í•´ì„œ policyë˜í•œ ì¡°ê¸ˆì”© ë³€í•˜ê²Œ ë˜ì–´ì„œ ì•ˆì •ì ì´ê³  ë¶€ë“œëŸ½ê²Œ ìˆ˜ë ´í•˜ê²Œ ë©ë‹ˆë‹¤.

### Stochastic Policy
  ë•Œë¡œëŠ” Stochastic Policyê°€ Optimal policyì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ìœ„ë°”ìœ„ë³´ ê²Œì„ì€ ë™ë“±í•˜ê²Œ ê°€ìœ„ì™€ ë°”ìœ„ì™€ ë³´ë¥¼ (1/3)ì”© ë‚´ëŠ” ê²ƒì´ Optimal í•œ Policyì…ë‹ˆë‹¤. value-based RLì—ì„œëŠ” Q functionì„ í† ëŒ€ë¡œ í•˜ë‚˜ì˜ actionë§Œ ì„ íƒí•˜ëŠ” optimal policyë¥¼ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë¬¸ì œì—ëŠ” ì ìš©ì‹œí‚¬ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.

***

## 3. Policy Objective Function

  ì´ì œ ê¸°ì¡´ì˜ ë°©ë²•ì²˜ëŸ¼ action-value functionì„ approximateí•˜ì§€ ì•Šê³  policyë¥¼ ë°”ë¡œ approximateí•  ê²ƒì…ë‹ˆë‹¤. í•™ìŠµì€ policyë¥¼ approximateí•œ parameterë“¤ì„ updateí•´ ë‚˜ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ parameterë“¤ì„ updateí•˜ë ¤ë©´ ê¸°ì¤€ì´ í•„ìš”í•œë° DQNì—ì„œëŠ” TD errorë¥¼ ì‚¬ìš©í–ˆì—ˆìŠµë‹ˆë‹¤.
  í•˜ì§€ë§Œ Policy Gradientì—ì„œëŠ” Objective functionì´ë¼ëŠ” ê²ƒì„ ì •ì˜í•©ë‹ˆë‹¤. ì •ì˜í•˜ëŠ” ë°©ë²•ì—ëŠ” ì„¸ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. state value, average value, average reward per time-stepì…ë‹ˆë‹¤.

  ë˜‘ê°™ì€ stateì—ì„œ ì‹œì‘í•˜ëŠ” ê²Œì„ì—ì„œëŠ” ì²˜ìŒ ì‹œì‘ stateì˜ value functionì´ ê°•í™”í•™ìŠµì´ ìµœëŒ€ë¡œ í•˜ê³ ì í•˜ëŠ” ëª©í‘œê°€ ë©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì„¸ ë²ˆì§¸ëŠ” ê° time-stepë§ˆë‹¤ ë°›ëŠ” rewardë“¤ì˜ expectationê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

  ì‚¬ì‹¤ì€ time-stepë§ˆë‹¤ ë°›ëŠ” rewardë“¤ì˜ expectationê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‚¬ì‹¤ì€ time-stepë§ˆë‹¤ ë°›ì€ rewardë“¤ì„ discountì‹œí‚¤ì§€ ì•Šê³  stationary distributionì„ ì‚¬ìš©í•´ì„œ ì–´ë–¤ í–‰ë™ì´ ì¢‹ì•˜ëƒì— ëŒ€í•œ credit assignment ë¬¸ì œë¥¼ í’€ê³  ìˆì§€ ì•Šë‚˜ ìƒê°ë©ë‹ˆë‹¤.

    > Policy Objective functions

     Goal : given policy ğœ‹_ğœƒ(s,a) which parameters ğœƒ, find best ğœƒ
     But how do we measure the quality of a policy ğœ‹_ğœƒ?
     In episode environments we can use the start value
      J_1(ğœƒ) = ~~



***






































3
