/*ì´ì›…ì›ë‹˜ Git*/

# Monte-Carlo Prediction

  - 5~7ì¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

## 1. Model-free

  ì´ì „ Chapterì—ì„œ Dynamic programmingì— ëŒ€í•´ì„œ ë°°ì›Œë³´ì•˜ìŠµë‹ˆë‹¤.
  Dynamic programmingì€ Bellman equationì„ í†µí•´ì„œ optimalí•œ í•´ë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ìœ¼ë¡œì„œ, MDPì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ê°€ì§„ ìƒíƒœì—ì„œ ë¬¸ì œë¥¼ í’€ì–´ë‚˜ê°€ëŠ” ë°©ë²•ì„ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.

  íŠ¹íˆ Environmentì˜ modelì¸ "Reward function"ê³¼ "State transition probability"ë¥¼ ì•Œì•„ì•¼í•˜ê¸° ë•Œë¬¸ì— Model-basedí•œ ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ì´ëŸ¬í•œ ë°©ë²•ì—ëŠ” ì•„ë˜ê³¼ ê°™ì€ ë¬¸ì œì ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

  (1) Full-width Backup => Expensive computation
  (2) Full knowledge about environment

  ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œëŠ” ë°”ë‘‘ê°™ì€ ê²½ìš°ì˜ ìˆ˜ê°€ ë§ì€ ë¬¸ì œë¥¼ í’€ ìˆ˜ê°€ ì—†ê³  ì‹¤ì œ ìƒˆìƒì— ì ìš©ì‹œí‚¬ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.
  ì‚¬ì‹¤ ìœ„ì™€ ê°™ì´ í•™ë¬¸ì ìœ¼ë¡œ ì ‘ê·¼í•˜ì§€ ì•Šë”ë¼ë„ ì´ëŸ¬í•œ ë°©ë²•ì´ ì‚¬ëŒì´ ë°°ìš°ëŠ” ë°©ë²•ê³¼ëŠ” ë§ì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ì´ì „ì—ë„ ì–¸ê¸‰í–ˆì—ˆì§€ë§Œ ì‚¬ëŒì€ ëª¨ë“  ê²ƒì„ ë‹¤ ì•ˆ í›„ì— ì›€ì§ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§Œì ¸ë³´ë©´ì„œ, ë°Ÿì•„ë³´ë©´ì„œ ì¡°ê¸ˆì”© ë°°ì›Œë‚˜ê°‘ë‹ˆë‹¤.

  ì´ì „ì—ë„ ë§í–ˆë“¯ì´ Trial and errorë¥¼ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ê°•í™”í•™ìŠµì˜ í° íŠ¹ì§•ì…ë‹ˆë‹¤. ë”°ë¼ì„œ DPì²˜ëŸ¼ full-width backupì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì œë¡œ ê²½í—˜í•œ ì •ë³´ë“¤ë¡œì„œ updateí•˜ëŠ” sample backupì„ í•˜ê²Œ ë©ë‹ˆë‹¤.

  ì´ë ‡ê²Œ ì‹¤ì œë¡œ ê²½í—˜í•œ ì •ë³´ë“¤ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì²˜ìŒë¶€í„° environmentì— ëŒ€í•´ì„œ ëª¨ë“  ê²ƒì„ ì•Œ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. environmentì˜ modelì„ ëª¨ë¥´ê³  í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— __Model-free__ ë¼ëŠ” ë§ì´ ë¶™ê²Œ ë©ë‹ˆë‹¤.

    > In subsequent lectures we will consider sample backups
      Using sample rewards and sample transitions <S, A, R, S'>
      Instead of reward function R and transition dynamics P
      Advantages :
        Model-free : no advance knowledge of MDP required
        Breaks the curse of dimensionality through sampling
        Cost of backup is constant, independent of n = |S|

  í˜„ì¬ì˜ policyë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›€ì§ì—¬ ë³´ë©´ì„œ, samplingì„ í†µí•´ value functionì„ updateí•˜ëŠ” ê²ƒì„ "model-free prediction"ì´ë¼ê³  í•˜ê³ , policyë¥¼ updateê¹Œì§€ í•˜ê²Œ ëœë‹¤ë©´ "model-free control"ì´ë¼ê³  í•©ë‹ˆë‹¤.

  ì´ë ‡ê²Œ Samplingì„ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” model-free ë°©ë²•ì—ëŠ” ë‹¤ìŒì˜ ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

  (1) Monte-Carlo
  (2) Temporal difference

  Monte-CarloëŠ” episodeë§ˆë‹¤ updateí•˜ëŠ” ë°©ë²•ì´ê³  Temporal DifferenceëŠ” time stepë§ˆë‹¤ updateí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë²ˆ Chapterì—ì„œëŠ” Monte-Carlo Learningì„ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

***

## 2. Monte-Carlo

  Monte-Carloë¼ëŠ” ë§ì— ëŒ€í•´ Suttonêµìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§í•©ë‹ˆë‹¤.

    > Monte-Carlo
      The term "Monte-Carlo" is often used more broadly for any estimation method whose operation

  Monte-Carlo ë‹¨ì–´ ìì²´ëŠ” ë¬´ì—‡ì¸ê°€ë¥¼ randomí•˜ê²Œ ì¸¡ì •í•˜ëŠ” ê²ƒì„ ëœ»í•˜ëŠ” ë§ì´ë¼ê³  í•©ë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œëŠ” "averaging complete returns"í•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì´ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

  Monte-Carloì™€ Temporal Differenceë¡œ ê°ˆë¦¬ëŠ” ê²ƒì€ value functionì„ estimationí•˜ëŠ” ë°©ë²•ì— ë”°ë¼ì„œ ì…ë‹ˆë‹¤. value functionì´ë¼ëŠ” ê²ƒì€ expected accumulative futre rewardë¡œì„œ ì§€ê¸ˆ ì´ stateì—ì„œ ì‹œì‘í•´ì„œ ë¯¸ë˜ê¹Œì§€ ë°›ì„ ê¸°ëŒ€ë˜ëŠ” rewardì˜ ì´í•©ì…ë‹ˆë‹¤. ì´ê²ƒì„ DPê°€ ì•„ë‹ˆë¼ë©´ ì–´ë–»ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆì„ê¹Œìš”?

  ê°€ì¥ ê¸°ë³¸ì ì¸ ìƒê°ì€ episodeë¥¼ ëê¹Œì§€ ê°€ë³¸ í›„ì— ë°›ì€ rewardë“¤ë¡œ ê° stateì˜ value functionë“¤ì„ ê±°ê¾¸ë¡œ ê³„ì‚°í•´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ MC(Monte-Carlo)ëŠ” ëë‚˜ì§€ ì•ŠëŠ” episodeì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. initial state S1ì—ì„œë¶€í„° ì‹œì‘í•´ì„œ terminal state Stê¹Œì§€ í˜„ì¬ policyë¥¼ ë”°ë¼ì„œ ì›€ì§ì´ê²Œ ëœë‹¤ë©´ í•œ time stepë§ˆë‹¤ rewardë¥¼ ë°›ê²Œ ë  í…ë°, ê·¸ rewardë“¤ì„ ê¸°ì–µí•´ ë‘ì—ˆë‹¤ê°€ Stê°€ ë˜ë©´ ë’¤ëŒì•„ë³´ë©´ì„œ ê° stateì˜ value fucntionì„ ê³„ì‚°í•˜ê²Œ ë©ë‹ˆë‹¤. ì•„ë˜ recall that the returnì´ë¼ê³  ë˜ì–´ìˆëŠ”ë° ì œê°€ ë§í•œ ê²ƒê³¼ ê°™ì€ ë§ì…ë‹ˆë‹¤. ìˆœê°„ ìˆœê°„ ë°›ì•˜ë˜ rewardë“¤ì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ discountì‹œì¼œì„œ sample returnì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > Monte-Carlo
      Goal: learn v_ğœ‹ from episodes of experience under policy ğœ‹

        S_1, A_1, R_2, ... , S_k ~ ğœ‹

      Recall that the return is the total discounted reward:

        G_t = R_(t+1) + ğ›¾ * R_(t+2) + ... + ğ›¾^(T-1) * R_(T)

      Recall that the value function is the expected return:
        v_ğœ‹(s) = E_ğœ‹[G_t | S_t = s]
      Monte-Carlo policy evaluation uses empirical mean return instead of expected return

***

## 3. First-Visit MC vs Every-Visit MC

  ìœ„ì—ì„œëŠ” í•œ ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´ ì–´ë–»ê²Œ í•˜ëŠ” ì§€ì— ëŒ€í•´ì„œ ë§í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ multiple episodesë¥¼ ì§„í–‰í•  ê²½ìš°ì—ëŠ” í•œ episodeë§ˆë‹¤ ì–»ì—ˆë˜ returnì„ ì–´ë–»ê²Œ ê³„ì‚°í•´ì•¼í• ê¹Œìš”? MCì—ì„œëŠ” ë‹¨ìˆœíˆ í‰ê· ì„ ì·¨í•´ì¤ë‹ˆë‹¤. í•œ episodeì—ì„œ ì–´ë–¤ stateì— ëŒ€í•´ returnì„ ê³„ì‚°í•´ë†¨ëŠ”ë° ë‹¤ë¥¸ episodeì—ì„œë„ ê·¸ stateë¥¼ ì§€ë‚˜ê°€ì„œ ë‹¤ì‹œ ìƒˆë¡œìš´ returnì„ ì–»ì—ˆì„ ê²½ìš°ì— ê·¸ ë‘ê°œì˜ returnì„ í‰ê· ì„ ì·¨í•´ì£¼ëŠ” ê²ƒì´ê³  ê·¸ returnë“¤ì´ ìŒ“ì´ë©´ ìŒ“ì¼ìˆ˜ë¡ true value functionì— ê°€ê¹Œì›Œì§€ê²Œ ë©ë‹ˆë‹¤.

  í•œ ê°€ì§€ ê³ ë¯¼í•´ì•¼í•  ì ì´ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ì— í•œ episodeë‚´ì—ì„œ ì–´ë– í•œ stateë¥¼ ë‘ ë²ˆ ë°©ë¬¸í•˜ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê°€ìš”? ì´ ë•Œ ì–´ë–»ê²Œ í•˜ëƒì— ë”°ë¼ì„œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰˜ê²Œ ë©ë‹ˆë‹¤.

  - First-visit Monte-Carlo Policy evaluation
  - Every-visit Monte-Carlo Policy evaluation

  ë§ ê·¸ëŒ€ë¡œ First-visitì€ ì²˜ìŒ ë°©ë¬¸í•œ stateë§Œ ì¸ì •í•˜ëŠ” ê²ƒì´ê³ (ë‘ ë²ˆì§¸ ê·¸ state ë°©ë¬¸ì— ëŒ€í•´ì„œëŠ” returnì„ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”) every-visitì€ ë°©ë¬¸í•  ë•Œë§ˆë‹¤ ë”°ë¡œ ë”°ë¡œ returnì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë‘ ë°©ë²•ì€ ëª¨ë‘ ë¬´í•œëŒ€ë¡œ ê°”ì„ ë•Œ true value functionìœ¼ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ First-visitì´ ì¢€ ë” ë„ë¦¬ ì˜¤ë«ë™ì•ˆ ì—°êµ¬ë˜ì–´ ì™”ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” First-visit MCì— ëŒ€í•´ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” First-Visit Monet-Carlo Policy Evalutationì— ëŒ€í•œ Silverêµìˆ˜ë‹˜ ìˆ˜ì—…ì˜ ìë£Œì…ë‹ˆë‹¤.

  > First-visit Monte-Carlo Policy evaluation
    To evaluate state s
    The first time-step t that state s is visit in an episode,
    Increment counter N(s) <- N(s) + 1
    Increment total return S(s) <- S(s) + G_t
    Value is estimated by mean return V(s) = S(s)/N(s)
    By law of large numbers, V(s) -> v_ğœ‹(s) as N(s) -> âˆ

***

## 4. Incermental Mean

  ìœ„ì˜ í‰ê· ì„ ì·¨í•˜ëŠ” ì‹ì„ ì¢€ ë” ë°œì „ì‹œì¼œë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì €í¬ê°€ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°œë¥¼ ëª¨ì•„ë†“ê³  í•œ ë²ˆì— í‰ê· ì„ ì·¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , í•˜ë‚˜ í•˜ë‚˜ ë”í•´ê°€ë©° í‰ê· ì„ ê³„ì‚°í•´ì–— ã…ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì€ incremental meatnì˜ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    The mean ğœ‡_1, ğœ‡_2, ... of a sequence x_1, x_2 ... can be computed incremntally,

      > ğœ‡_k = (1/k) * {j = 1 -> k } Î£ x_j
            = (1/k) * (x_k + {j = 1 -> k-1 } Î£ x_j )
            = (1/k) * (x_k + (k-1) * ğœ‡_(k-1))
            = (1/k) * (x_k + k * ğœ‡_(k-1) - ğœ‡_(k-1))
            = ğœ‡_(k-1) + (1/k) * (x_k - ğœ‡_(k-1))

  ì´ Incremental Meanì„ ìœ„ì˜ First-visit MCì— ì ìš©ì‹œí‚¤ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ê°™ì€ ì‹ì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤. ì´ ë•Œ, ë¶„ìˆ˜ë¡œ ê°€ìˆëŠ” N(S_t)ê°€ ì ì  ë¬´í•œëŒ€ë¡œ ê°€ê²Œë˜ëŠ”ë°, ì´ë¥¼ ì•ŒíŒŒë¡œ ê³ ì •ì‹œì¼œë†“ìœ¼ë©´ íš¨ê³¼ì ìœ¼ë¡œ í‰ê· ì„ ì·¨í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë§¨ ì²˜ìŒ ì •ë³´ë“¤ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ëœ ì£¼ëŠ” í˜•íƒœë¼ê³  ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. (Complementary filterì— ëŒ€í•´ì„œ ì•Œë©´ ì´í•´ê°€ ì‰¬ìš´ ë¶€ë¶„ì…ë‹ˆë‹¤.) ì´ì™€ ê°™ì´ í•˜ëŠ” ì´ìœ ëŠ” ê°•í™”í•™ìŠµì´ stationary problemì´ ì•„ë‹ˆê¸° ëŒ€ë¬¸ì…ë‹ˆë‹¤. ë§¤ epixodeë§ˆë‹¤ ìƒˆë¡œìš´ policyë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— non-stationary problemì´ë¯€ë¡œ updateí•˜ëŠ” ìƒìˆ˜ë¥¼ ì¼ì •í•˜ê²Œ ê³ ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

      > Incremental Mean
        Update V(s) incrementally after episodes S_1, A_1, R_2, ..., S_T
        For each State S_t with return G_t

          N(S_t) <- N(S_t) + 1
          V(S_t) <- V(S_t) + (1/N(S_t)) * (G_t - V(S_t))

        In non-sationary problems, it can be useful to track a running mean, i.e. forget old episodes.

          V(S_t) <- V(S_t) + ğ›¼ * (G_t - V(S_t))
          => alphaê°€ 0.3ì´ë¼ë©´, ìµœê·¼ ê²ƒì€ 0.3 ì´ì „ê²ƒì€ 0.3^2 ì´ëŸ°ì‹ìœ¼ë¡œ update ë˜ëŠ” ê²ƒ.

***

## 5. Backup Diagram

  ì´ëŸ¬í•œ MCì˜ backup ê³¼ì •ì€ DPì™€ ë‹¤ë¦…ë‹ˆë‹¤. DPì—ì„œëŠ” one-step backupì—ì„œ ê·¸ ë‹¤ìŒìœ¼ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  stateë“¤ë¡œ ê°€ì§€ê°€ ë»—ì—ˆì—ˆëŠ”ë° MCì—ì„œëŠ” samplingì„ í•˜ê¸° ë•Œë¬¸ì— í•˜ë‚˜ì˜ ê°€ì§€ë¡œ terminal stateê¹Œì§€ ê°€ê²Œë©ë‹ˆë‹¤.

  Monte-CarloëŠ” ì²˜ìŒì— random processë¥¼ í¬í•¨í•œ ë°©ë²•ì´ë¼ê³  ë§í–ˆì—ˆëŠ”ë° episode ë§ˆë‹¤ updateí•˜ê¸° ë•Œë¬¸ì—, ì²˜ìŒ ì‹œì‘ì´ ì–´ë””ì—ˆëƒì— ë”°ë¼ì„œ ë˜í•œ ê°™ì€ stateì—ì„œ ì™¼ìª½ìœ¼ë¡œ ê°€ëƒ, ì˜¤ë¥¸ ìª½ìœ¼ë¡œ ê°€ëƒì— ë”°ë¼ì„œ ì „í˜€ ë‹¤ë¥¸ experienceê°€ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ randomí•œ ìš”ì†Œë¥¼ í¬í•¨í•˜ê³  ìˆì–´ì„œ MCëŠ” varianceê°€ ë†’ìŠµë‹ˆë‹¤. ëŒ€ì‹ ì— randomì¸ë§Œí¼ ì–´ë”˜ê°€ì— ì¹˜ìš°ì¹˜ëŠ” ê²½í–¥ì€ ì ì–´ì„œ biasëŠ” ë‚®ì€ í¸ì…ë‹ˆë‹¤.

***

# Monte-Carlo Control

## 1. Monte-Carlo Policy iteration

  ìœ„ì—ì„œëŠ” Monte-Carlo Policy Evaluation = Predictionì„ ë³´ì•˜ìŠµë‹ˆë‹¤.
  Dynamic Programmingë•Œë„ Policy evalutaion + Policy Improvement = Policy iterationì´ì—ˆë“¯ì´ MCì—ì„œë„ MC Policy Evaluation + Policy Improvementë¥¼ í•˜ë©´ MC Policy iterationì´ ë©ë‹ˆë‹¤.

  ë‹¤ì‹œ í•œë²ˆ DPì˜ Policy iterationì„ ìƒê°í•´ ë´…ì‹œë‹¤. í˜„ì¬ policyë¥¼ í† ëŒ€ë¡œ Value functionì„ iterativeí•˜ê²Œ ê³„ì‚°í•´ì„œ policyë¥¼ evaluation(true value functionì— ìˆ˜ë ´í•  ë•Œê¹Œì§€)í•˜ê³  ê·¸ value functionì„ í† ëŒ€ë¡œ greedyí•˜ê²Œ poilicyë¥¼ improveí•˜ê³  ê·¸ëŸ¬í•œ ê³¼ì •ì„ optimal policyë¥¼ ì–»ì„ ë•Œê¹Œì§€ ë°˜ë³µí•˜ì˜€ìŠµë‹ˆë‹¤.

  ì—¬ê¸°ì— Policy evaluationë§Œ Monte-Carlo Policy evaluationìœ¼ë¡œ ë°”ê¾¸ì–´ ì£¼ë©´, Monte-Carlo Policy iterationì´ ë©ë‹ˆë‹¤.

    > Monte-Carlo Policy iteration
      Policy evalutaion  : Monte-Carlo policy evalutation, V = v_ğœ‹?
      Policy improvement : Greedy policy improvement

***

## 2. Monte-Carlo Control

  í•˜ì§€ë§Œ, Monte-Carlo Policy iterationì—ëŠ” ì„¸ ê°€ì§€ ë¬¸ì œì ì´ ìˆìŠµë‹ˆë‹¤.
  - Value function
  - Exploration
  - Policy iteration

  ### (1) Value function

  í˜„ì¬ MCë¡œì¨ Policyë¥¼ evaluationí•˜ëŠ”ë° Value functionì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ value functionì„ ì‚¬ìš©í•˜ë©´ policyë¥¼ improve(greedy)í•  ë•Œ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì›ë˜ MCë¥¼ í–ˆë˜ ì´ìœ ëŠ” Model-freeë¥¼ í•˜ê¸° ìœ„í•´ì„œ ì˜€ëŠ”ë°, value functionìœ¼ë¡œ policyë¥¼ improveí•˜ë ¤ë©´ MDPì˜ modelì„ ì•Œì•„ì•¼í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ë‹¤ìŒ policyë¥¼ ê³„ì‚°í•˜ë ¤ë©´ rewardì™€ transition probabilityë¥¼ ì•Œì•„ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ value function ëŒ€ì‹ ì— action value functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì´ëŸ¬í•œ ë¬¸ì œì—†ì´ model-freeê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > Value function for MC control
      Greedy policy improvement over V(s) requires model of MDP
        ğœ‹â€²(ğ‘ ) = {ğ‘ âˆˆ ğ´} ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘…(ğ‘ ,ğ‘))+ğ‘ƒ(ğ‘ ğ‘ â€²,ğ‘) âˆ— ğ‘‰(ğ‘ â€²)
      Greedy policy improvement over Q(s,a) is model-free
        ğœ‹â€²(ğ‘ ) = {ğ‘ âˆˆ ğ´} ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(Q(s,a))

***

  ### (2) Exploration

  í˜„ì¬ëŠ” policy improveëŠ” greedy policy improvementë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê³„ì† í˜„ì¬ ìƒí™©ì—ì„œ ìµœê³ ì˜ ê²ƒë§Œ ë³´ê³  íŒë‹¨ì„ í•  ê²½ìš°ì—ëŠ” global optimumìœ¼ë¡œ ê°€ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , local optimumì— ë¹ ì ¸ë²„ë¦´ ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.

  ì¶©ë¶„íˆ explorationì„ í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— global optimumì— ê°€ì§€ ëª»í–ˆë˜ ê²ƒì…ë‹ˆë‹¤. í˜„ì¬ action aê°€ ê°€ì¥ ë†’ì€ value functionì„ ê°€ì§„ë‹¤ê³  ì¸¡ì •ì´ ë˜ì–´ì„œ actionì„ aë§Œ í•˜ê²Œ ë˜ë©´ ì‚¬ì‹¤ì€ bê°€ ë” ë†’ì€ value functionì„ ê°€ì§ˆ ìˆ˜ë„ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ë°°ì œí•´ë²„ë¦¬ê²Œ ë©ë‹ˆë‹¤. ë§ˆì¹˜ ëŒ€í•™êµë‚˜ ì„±ì ë§Œ ë³´ê³  ì‚¬ëŒì„ ë½‘ì•„ì“°ëŠ” ê²ƒê³¼ ê°™ì€ ì‹¤ìˆ˜ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤.


  ì•„ë˜ì™€ ê°™ì´ ì„ íƒí•  ìˆ˜ ìˆëŠ” acitonì´ mê°œ ìˆì„ ê²½ìš°ì— greedy action(ê°€ì¥ action value functionì´ ë†’ì€ action)ê³¼ ë‹¤ë¥¸ actionë“¤ì„ ì•„ë˜ì™€ ê°™ì€ í™•ë¥ ë¡œ ë‚˜ëˆ ì„œ ì„ íƒí•©ë‹ˆë‹¤. ì´ë¡œì¨, ë¶€ì¡±í–ˆë˜ explorationì„ í•  ìˆ˜ ìˆê²Œ ëœ ê²ƒì…ë‹ˆë‹¤.

    > Exploration
      Simplest idea for ensuring continual exploration
      All m actions are tried with non-zero probability
      With probability 1 - Ïµ choose the greedy action
      With probability Ïµ choose action at random

        ğœ‹(ğ‘â”‚ğ‘ ) = ğœ–/ğ‘š + 1 - ğœ–  (if ğ‘âˆ— = {ğ‘ âˆˆ ğ´} ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘„(ğ‘ ,ğ‘)))
        ğœ‹(ğ‘â”‚ğ‘ ) = ğœ–/ğ‘š          (otherwise)
