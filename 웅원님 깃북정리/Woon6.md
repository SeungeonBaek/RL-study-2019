/*ì´ì›…ì›ë‹˜ Git*/

# Monte-Carlo Prediction

  - 5~7ìž¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìžˆìŠµë‹ˆë‹¤.

## 1. Model-free

  ì´ì „ Chapterì—ì„œ Dynamic programmingì— ëŒ€í•´ì„œ ë°°ì›Œë³´ì•˜ìŠµë‹ˆë‹¤.
  Dynamic programmingì€ Bellman equationì„ í†µí•´ì„œ optimalí•œ í•´ë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ìœ¼ë¡œì„œ, MDPì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ê°€ì§„ ìƒíƒœì—ì„œ ë¬¸ì œë¥¼ í’€ì–´ë‚˜ê°€ëŠ” ë°©ë²•ì„ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.

  íŠ¹ížˆ Environmentì˜ modelì¸ "Reward function"ê³¼ "State transition probability"ë¥¼ ì•Œì•„ì•¼í•˜ê¸° ë•Œë¬¸ì— Model-basedí•œ ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

  ì´ëŸ¬í•œ ë°©ë²•ì—ëŠ” ì•„ëž˜ê³¼ ê°™ì€ ë¬¸ì œì ì´ ì¡´ìž¬í•©ë‹ˆë‹¤.

  (1) Full-width Backup => Expensive computation
  (2) Full knowledge about environment

  ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œëŠ” ë°”ë‘‘ê°™ì€ ê²½ìš°ì˜ ìˆ˜ê°€ ë§Žì€ ë¬¸ì œë¥¼ í’€ ìˆ˜ê°€ ì—†ê³  ì‹¤ì œ ìƒˆìƒì— ì ìš©ì‹œí‚¬ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.
  ì‚¬ì‹¤ ìœ„ì™€ ê°™ì´ í•™ë¬¸ì ìœ¼ë¡œ ì ‘ê·¼í•˜ì§€ ì•Šë”ë¼ë„ ì´ëŸ¬í•œ ë°©ë²•ì´ ì‚¬ëžŒì´ ë°°ìš°ëŠ” ë°©ë²•ê³¼ëŠ” ë§Žì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

  ì´ì „ì—ë„ ì–¸ê¸‰í–ˆì—ˆì§€ë§Œ ì‚¬ëžŒì€ ëª¨ë“  ê²ƒì„ ë‹¤ ì•ˆ í›„ì— ì›€ì§ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§Œì ¸ë³´ë©´ì„œ, ë°Ÿì•„ë³´ë©´ì„œ ì¡°ê¸ˆì”© ë°°ì›Œë‚˜ê°‘ë‹ˆë‹¤.

  ì´ì „ì—ë„ ë§í–ˆë“¯ì´ Trial and errorë¥¼ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ê°•í™”í•™ìŠµì˜ í° íŠ¹ì§•ìž…ë‹ˆë‹¤. ë”°ë¼ì„œ DPì²˜ëŸ¼ full-width backupì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì œë¡œ ê²½í—˜í•œ ì •ë³´ë“¤ë¡œì„œ updateí•˜ëŠ” sample backupì„ í•˜ê²Œ ë©ë‹ˆë‹¤.

  ì´ë ‡ê²Œ ì‹¤ì œë¡œ ê²½í—˜í•œ ì •ë³´ë“¤ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì²˜ìŒë¶€í„° environmentì— ëŒ€í•´ì„œ ëª¨ë“  ê²ƒì„ ì•Œ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. environmentì˜ modelì„ ëª¨ë¥´ê³  í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— __Model-free__ ë¼ëŠ” ë§ì´ ë¶™ê²Œ ë©ë‹ˆë‹¤.

    > In subsequent lectures we will consider sample backups
      Using sample rewards and sample transitions <S, A, R, S'>
      Instead of reward function R and transition dynamics P
      Advantages :
        Model-free : no advance knowledge of MDP required
        Breaks the curse of dimensionality through sampling
        Cost of backup is constant, independent of n = |S|

  í˜„ìž¬ì˜ policyë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›€ì§ì—¬ ë³´ë©´ì„œ, samplingì„ í†µí•´ value functionì„ updateí•˜ëŠ” ê²ƒì„ "model-free prediction"ì´ë¼ê³  í•˜ê³ , policyë¥¼ updateê¹Œì§€ í•˜ê²Œ ëœë‹¤ë©´ "model-free control"ì´ë¼ê³  í•©ë‹ˆë‹¤.

  ì´ë ‡ê²Œ Samplingì„ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” model-free ë°©ë²•ì—ëŠ” ë‹¤ìŒì˜ ë‘ ê°€ì§€ ë°©ë²•ì´ ìžˆìŠµë‹ˆë‹¤.

  (1) Monte-Carlo
  (2) Temporal difference

  Monte-CarloëŠ” episodeë§ˆë‹¤ updateí•˜ëŠ” ë°©ë²•ì´ê³  Temporal DifferenceëŠ” time stepë§ˆë‹¤ updateí•˜ëŠ” ë°©ë²•ìž…ë‹ˆë‹¤. ì´ë²ˆ Chapterì—ì„œëŠ” Monte-Carlo Learningì„ ì‚´íŽ´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

***

## 2. Monte-Carlo

  Monte-Carloë¼ëŠ” ë§ì— ëŒ€í•´ Suttonêµìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§í•©ë‹ˆë‹¤.

    > Monte-Carlo
      The term "Monte-Carlo" is often used more broadly for any estimation method whose operation

  Monte-Carlo ë‹¨ì–´ ìžì²´ëŠ” ë¬´ì—‡ì¸ê°€ë¥¼ randomí•˜ê²Œ ì¸¡ì •í•˜ëŠ” ê²ƒì„ ëœ»í•˜ëŠ” ë§ì´ë¼ê³  í•©ë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œëŠ” "averaging complete returns"í•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì´ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

  Monte-Carloì™€ Temporal Differenceë¡œ ê°ˆë¦¬ëŠ” ê²ƒì€ value functionì„ estimationí•˜ëŠ” ë°©ë²•ì— ë”°ë¼ì„œ ìž…ë‹ˆë‹¤. value functionì´ë¼ëŠ” ê²ƒì€ expected accumulative futre rewardë¡œì„œ ì§€ê¸ˆ ì´ stateì—ì„œ ì‹œìž‘í•´ì„œ ë¯¸ëž˜ê¹Œì§€ ë°›ì„ ê¸°ëŒ€ë˜ëŠ” rewardì˜ ì´í•©ìž…ë‹ˆë‹¤. ì´ê²ƒì„ DPê°€ ì•„ë‹ˆë¼ë©´ ì–´ë–»ê²Œ ì¸¡ì •í•  ìˆ˜ ìžˆì„ê¹Œìš”?

  ê°€ìž¥ ê¸°ë³¸ì ì¸ ìƒê°ì€ episodeë¥¼ ëê¹Œì§€ ê°€ë³¸ í›„ì— ë°›ì€ rewardë“¤ë¡œ ê° stateì˜ value functionë“¤ì„ ê±°ê¾¸ë¡œ ê³„ì‚°í•´ë³´ëŠ” ê²ƒìž…ë‹ˆë‹¤. ë”°ë¼ì„œ MC(Monte-Carlo)ëŠ” ëë‚˜ì§€ ì•ŠëŠ” episodeì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë°©ë²•ìž…ë‹ˆë‹¤. initial state S1ì—ì„œë¶€í„° ì‹œìž‘í•´ì„œ terminal state Stê¹Œì§€ í˜„ìž¬ policyë¥¼ ë”°ë¼ì„œ ì›€ì§ì´ê²Œ ëœë‹¤ë©´ í•œ time stepë§ˆë‹¤ rewardë¥¼ ë°›ê²Œ ë  í…ë°, ê·¸ rewardë“¤ì„ ê¸°ì–µí•´ ë‘ì—ˆë‹¤ê°€ Stê°€ ë˜ë©´ ë’¤ëŒì•„ë³´ë©´ì„œ ê° stateì˜ value fucntionì„ ê³„ì‚°í•˜ê²Œ ë©ë‹ˆë‹¤. ì•„ëž˜ recall that the returnì´ë¼ê³  ë˜ì–´ìžˆëŠ”ë° ì œê°€ ë§í•œ ê²ƒê³¼ ê°™ì€ ë§ìž…ë‹ˆë‹¤. ìˆœê°„ ìˆœê°„ ë°›ì•˜ë˜ rewardë“¤ì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ discountì‹œì¼œì„œ sample returnì„ êµ¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

    > Monte-Carlo
      Goal: learn v_ðœ‹ from episodes of experience under policy ðœ‹

        S_1, A_1, R_2, ... , S_k ~ ðœ‹

      Recall that the return is the total discounted reward:

        G_t = R_(t+1) + ð›¾ * R_(t+2) + ... + ð›¾^(T-1) * R_(T)

      Recall that the value function is the expected return:
        v_ðœ‹(s) = E_ðœ‹[G_t | S_t = s]
      Monte-Carlo policy evaluation uses empirical mean return instead of expected return

***

## 3. First-Visit MC vs Every-Visit MC

  ìœ„ì—ì„œëŠ” í•œ ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´ ì–´ë–»ê²Œ í•˜ëŠ” ì§€ì— ëŒ€í•´ì„œ ë§í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ multiple episodesë¥¼ ì§„í–‰í•  ê²½ìš°ì—ëŠ” í•œ episodeë§ˆë‹¤ ì–»ì—ˆë˜ returnì„ ì–´ë–»ê²Œ ê³„ì‚°í•´ì•¼í• ê¹Œìš”? MCì—ì„œëŠ” ë‹¨ìˆœížˆ í‰ê· ì„ ì·¨í•´ì¤ë‹ˆë‹¤. í•œ episodeì—ì„œ ì–´ë–¤ stateì— ëŒ€í•´ returnì„ ê³„ì‚°í•´ë†¨ëŠ”ë° ë‹¤ë¥¸ episodeì—ì„œë„ ê·¸ stateë¥¼ ì§€ë‚˜ê°€ì„œ ë‹¤ì‹œ ìƒˆë¡œìš´ returnì„ ì–»ì—ˆì„ ê²½ìš°ì— ê·¸ ë‘ê°œì˜ returnì„ í‰ê· ì„ ì·¨í•´ì£¼ëŠ” ê²ƒì´ê³  ê·¸ returnë“¤ì´ ìŒ“ì´ë©´ ìŒ“ì¼ìˆ˜ë¡ true value functionì— ê°€ê¹Œì›Œì§€ê²Œ ë©ë‹ˆë‹¤.

  í•œ ê°€ì§€ ê³ ë¯¼í•´ì•¼í•  ì ì´ ìžˆìŠµë‹ˆë‹¤. ë§Œì•½ì— í•œ episodeë‚´ì—ì„œ ì–´ë– í•œ stateë¥¼ ë‘ ë²ˆ ë°©ë¬¸í•˜ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê°€ìš”? ì´ ë•Œ ì–´ë–»ê²Œ í•˜ëƒì— ë”°ë¼ì„œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰˜ê²Œ ë©ë‹ˆë‹¤.

  - First-visit Monte-Carlo Policy evaluation
  - Every-visit Monte-Carlo Policy evaluation

  ë§ ê·¸ëŒ€ë¡œ First-visitì€ ì²˜ìŒ ë°©ë¬¸í•œ stateë§Œ ì¸ì •í•˜ëŠ” ê²ƒì´ê³ (ë‘ ë²ˆì§¸ ê·¸ state ë°©ë¬¸ì— ëŒ€í•´ì„œëŠ” returnì„ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”) every-visitì€ ë°©ë¬¸í•  ë•Œë§ˆë‹¤ ë”°ë¡œ ë”°ë¡œ returnì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ìž…ë‹ˆë‹¤. ë‘ ë°©ë²•ì€ ëª¨ë‘ ë¬´í•œëŒ€ë¡œ ê°”ì„ ë•Œ true value functionìœ¼ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ First-visitì´ ì¢€ ë” ë„ë¦¬ ì˜¤ëž«ë™ì•ˆ ì—°êµ¬ë˜ì–´ ì™”ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” First-visit MCì— ëŒ€í•´ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì•„ëž˜ëŠ” First-Visit Monet-Carlo Policy Evalutationì— ëŒ€í•œ Silverêµìˆ˜ë‹˜ ìˆ˜ì—…ì˜ ìžë£Œìž…ë‹ˆë‹¤.

  > First-visit Monte-Carlo Policy evaluation
    To evaluate state s
    The first time-step t that state s is visit in an episode,
    Increment counter N(s) <- N(s) + 1
    Increment total return S(s) <- S(s) + G_t
    Value is estimated by mean return V(s) = S(s)/N(s)
    By law of large numbers, V(s) -> v_ðœ‹(s) as N(s) -> âˆž

***

## 4. Incermental Mean

  ìœ„ì˜ í‰ê· ì„ ì·¨í•˜ëŠ” ì‹ì„ ì¢€ ë” ë°œì „ì‹œì¼œë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì €í¬ê°€ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°œë¥¼ ëª¨ì•„ë†“ê³  í•œ ë²ˆì— í‰ê· ì„ ì·¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , í•˜ë‚˜ í•˜ë‚˜ ë”í•´ê°€ë©° í‰ê· ì„ ê³„ì‚°í•´ì–— ã…ê¸° ë•Œë¬¸ì— ì•„ëž˜ì™€ ê°™ì€ incremental meatnì˜ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    The mean ðœ‡_1, ðœ‡_2, ... of a sequence x_1, x_2 ... can be computed incremntally,

      > ðœ‡_k = (1/k) * {j = 1 -> k } Î£ x_j
            = (1/k) * (x_k + {j = 1 -> k-1 } Î£ x_j )
            = (1/k) * (x_k + (k-1) * ðœ‡_(k-1))
            = (1/k) * (x_k + k * ðœ‡_(k-1) - ðœ‡_(k-1))
            = ðœ‡_(k-1) + (1/k) * (x_k - ðœ‡_(k-1))

  ì´ Incremental Meanì„ ìœ„ì˜ First-visit MCì— ì ìš©ì‹œí‚¤ë©´ ì•„ëž˜ì™€ ê°™ìŠµë‹ˆë‹¤. ê°™ì€ ì‹ì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•œ ê²ƒìž…ë‹ˆë‹¤. ì´ ë•Œ, ë¶„ìˆ˜ë¡œ ê°€ìžˆëŠ” N(S_t)ê°€ ì ì  ë¬´í•œëŒ€ë¡œ ê°€ê²Œë˜ëŠ”ë°, ì´ë¥¼ ì•ŒíŒŒë¡œ ê³ ì •ì‹œì¼œë†“ìœ¼ë©´ íš¨ê³¼ì ìœ¼ë¡œ í‰ê· ì„ ì·¨í•  ìˆ˜ ìžˆê²Œ ë©ë‹ˆë‹¤. ë§¨ ì²˜ìŒ ì •ë³´ë“¤ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ëœ ì£¼ëŠ” í˜•íƒœë¼ê³  ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. (Complementary filterì— ëŒ€í•´ì„œ ì•Œë©´ ì´í•´ê°€ ì‰¬ìš´ ë¶€ë¶„ìž…ë‹ˆë‹¤.) ì´ì™€ ê°™ì´ í•˜ëŠ” ì´ìœ ëŠ” ê°•í™”í•™ìŠµì´ stationary problemì´ ì•„ë‹ˆê¸° ëŒ€ë¬¸ìž…ë‹ˆë‹¤. ë§¤ epixodeë§ˆë‹¤ ìƒˆë¡œìš´ policyë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— non-stationary problemì´ë¯€ë¡œ updateí•˜ëŠ” ìƒìˆ˜ë¥¼ ì¼ì •í•˜ê²Œ ê³ ì •í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤.

      > Incremental Mean
        Update V(s) incrementally after episodes S_1, A_1, R_2, ..., S_T
        For each State S_t with return G_t

          N(S_t) <- N(S_t) + 1
          V(S_t) <- V(S_t) + (1/N(S_t)) * (G_t - V(S_t))

        In non-sationary problems, it can be useful to track a running mean, i.e. forget old episodes.

          V(S_t) <- V(S_t) + ð›¼ * (G_t - V(S_t))
          => alphaê°€ 0.3ì´ë¼ë©´, ìµœê·¼ ê²ƒì€ 0.3 ì´ì „ê²ƒì€ 0.3^2 ì´ëŸ°ì‹ìœ¼ë¡œ update ë˜ëŠ” ê²ƒ.

***

## 5. Backup Diagram

  ì´ëŸ¬í•œ MCì˜ backup ê³¼ì •ì€ DPì™€ ë‹¤ë¦…ë‹ˆë‹¤. DPì—ì„œëŠ” one-step backupì—ì„œ ê·¸ ë‹¤ìŒìœ¼ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  stateë“¤ë¡œ ê°€ì§€ê°€ ë»—ì—ˆì—ˆëŠ”ë° MCì—ì„œëŠ” samplingì„ í•˜ê¸° ë•Œë¬¸ì— í•˜ë‚˜ì˜ ê°€ì§€ë¡œ terminal stateê¹Œì§€ ê°€ê²Œë©ë‹ˆë‹¤.

  Monte-CarloëŠ” ì²˜ìŒì— random processë¥¼ í¬í•¨í•œ ë°©ë²•ì´ë¼ê³  ë§í–ˆì—ˆëŠ”ë° episode ë§ˆë‹¤ updateí•˜ê¸° ë•Œë¬¸ì—, ì²˜ìŒ ì‹œìž‘ì´ ì–´ë””ì—ˆëƒì— ë”°ë¼ì„œ ë˜í•œ ê°™ì€ stateì—ì„œ ì™¼ìª½ìœ¼ë¡œ ê°€ëƒ, ì˜¤ë¥¸ ìª½ìœ¼ë¡œ ê°€ëƒì— ë”°ë¼ì„œ ì „í˜€ ë‹¤ë¥¸ experienceê°€ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ randomí•œ ìš”ì†Œë¥¼ í¬í•¨í•˜ê³  ìžˆì–´ì„œ MCëŠ” varianceê°€ ë†’ìŠµë‹ˆë‹¤. ëŒ€ì‹ ì— randomì¸ë§Œí¼ ì–´ë”˜ê°€ì— ì¹˜ìš°ì¹˜ëŠ” ê²½í–¥ì€ ì ì–´ì„œ biasëŠ” ë‚®ì€ íŽ¸ìž…ë‹ˆë‹¤.

***

# Monte-Carlo Control

## 1. Monte-Carlo Policy iteration

  ìœ„ì—ì„œëŠ” Monte-Carlo Policy Evaluation = Predictionì„ ë³´ì•˜ìŠµë‹ˆë‹¤.
  Dynamic Programmingë•Œë„ Policy evalutaion + Policy Improvement = Policy iterationì´ì—ˆë“¯ì´ MCì—ì„œë„ MC Policy Evaluation + Policy Improvementë¥¼ í•˜ë©´ MC Policy iterationì´ ë©ë‹ˆë‹¤.

  ë‹¤ì‹œ í•œë²ˆ DPì˜ Policy iterationì„ ìƒê°í•´ ë´…ì‹œë‹¤. í˜„ìž¬ policyë¥¼ í† ëŒ€ë¡œ Value functionì„ iterativeí•˜ê²Œ ê³„ì‚°í•´ì„œ policyë¥¼ evaluation(true value functionì— ìˆ˜ë ´í•  ë•Œê¹Œì§€)í•˜ê³  ê·¸ value functionì„ í† ëŒ€ë¡œ greedyí•˜ê²Œ poilicyë¥¼ improveí•˜ê³  ê·¸ëŸ¬í•œ ê³¼ì •ì„ optimal policyë¥¼ ì–»ì„ ë•Œê¹Œì§€ ë°˜ë³µí•˜ì˜€ìŠµë‹ˆë‹¤.

  ì—¬ê¸°ì— Policy evaluationë§Œ Monte-Carlo Policy evaluationìœ¼ë¡œ ë°”ê¾¸ì–´ ì£¼ë©´, Monte-Carlo Policy iterationì´ ë©ë‹ˆë‹¤.

    > Monte-Carlo Policy iteration
      Policy evalutaion  : Monte-Carlo policy evalutation, V = v_ðœ‹?
      Policy improvement : Greedy policy improvement

***

## 2. Monte-Carlo Control

  í•˜ì§€ë§Œ, Monte-Carlo Policy iterationì—ëŠ” ì„¸ ê°€ì§€ ë¬¸ì œì ì´ ìžˆìŠµë‹ˆë‹¤.
  - Value function
  - Exploration
  - Policy iteration

  ### (1) Value function

  í˜„ìž¬ MCë¡œì¨ Policyë¥¼ evaluationí•˜ëŠ”ë° Value functionì„ ì‚¬ìš©í•˜ê³  ìžˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ value functionì„ ì‚¬ìš©í•˜ë©´ policyë¥¼ improve(greedy)í•  ë•Œ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì›ëž˜ MCë¥¼ í–ˆë˜ ì´ìœ ëŠ” Model-freeë¥¼ í•˜ê¸° ìœ„í•´ì„œ ì˜€ëŠ”ë°, value functionìœ¼ë¡œ policyë¥¼ improveí•˜ë ¤ë©´ MDPì˜ modelì„ ì•Œì•„ì•¼í•©ë‹ˆë‹¤. ì•„ëž˜ì™€ ê°™ì´ ë‹¤ìŒ policyë¥¼ ê³„ì‚°í•˜ë ¤ë©´ rewardì™€ transition probabilityë¥¼ ì•Œì•„ì•¼ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ value function ëŒ€ì‹ ì— action value functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì´ëŸ¬í•œ ë¬¸ì œì—†ì´ model-freeê°€ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

    > Value function for MC control
      Greedy policy improvement over V(s) requires model of MDP
        ðœ‹â€²(ð‘ ) = {ð‘Ž âˆˆ ð´} ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘…(ð‘ ,ð‘Ž))+ð‘ƒ(ð‘ ð‘ â€²,ð‘Ž) âˆ— ð‘‰(ð‘ â€²)
      Greedy policy improvement over Q(s,a) is model-free
        ðœ‹â€²(ð‘ ) = {ð‘Ž âˆˆ ð´} ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(Q(s,a))

***

  ### (2) Exploration

  í˜„ìž¬ëŠ” policy improveëŠ” greedy policy improvementë¥¼ ì‚¬ìš©í•˜ê³  ìžˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê³„ì† í˜„ìž¬ ìƒí™©ì—ì„œ ìµœê³ ì˜ ê²ƒë§Œ ë³´ê³  íŒë‹¨ì„ í•  ê²½ìš°ì—ëŠ” global optimumìœ¼ë¡œ ê°€ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , local optimumì— ë¹ ì ¸ë²„ë¦´ ìˆ˜ê°€ ìžˆìŠµë‹ˆë‹¤.

  ì¶©ë¶„ížˆ explorationì„ í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— global optimumì— ê°€ì§€ ëª»í–ˆë˜ ê²ƒìž…ë‹ˆë‹¤. í˜„ìž¬ action aê°€ ê°€ìž¥ ë†’ì€ value functionì„ ê°€ì§„ë‹¤ê³  ì¸¡ì •ì´ ë˜ì–´ì„œ actionì„ aë§Œ í•˜ê²Œ ë˜ë©´ ì‚¬ì‹¤ì€ bê°€ ë” ë†’ì€ value functionì„ ê°€ì§ˆ ìˆ˜ë„ ìžˆëŠ” ê°€ëŠ¥ì„±ì„ ë°°ì œí•´ë²„ë¦¬ê²Œ ë©ë‹ˆë‹¤. ë§ˆì¹˜ ëŒ€í•™êµë‚˜ ì„±ì ë§Œ ë³´ê³  ì‚¬ëžŒì„ ë½‘ì•„ì“°ëŠ” ê²ƒê³¼ ê°™ì€ ì‹¤ìˆ˜ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤.

  ë”°ë¼ì„œ ê·¸ì— ë”°ë¥¸ ëŒ€ì•ˆìœ¼ë¡œì„œ ì¼ì • í™•ë¥ ë¡œ í˜„ìž¬ ìƒíƒœì—ì„œ ê°€ìž¥ ë†’ì€ ê°€ì¹˜ë¥¼ ê°€ì§€ì§€ ì•Šì€ ë‹¤ë¥¸ actionì„ í•˜ë„ë¡ í•©ë‹ˆë‹¤. ê·¸ ì¼ì • í™•ë¥ ì„ epsilon Ïµë¡œ í‘œê¸°í•˜ë©°, ì´ë ‡ê²Œ explorationì„ í¬í•¨í•œ improvementë°©ë²•ì„ epsilon greedy policy improvementë¼ê³  í•©ë‹ˆë‹¤.

  ì•„ëž˜ì™€ ê°™ì´ ì„ íƒí•  ìˆ˜ ìžˆëŠ” acitonì´ mê°œ ìžˆì„ ê²½ìš°ì— greedy action(ê°€ìž¥ action value functionì´ ë†’ì€ action)ê³¼ ë‹¤ë¥¸ actionë“¤ì„ ì•„ëž˜ì™€ ê°™ì€ í™•ë¥ ë¡œ ë‚˜ëˆ ì„œ ì„ íƒí•©ë‹ˆë‹¤. ì´ë¡œì¨, ë¶€ì¡±í–ˆë˜ explorationì„ í•  ìˆ˜ ìžˆê²Œ ëœ ê²ƒìž…ë‹ˆë‹¤.

    > Exploration
      Simplest idea for ensuring continual exploration
      All m actions are tried with non-zero probability
      With probability 1 - Ïµ choose the greedy action
      With probability Ïµ choose action at random

        ðœ‹(ð‘Žâ”‚ð‘ ) = ðœ–/ð‘š + 1 - ðœ–  (if ð‘Žâˆ— = {ð‘Ž âˆˆ ð´} ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘„(ð‘ ,ð‘Ž)))
               = ðœ–/ð‘š          (otherwise)

***

  ### (3) Policy iteration

  Policy iterationì—ì„œëŠ” evalutation ê³¼ì •ì´ true value functionìœ¼ë¡œ ìˆ˜ë ´í•  ë•Œê¹Œì§€ í•´ì•¼í•˜ëŠ”ë° ê·¸ë ‡ê²Œ í•˜ì§€ ì•Šê³  í•œ ë²ˆ evaluationí•œ ë‹¤ìŒì— policy improveë¥¼ í•´ë„ optimalë¡œ ê°„ë‹¤ê³  ë§í–ˆì—ˆìŠµë‹ˆë‹¤.

  ê·¸ê²ƒì´ Value iterationì´ì—ˆëŠ”ë° Monte-Carloì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ ì´ evaluationê³¼ì •ì„ ì¤„ìž„ìœ¼ë¡œì¨ Monte-Carlo policy iterationì—ì„œ Monte-Carlo controlì´ ë©ë‹ˆë‹¤. ê²°êµ­ Monte-Carlo Controlì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    > Monte-Carlo Control
      Every episode:
        Policy evalutation : Monte-Carlo policy evalutaion, Q â‰ˆ q_ðœ‹
        Policy improvement : Ïµ-greedy policy improvement

***

## 3. GLIE (Greedy in the Limit with Infinite Exploration)

  GLIEëž€ suttonêµìˆ˜ë‹˜ ì±…ì—ëŠ” ì•ˆ ë‚˜ì™”ì§€ë§Œ, Silver êµìˆ˜ë‹˜ ê°•ì˜ì—ì„œ ë‚˜ì™”ë˜ ë‚´ìš©ìž…ë‹ˆë‹¤. í•™ìŠµì„ í•´ë‚˜ê°ì— ë”°ë¼ ì¶©ë¶„í•œ íƒí—˜ì„ í–ˆë‹¤ë©´ greedy policyì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.

  í•˜ì§€ë§Œ Ïµ-greedy policyë¡œì„œëŠ” greedyí•˜ê²Œ í•˜ë‚˜ì˜ actionë§Œ ì„ íƒí•˜ì§€ ì•ŠëŠ”ë° ì´ëŸ´ ê²½ìš°ëŠ” GLIEí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³´í†µ learningì„ í†µí•´ì„œ ë°°ìš°ë ¤ëŠ” optimal policyëŠ” greedy policyìž…ë‹ˆë‹¤. ë”°ë¼ì„œ explorationë¬¸ì œ ë•Œë¬¸ì— ì‚¬ìš©í•˜ëŠ” Ïµ-greedyì—ì„œ epsilon Ïµê°€ ì‹œê°„ì— íë¦„ì— ë”°ë¼ 0ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤ë©´ Ïµ-greedy ë˜í•œ GLIEê°€ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. í›„ì—ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ off-policy controlë¡œì¨ Q-learningì„ ì“°ë©´ì„œ í•´ê²°í•˜ê²Œ ë©ë‹ˆë‹¤.

    > GLIE Definition
      Greedy in the Limit with Infinite Exploration (GLIE)
      All state-action pairs are explored infinitely many times,
        lim

      The policy converges on a greedy policy,
        lim

      For example, Ïµ-greedy is GLIE if Ïµ reduces to zero at Ïµ_k = 1/k
