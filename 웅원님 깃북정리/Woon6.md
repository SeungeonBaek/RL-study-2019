/*ì´ì›…ì›ë‹˜ Git*/

# Monte-Carlo Methods

  - 5~7ì¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

## 1. Model-free

  ì´ì „ Chapterì—ì„œ Dynamic programmingì— ëŒ€í•´ì„œ ë°°ì›Œë³´ì•˜ìŠµë‹ˆë‹¤.
  Dynamic programmingì€ Bellman equationì„ í†µí•´ì„œ optimalí•œ í•´ë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ìœ¼ë¡œì„œ, MDPì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ê°€ì§„ ìƒíƒœì—ì„œ ë¬¸ì œë¥¼ í’€ì–´ë‚˜ê°€ëŠ” ë°©ë²•ì„ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.

  íŠ¹íˆ Environmentì˜ modelì¸ "Reward function"ê³¼ "State transition probability"ë¥¼ ì•Œì•„ì•¼í•˜ê¸° ë•Œë¬¸ì— Model-basedí•œ ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ì´ëŸ¬í•œ ë°©ë²•ì—ëŠ” ì•„ë˜ê³¼ ê°™ì€ ë¬¸ì œì ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

  (1) Full-width Backup => Expensive computation
  (2) Full knowledge about environment

  ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œëŠ” ë°”ë‘‘ê°™ì€ ê²½ìš°ì˜ ìˆ˜ê°€ ë§ì€ ë¬¸ì œë¥¼ í’€ ìˆ˜ê°€ ì—†ê³  ì‹¤ì œ ìƒˆìƒì— ì ìš©ì‹œí‚¬ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.
  ì‚¬ì‹¤ ìœ„ì™€ ê°™ì´ í•™ë¬¸ì ìœ¼ë¡œ ì ‘ê·¼í•˜ì§€ ì•Šë”ë¼ë„ ì´ëŸ¬í•œ ë°©ë²•ì´ ì‚¬ëŒì´ ë°°ìš°ëŠ” ë°©ë²•ê³¼ëŠ” ë§ì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ì´ì „ì—ë„ ì–¸ê¸‰í–ˆì—ˆì§€ë§Œ ì‚¬ëŒì€ ëª¨ë“  ê²ƒì„ ë‹¤ ì•ˆ í›„ì— ì›€ì§ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§Œì ¸ë³´ë©´ì„œ, ë°Ÿì•„ë³´ë©´ì„œ ì¡°ê¸ˆì”© ë°°ì›Œë‚˜ê°‘ë‹ˆë‹¤.

  ì´ì „ì—ë„ ë§í–ˆë“¯ì´ Trial and errorë¥¼ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ê°•í™”í•™ìŠµì˜ í° íŠ¹ì§•ì…ë‹ˆë‹¤. ë”°ë¼ì„œ DPì²˜ëŸ¼ full-width backupì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì œë¡œ ê²½í—˜í•œ ì •ë³´ë“¤ë¡œì„œ updateí•˜ëŠ” sample backupì„ í•˜ê²Œ ë©ë‹ˆë‹¤.

  ì´ë ‡ê²Œ ì‹¤ì œë¡œ ê²½í—˜í•œ ì •ë³´ë“¤ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì²˜ìŒë¶€í„° environmentì— ëŒ€í•´ì„œ ëª¨ë“  ê²ƒì„ ì•Œ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. environmentì˜ modelì„ ëª¨ë¥´ê³  í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— __Model-free__ ë¼ëŠ” ë§ì´ ë¶™ê²Œ ë©ë‹ˆë‹¤.

    > In subsequent lectures we will consider sample backups
      Using sample rewards and sample transitions <S, A, R, S'>
      Instead of reward function R and transition dynamics P
      Advantages :
        Model-free : no advance knowledge of MDP required
        Breaks the curse of dimensionality through sampling
        Cost of backup is constant, independent of n = |S|

  í˜„ì¬ì˜ policyë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›€ì§ì—¬ ë³´ë©´ì„œ, samplingì„ í†µí•´ value functionì„ updateí•˜ëŠ” ê²ƒì„ "model-free prediction"ì´ë¼ê³  í•˜ê³ , policyë¥¼ updateê¹Œì§€ í•˜ê²Œ ëœë‹¤ë©´ "model-free control"ì´ë¼ê³  í•©ë‹ˆë‹¤.

  ì´ë ‡ê²Œ Samplingì„ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” model-free ë°©ë²•ì—ëŠ” ë‹¤ìŒì˜ ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

  (1) Monte-Carlo
  (2) Temporal difference

  Monte-CarloëŠ” episodeë§ˆë‹¤ updateí•˜ëŠ” ë°©ë²•ì´ê³  Temporal DifferenceëŠ” time stepë§ˆë‹¤ updateí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë²ˆ Chapterì—ì„œëŠ” Monte-Carlo Learningì„ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

***

## 2. Monte-Carlo

  Monte-Carloë¼ëŠ” ë§ì— ëŒ€í•´ Suttonêµìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§í•©ë‹ˆë‹¤.

    > Monte-Carlo
      The term "Monte-Carlo" is often used more broadly for any estimation method whose operation

  Monte-Carlo ë‹¨ì–´ ìì²´ëŠ” ë¬´ì—‡ì¸ê°€ë¥¼ randomí•˜ê²Œ ì¸¡ì •í•˜ëŠ” ê²ƒì„ ëœ»í•˜ëŠ” ë§ì´ë¼ê³  í•©ë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œëŠ” "averaging complete returns"í•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì´ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

  Monte-Carloì™€ Temporal Differenceë¡œ ê°ˆë¦¬ëŠ” ê²ƒì€ value functionì„ estimationí•˜ëŠ” ë°©ë²•ì— ë”°ë¼ì„œ ì…ë‹ˆë‹¤. value functionì´ë¼ëŠ” ê²ƒì€ expected accumulative futre rewardë¡œì„œ ì§€ê¸ˆ ì´ stateì—ì„œ ì‹œì‘í•´ì„œ ë¯¸ë˜ê¹Œì§€ ë°›ì„ ê¸°ëŒ€ë˜ëŠ” rewardì˜ ì´í•©ì…ë‹ˆë‹¤. ì´ê²ƒì„ DPê°€ ì•„ë‹ˆë¼ë©´ ì–´ë–»ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆì„ê¹Œìš”?

  ê°€ì¥ ê¸°ë³¸ì ì¸ ìƒê°ì€ episodeë¥¼ ëê¹Œì§€ ê°€ë³¸ í›„ì— ë°›ì€ rewardë“¤ë¡œ ê° stateì˜ value functionë“¤ì„ ê±°ê¾¸ë¡œ ê³„ì‚°í•´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ MC(Monte-Carlo)ëŠ” ëë‚˜ì§€ ì•ŠëŠ” episodeì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. initial state S1ì—ì„œë¶€í„° ì‹œì‘í•´ì„œ terminal state Stê¹Œì§€ í˜„ì¬ policyë¥¼ ë”°ë¼ì„œ ì›€ì§ì´ê²Œ ëœë‹¤ë©´ í•œ time stepë§ˆë‹¤ rewardë¥¼ ë°›ê²Œ ë  í…ë°, ê·¸ rewardë“¤ì„ ê¸°ì–µí•´ ë‘ì—ˆë‹¤ê°€ Stê°€ ë˜ë©´ ë’¤ëŒì•„ë³´ë©´ì„œ ê° stateì˜ value fucntionì„ ê³„ì‚°í•˜ê²Œ ë©ë‹ˆë‹¤. ì•„ë˜ recall that the returnì´ë¼ê³  ë˜ì–´ìˆëŠ”ë° ì œê°€ ë§í•œ ê²ƒê³¼ ê°™ì€ ë§ì…ë‹ˆë‹¤. ìˆœê°„ ìˆœê°„ ë°›ì•˜ë˜ rewardë“¤ì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ discountì‹œì¼œì„œ sample returnì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > Monte-Carlo
      Goal: learn v_ğœ‹ from episodes of experience under policy ğœ‹
        S_1, A_1, R_2, ... , S_k ~ ğœ‹
      Recall that the return is the total discounted reward:
        G_t = R_(t+1) + ğ›¾ * R_(t+2) + ... + ğ›¾^(T-1) * R_(T)
      Recall that the value function is the expected return:
        v_ğœ‹(s) = E_ğœ‹[G_t | S_t = s]
      Monte-Carlo policy evaluation uses empirical mean return instead of expected return

***

## 3. First-Visit MC vs Every-Visit MC

  ìœ„ì—ì„œëŠ” í•œ ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´ ì–´ë–»ê²Œ í•˜ëŠ” ì§€ì— ëŒ€í•´ì„œ ë§í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ multiple episodesë¥¼ ì§„í–‰í•  ê²½ìš°ì—ëŠ” í•œ episodeë§ˆë‹¤ ì–»ì—ˆë˜ returnì„ ì–´ë–»ê²Œ ê³„ì‚°í•´ì•¼í• ê¹Œìš”? MCì—ì„œëŠ” ë‹¨ìˆœíˆ í‰ê· ì„ ì·¨í•´ì¤ë‹ˆë‹¤. í•œ episodeì—ì„œ ì–´ë–¤ stateì— ëŒ€í•´ returnì„ ê³„ì‚°í•´ë†¨ëŠ”ë° ë‹¤ë¥¸ episodeì—ì„œë„ ê·¸ stateë¥¼ ì§€ë‚˜ê°€ì„œ ë‹¤ì‹œ ìƒˆë¡œìš´ returnì„ ì–»ì—ˆì„ ê²½ìš°ì— ê·¸ ë‘ê°œì˜ returnì„ í‰ê· ì„ ì·¨í•´ì£¼ëŠ” ê²ƒì´ê³  ê·¸ returnë“¤ì´ ìŒ“ì´ë©´ ìŒ“ì¼ìˆ˜ë¡ true value functionì— ê°€ê¹Œì›Œì§€ê²Œ ë©ë‹ˆë‹¤.

  í•œ ê°€ì§€ ê³ ë¯¼í•´ì•¼í•  ì ì´ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ì— í•œ episodeë‚´ì—ì„œ ì–´ë– í•œ stateë¥¼ ë‘ ë²ˆ ë°©ë¬¸í•˜ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê°€ìš”? ì´ ë•Œ ì–´ë–»ê²Œ í•˜ëƒì— ë”°ë¼ì„œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰˜ê²Œ ë©ë‹ˆë‹¤.

  - First-visit Monte-Carlo Policy evaluation
  - Every-visit Monte-Carlo Policy evaluation

  ë§ ê·¸ëŒ€ë¡œ First-visitì€ ì²˜ìŒ ë°©ë¬¸í•œ stateë§Œ ì¸ì •í•˜ëŠ” ê²ƒì´ê³ (ë‘ ë²ˆì§¸ ê·¸ state ë°©ë¬¸ì— ëŒ€í•´ì„œëŠ” returnì„ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”) every-visitì€ ë°©ë¬¸í•  ë•Œë§ˆë‹¤ ë”°ë¡œ ë”°ë¡œ returnì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë‘ ë°©ë²•ì€ ëª¨ë‘ ë¬´í•œëŒ€ë¡œ ê°”ì„ ë•Œ true value functionìœ¼ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ First-visitì´ ì¢€ ë” ë„ë¦¬ ì˜¤ë«ë™ì•ˆ ì—°êµ¬ë˜ì–´ ì™”ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” First-visit MCì— ëŒ€í•´ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” First-Visit Monet-Carlo Policy Evalutationì— ëŒ€í•œ Silverêµìˆ˜ë‹˜ ìˆ˜ì—…ì˜ ìë£Œì…ë‹ˆë‹¤.

  > First-visit Monte-Carlo Policy evaluation
    To evaluate state s
    The first time-step t that state s is visit in an episode,
    Increment counter N(s) <- N(s) + 1
    Increment total return S(s) <- S(s) + G_t
    Value is estimated by mean return V(s) = S(s)/N(s)
    By law of large numbers, V(s) -> v_ğœ‹(s) as N(s) -> âˆ

***

## 4. Incermental Mean

  ìœ„ì˜ í‰ê· ì„ ì·¨í•˜ëŠ” ì‹ì„ ì¢€ ë” ë°œì „ì‹œì¼œë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì €í¬ê°€ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°œë¥¼ ëª¨ì•„ë†“ê³  í•œ ë²ˆì— í‰ê· ì„ ì·¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , í•˜ë‚˜ í•˜ë‚˜ ë”í•´ê°€ë©° í‰ê· ì„ ê³„ì‚°í•´ì–— ã…ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì€ incremental meatnì˜ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    The mean ğœ‡_1, ğœ‡_2, ... of a sequence x_1, x_2 ... can be computed incremntally,

      > ğœ‡_k = (1/k) * {j = 1 -> k } Î£ x_j
            = (1/k) * (x_k + {j = 1 -> k-1 } Î£ x_j )
            = (1/k) * (x_k + (k-1) * ğœ‡_(k-1))
            = (1/k) * (x_k + k * ğœ‡_(k-1) - ğœ‡_(k-1))
            = ğœ‡_(k-1) + (1/k) * (x_k - ğœ‡_(k-1))

  ì´ Incremental Meanì„ ìœ„ì˜ First-visit MCì— ì ìš©ì‹œí‚¤ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ê°™ì€ ì‹ì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤. ì´ ë•Œ, ë¶„ìˆ˜ë¡œ ê°€ìˆëŠ” N(S_t)ê°€ ì ì  ë¬´í•œëŒ€ë¡œ ê°€ê²Œë˜ëŠ”ë°, ì´ë¥¼ ì•ŒíŒŒë¡œ ê³ ì •ì‹œì¼œë†“ìœ¼ë©´ íš¨ê³¼ì ìœ¼ë¡œ í‰ê· ì„ ì·¨í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë§¨ ì²˜ìŒ ì •ë³´ë“¤ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ëœ ì£¼ëŠ” í˜•íƒœë¼ê³  ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. (Complementary filterì— ëŒ€í•´ì„œ ì•Œë©´ ì´í•´ê°€ ì‰¬ìš´ ë¶€ë¶„ì…ë‹ˆë‹¤.) ì´ì™€ ê°™ì´ í•˜ëŠ” ì´ìœ ëŠ” ê°•í™”í•™ìŠµì´ stationary problemì´ ì•„ë‹ˆê¸° ëŒ€ë¬¸ì…ë‹ˆë‹¤. ë§¤ epixodeë§ˆë‹¤ ìƒˆë¡œìš´ policyë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— non-stationary problemì´ë¯€ë¡œ updateí•˜ëŠ” ìƒìˆ˜ë¥¼ ì¼ì •í•˜ê²Œ ê³ ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.



***
