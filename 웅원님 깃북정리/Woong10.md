ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/ DQN

# Neural Network

## 1. What is DQN

  ê°•í™”í•™ìŠµì—ì„œ agentëŠ” environmentë¥¼ MDPë¥¼ í†µí•´ì„œ ì´í•´ë¥¼ í•˜ëŠ”ë° tableí˜•íƒœë¡œ í•™ìŠµì„ ëª¨ë“  stateì— ëŒ€í•œ action-value functionì˜ ê°’ì„ ì €ì¥í•˜ê³  update ì‹œì¼œë‚˜ê°€ëŠ” ì‹ìœ¼ë¡œ í•˜ë©´ í•™ìŠµì´ ìƒë‹¹íˆ ëŠë ¤ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ approximationì„ í•˜ê²Œ ë˜ê³ , ê·¸ approximationë°©ë²• ì¤‘ì—ì„œ nonlinear function approximatorë¡œ deep neural networkê°€ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ action-value function(q-value)ë¥¼ approximateí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ deep neural networkë¥¼ íƒí•œ reinforcement learningë°©ë²•ì´ Deep Rienforcement Learning(deepRL)ì…ë‹ˆë‹¤. ë˜í•œ action value functionë¿ë§Œ ì•„ë‹ˆë¼ policy ìì²´ë¥¼ approximateí•  ìˆ˜ë„ ìˆëŠ”ë° ê·¸ approximatorë¡œ DNNì„ ì‚¬ìš©í•´ë„ DeepRLì´ ë©ë‹ˆë‹¤.

  action value functionì„ approximateí•˜ëŠ” deep neural networksë¥¼ Deep Q-Networks(DQN)ì´ë¼ê³  í•˜ëŠ”ë° ê·¸ë ‡ë‹¤ë©´ DQNìœ¼ë¡œ ì–´ë–»ê²Œ í•™ìŠµí• ê¹Œìš”? DQNì´ë¼ëŠ” ê°œë…ì€ DeepMindì˜ "Playing Atari with Deep Reinforcement Learning"ì´ë¼ëŠ” ë…¼ë¬¸ì— ì†Œê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
  https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf

***

## 2. Artificial Neural Networks (ANN)

  http://sanghyukchun.github.io/74/
  http://arxiv.org/pdf/cs/0308031.pdf
  http://www.slideshare.net/imanog/artificial-neural-network-48027460
  http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
  ì´ì›…ì› ë‹˜ì€ ë‹¤ìŒì˜ referenceë¥¼ í†µí•´ì„œ ê¹ƒë¶ì„ ì‘ì„±í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.

  DeepRLì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ì ì¸ ê°œë…ì— ëŒ€í•´ì„œ ì•Œ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ê°•í™”í•™ìŠµì´ ì‚¬ëŒì˜ í–‰ë™ë°©ì‹ì„ ëª¨ë°©í–ˆë‹¤ë¼ê³  í•œë‹¤ë©´, artificial neural networksëŠ” ì‚¬ëŒì˜ ë‡Œì˜ êµ¬ì¡°ë¥¼ ëª¨ë°©í–ˆìŠµë‹ˆë‹¤.

  ì¸ê³µì§€ëŠ¥ì´ ì‚¬ëŒì˜ ë‡Œë¥¼ ëª¨ë°©í•˜ê²Œ ëœ ê²ƒì—ëŠ” ì»´í“¨í„°ê°€ ê³„ì‚°ê³¼ ê°™ì€ ì¼ì—ëŠ” ì‚¬ëŒë³´ë‹¤ ë›°ì–´ë‚œ performanceë¥¼ ë‚´ì§€ë§Œ ê°œì™€ ê³ ì–‘ì´ë¥¼ êµ¬ë³„í•˜ëŠ” ì‚¬ëŒì´ë¼ë©´ ëˆ„êµ¬ë‚˜ ê°„ë‹¨í•˜ê²Œ í•˜ëŠ” ì¼ì€ ì»´í“¨í„°ëŠ” í•˜ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

  ë”°ë¼ì„œ ì´ë¯¸ ë‡Œì˜ êµ¬ì¡°ì— ëŒ€í•´ì„œëŠ” ìˆ˜ë§ì€ ë‰´ëŸ°ë“¤ê³¼ ì‹œëƒ…ìŠ¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œê³  ê·¸ê²ƒì„ ìˆ˜í•™ì  ëª¨ë¸ë¡œ ë§Œë“¤ì–´ì„œ ì»´í“¨í„°ì˜ ì•Œê³ ë¦¬ì¦˜ì— ì ìš©ì‹œí‚¤ëŠ” ë°©ë²•ì„ íƒí•œ ê²ƒì…ë‹ˆë‹¤.

  neural networksì˜ ìˆ˜í•™ì  ëª¨ë¸ì€ ì‹¤ì œ ë‰´ëŸ°ì˜ êµ¬ì¡° ë° ìƒí™”í•™ì  ì›ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ ì¡ŒìŠµë‹ˆë‹¤. ê° Neuronë“¤ì€ synapseë¥¼ í†µí•´ì„œ signalì„ ë°›ìŠµë‹ˆë‹¤. ë§Œì•½ signalì´ ì–´ë–¤ íŠ¹ì •í•œ thresholdë¥¼ ë„˜ì–´ê°„ë‹¤ë©´ neruonì´ activateë˜ê³  ê·¸ ë‰´ëŸ°ì€ axonì„ í†µí•´ì„œ signalì„ ë‹¤ë¥¸ synapseë¡œ ë³´ëƒ…ë‹ˆë‹¤.

  ë˜í•œ ë³´í†µ ë‰´ëŸ°ì€ ì—¬ëŸ¬ê°œì˜ inputì´ ë“¤ì–´ì™€ì„œ ì—¬ëŸ¬ê°œì˜ outputì´ ë‚˜ê°€ëŠ” êµ¬ì¡°ì´ë©°, ê·¸ inputê³¼ outputì€ ë‰´ëŸ°ì‚¬ì´ì˜ ì—°ê²°ì„ í†µí•´ì„œ ì „ë‹¬ì´ ë©ë‹ˆë‹¤.

  ì˜ˆë¥¼ ë“¤ì–´ ë‰´ëŸ°ì˜ ì‹œëƒ…ìŠ¤ê°€ 10ê°œë¼ê³  ê°€ì •í•˜ê²Œ ë˜ë©´ ì´ ì‹œëƒ…ìŠ¤ë¥¼ í†µí•´ì„œ 10ê°œì˜ ë‹¤ë¥¸ inputë“¤ì´ ë“¤ì–´ì˜¤ê²Œ ë©ë‹ˆë‹¤. ë˜í•œ neuronì˜ processì— ë“¤ì–´ê°€ëŠ” ê°’ì€ ì´ 10ê°œì˜ inputë“¤ì˜ linear combinationì…ë‹ˆë‹¤. ì´ processë¥¼ ê±°ì¹œ yê°’ì€ ë‹¤ì‹œ ë‹¤ë¥¸ ë‰´ëŸ°ë“¤ì˜ synapseì˜ inputìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‚¬ëŒì˜ ë‰´ëŸ°ì˜ êµ¬ì¡°ë¥¼ ëª¨ë°©í•´ì„œ ì¸ê³µì‹ ê²½ë§ì„ êµ¬ì„±í•˜ê²Œ ë˜ë©´, ê° neuronì€ nodeê°€ ë˜ê³  synapseë¥¼ í†µí•´ì„œ ë“¤ì–´ì˜¤ëŠ” signalì€ inputì´ ë˜ê³  ê°ê° ë‹¤ë¥¸ synapseë¥¼ í†µí•´ì„œ ë“¤ì–´ì˜¤ëŠ” signalë“¤ì˜ ì¤‘ìš”ë„ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ weightë¥¼ ê³±í•´ì¤˜ì„œ ë“¤ì–´ì˜¤ê²Œ ë©ë‹ˆë‹¤.

  ì´ signalë“¤ì´ weightì™€ ê³±í•´ì§„ ê²ƒì´ ë°”ë¡œ net input signal(ì•Œì§œ ì…ë ¥ ì‹ í˜¸)ì…ë‹ˆë‹¤. ê·¸ net input signalì„ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    > Net input signal received through synaptic junctions is
      net = b + Î£ w_i * x_i = b + W^T * x

      Wieght vector : W = [w_1, w_2, ... , w_m]'

      Input vector  : X = [x_1, x_2, ... , x_m]'

      Each output is a function of the net stimulus signal(f is called the activation function)

        y = f(net) = f(b + W^T * x)

  ì‹œëƒ…ìŠ¤ë¡œ ë“¤ì–´ì˜¤ëŠ” ê°ê°ì˜ inputì„ vectorë¡œ í‘œí˜„í•˜ê³  ê·¸ inputì— ê°ê° ê³±í•´ì§€ëŠ” weight ë˜í•œ ê·¸ì— ë”°ë¼ vectorë¡œ ë§Œë“¤ì–´ì„œ ë‘ vectorë¥¼ ê³±í•´ì„œ inputê³¼ weightì˜ linear combinationì„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìƒˆë¡œìš´ ê°œë…ì´ ë‚˜íƒ€ë‚˜ëŠ”ë°, bë¡œ ì¨ì§€ëŠ” biasì…ë‹ˆë‹¤.

  Biasê°€ linear combinationì— ë”í•´ì ¸ì„œ net input signalë¡œ ë“¤ì–´ê°€ëŠ” ì´ìœ ëŠ” ê°„ë‹¨í•˜ê²Œ ë§í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì¢Œí‘œí‰ë©´ì—ì„œ (0,0)ê³¼ (5,5)ë¥¼ ì–´ë– í•œ ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„í•˜ê³  ì‹¶ë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤.

  biasê°€ ì—†ëŠ” y = axê°™ì€ í•¨ìˆ˜ì˜ ê²½ìš°ì—ëŠ” ë‘ ì ì„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì „í˜€ ì—†ìŠµë‹ˆë‹¤. (0,0)ì´ ì§ì„  ìœ„ì— ìˆê¸° ë•Œë¬¸ì´ì£ . í•˜ì§€ë§Œ, y = ax + bì˜ í•¨ìˆ˜ëŠ” ì´ ë‘ì ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ë˜í•œ ë‹¤ë¥¸ ì‹ìœ¼ë¡œ biasì˜ í•„ìš”ì„±ì„ ì„¤ëª…í•œ stackoverflowì˜ ëŒ“ê¸€ì´ ìˆìŠµë‹ˆë‹¤.

    > Role of bias in neural networks
      Modification of neuron WEIGHTS alone only serves to manipulate the shape/curvature of your transfer function,
      and not its equilibrium/zero crossing points.

      The introduction of BIAS neurons allows you to shift the transfer function curve horizontally (left/right) along
      the input axis while leaving the shape/curvature unaltered.

      This will allow the network to produce arbitrary outputs differnet from the defaults and hence you can
      customize/shift the input to output mapping to suit your particular needs.

  ì¦‰, ë…¸ë“œë¡œ ë“¤ì–´ê°€ëŠ” inputë“¤ì— ê³±í•´ì§€ëŠ” weight(í•™ìŠµì‹œí‚¤ë ¤ëŠ” ëŒ€ìƒ)ì„ ë³€í™”ì‹œí‚¤ë©´ í•¨ìˆ˜ì˜ ëª¨ì–‘ë§Œ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆì§€, ì™¼ìª½/ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ì‹œì¼œì„œ 0ì´ ë˜ëŠ” pointë¥¼ ë³€í˜• ì‹œí‚¬ìˆ˜ëŠ” ì—†ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ë”°ë¼ì„œ biasë¥¼ ì‚¬ìš©í•˜ë©´ ìš”êµ¬ì— ë”°ë¼ ê·¸ë˜í”„ë¥¼ ì´ë™ ë° ë³€í˜•ì‹œì¼œì„œ í•™ìŠµí•  ìˆ˜ê°€ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

  Input signalë“¤ê³¼ weightì´ ê³±í•´ì§€ê³  biasê°€ ë”í•´ì§„ net input signalì´ nodeë¥¼ activateì‹œí‚¤ëŠ”ë° ê·¸ í˜•ì‹ì„ functionìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬í•œ í•¨ìˆ˜ë¥¼ activation functionì´ë¼ í•©ë‹ˆë‹¤.

  ìœ„ ìœ„ì˜ ì‹ì—ì„œ fë¡œ í‘œí˜„í•œ activation functionì˜ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœëŠ” ë“¤ì–´ì˜¨ inputë“¤ì˜ í•©ì´ ì–´ë–¤ thresholdë³´ë‹¤ ë†’ìœ¼ë©´ 1ì´ ë‚˜ì˜¤ê³  ë‚®ìœ¼ë©´ 0ì´ ë‚˜ì˜¤ëŠ” í˜•íƒœì¼ ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ° í˜•íƒœì˜ activation functionì˜ ê²½ìš°ì—ëŠ” ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥ í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ´ ê²½ìš° gradient descentë¥¼ í†µí•œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì§€ ëª» í•˜ê¸° ë•Œë¬¸ì—, ê·¸ ì´ì™¸ì˜ ë¯¸ë¶„ê°€ëŠ¥í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

  ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê°„ë‹¨í•œ activation functionì€ sigmoid functionì…ë‹ˆë‹¤. sigmoid functionì´ë€ ë¬´ì—‡ì¼ê¹Œìš”? f(x) = 1 / (1 + e^-ax)ë¡œ í‘œí˜„ë˜ë©°, [0 1]ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

  activation functionì˜ ì˜ˆì‹œì—ëŠ” sigmoid ë§ê³ ë„ ì„¸ ê°€ì§€ ë‹¤ë¥¸ í•¨ìˆ˜ë“¤ì´ ìˆëŠ”ë° ì´ í•¨ìˆ˜ë“¤ì€ ëª¨ë‘ non-linearí•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” activation functionì´ linearí•  ê²½ìš°ì—ëŠ” ì•„ë¬´ë¦¬ ë§ì€ neuron layerë¥¼ ìŒ“ëŠ”ë‹¤ í•˜ë”ë¼ë„ ê·¸ê²ƒì´ ê²°êµ­ í•˜ë‚˜ì˜ layerë¡œ í‘œí˜„ì´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

  - sigmoid function
  - tanh function
  - absolute function
  - ReLU function

  ê°€ì¥ ì‹¤ìš©ì ì¸ activation functionì€ ReLU functionì´ë¼ê³ ë“¤ í•©ë‹ˆë‹¤. ReLUë€ ì–´ë–¤ í•¨ìˆ˜ì¼ê¹Œìš”?

  ReLU í•¨ìˆ˜ëŠ” xê°€ 0ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì„ë•ŒëŠ” yê°€ 0ì´ ë‚˜ì˜¤ê³ , xê°€ 0ë³´ë‹¤ í´ ê²½ìš°ì—ëŠ” y = xì˜ í˜•íƒœë¥¼ ë„ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

  ì‚¬ì‹¤ ë”¥ëŸ¬ë‹ì´ ìµœê·¼ì— ê°‘ìê¸° ê¸‰ ë¶€ìƒí•œ ê²ƒì€ ì—„ì²­ë‚˜ê²Œ í˜ì‹ ì ì¸ ë³€í™”ê°€ ìˆì—ˆë‹¤ê¸° ë³´ë‹¤ëŠ” Computation timeì˜ ê°ì†Œì™€ ë”ë¶ˆì–´ activation functionì„ sigmoidì—ì„œ ReLUë¡œ ë°”ê¾¸ëŠ” ë“±ì˜ ì‘ì€ ë³€í™”ë“¤ì´ ì¤‘ì²©ë˜ë©° ì¼ì–´ë‚¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

  sigmoid í•¨ìˆ˜ì— ë¹„í•´ ReLUê°€ ê°€ì§€ê³  ìˆëŠ” ì¥ì ì€ ì–´ë–¤ ê²ƒì´ ìˆëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ReLUì˜ ì§ì„ ì ì¸ í˜•íƒœì™€ sigmoidí•¨ìˆ˜ ì²˜ëŸ¼ ìˆ˜ë ´í•˜ëŠ” í˜•íƒœê°€ ì•„ë‹Œ ì ì´ ReLUì˜ stochastic gradient descentê°€ ë” ì˜ ìˆ˜ë ´í•˜ê²Œ í•´ì¤ë‹ˆë‹¤. ë˜í•œ ìƒëŒ€ì ìœ¼ë¡œ sigmoidí•¨ìˆ˜ì— ë¹„í•´ì„œ ê³„ì‚°ëŸ‰ì´ ì¤„ì–´ë“ ë‹¤ëŠ” ì¥ì ë„ ìˆìŠµë‹ˆë‹¤.

  ì¥ì ì´ ìˆìœ¼ë©´ ë‹¨ì ë„ ìˆëŠ” ë²•ì…ë‹ˆë‹¤. ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. Learning rateì— ë”°ë¼ì„œ ì¤‘ê°„ì— ìµœëŒ€ 40%ì˜ networkê°€ "die"í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ë‹¨, learning rateë¥¼ ì˜ ì¡°ì ˆí•˜ë©´ ì´ ë¬¸ì œëŠ” ê·¸ë ‡ê²Œ í¬ì§€ ì•Šë‹¤ê³  í•©ë‹ˆë‹¤.

***

## 3. Stochastic Gradient Descent(SGD) and Back-Propagation

### (1) SGD

  ì§€ê¸ˆê¹Œì§€ëŠ” deep neural networkê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì´ê¸€ì˜ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ì„œ DQNì´ë€ action-value functionì„ deep neural networkë¡œ approximation í•œ ê²ƒì„ ë§í•©ë‹ˆë‹¤. ê°•í™”í•™ìŠµì˜ ëª©í‘œëŠ” optimal policyë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ê³  ê° stateì—ì„œ optimalí•œ action value fucntionì„ ì•Œê³  ìˆìœ¼ë©´ q ê°’ì´ í° actionì„ ì·¨í•˜ë©´ ë˜ëŠ” ê²ƒì´ë¯€ë¡œ ê²°êµ­ì€ q-valueë¥¼ êµ¬í•˜ë©´ ê°•í™”í•™ìŠµ ë¬¸ì œë¥¼ í’€ê²Œ ë©ë‹ˆë‹¤.

  ì´ q-valueëŠ” DNN(deep neural networks)ë¥¼ í†µí•´ì„œ ë‚˜ì˜¤ê²Œ ë˜ëŠ”ë° ê²°êµ­ DNNì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ëª©í‘œê°€ ë˜ê²Œ ë©ë‹ˆë‹¤.

  ë”°ë¼ì„œ approximationí•˜ì§€ ì•Šì•˜ì„ ë•Œì™€ ë‹¤ë¥¸ ê²ƒì€ q-tableì„ ë§Œë“¤ì–´ì„œ ê°ê°ì˜ q-valueë¥¼ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  DNNì•ˆì˜ weightì™€ biasë¥¼ updateí•˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ updateí• ê¹Œìš”?

  ì´ ë•Œ ì´ì „ì— ë°°ì› ë˜ Stochastic Gradient Descentê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. ì •ë¦¬í•˜ìë©´ graidnet descentë¼ëŠ” ê²ƒì€ wë¥¼ parameterë¡œ ê°€ì§€ëŠ” Jë¼ëŠ” objective funcitonì„ minimize í•˜ëŠ” ë°©ë²•ì¤‘ì˜ í•˜ë‚˜ë¡œì¨ wì— ëŒ€í•œ Jì˜ gradient ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ wë¥¼ updateí•˜ëŠ” ë°©ì‹ì„ ë§í•©ë‹ˆë‹¤.

    > Gradient descent
        âˆ†w = -(1/2) * ğ›¼ * âˆ‡w J(w)
           = ğ›¼ * E_ğœ‹[{v_ğœ‹(s)-vhat(s,w)} * âˆ‡w vhat(s,w)]

      Stochastic Gradient descent samples the gradient
        âˆ†w = ğ›¼ * {v_ğœ‹(s)-vhat(s,w)} * âˆ‡w vhat(s,w)

  ì´ëŸ° ì‹ìœ¼ë¡œ updateë¥¼ í•˜ê²Œ ë˜ëŠ”ë° ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ì„œ gardientë¥¼ êµ¬í•´ì„œ í•œ ë²ˆ updatí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  Samplingì„ í†µí•´ì„œ ìˆœì°¨ì ìœ¼ë¡œ updateë¥¼ í•˜ê² ë‹¤ëŠ” gradient descent ë°©ë²•ì´ stochastic gradient descentì…ë‹ˆë‹¤.

  ì•„ë˜ í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ë³´ë©´ ê·¸ë ‡ê²Œ í•  ê²½ìš° ìˆ˜ë ´í•˜ëŠ” ì†ë„ê°€ í›¨ì”¬ ë¹ ë¥´ë©° onlineìœ¼ë¡œë„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ í•˜ë‚˜ ì¤‘ìš”í•œ ì ì€ gradient descentë°©ë²•ì´ local optimumìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆëŠ” ìµœì í™” ë°©ë²•ì´ë¼ëŠ” ê²ƒì„ ê¸°ì–µ í•´ì•¼ í•©ë‹ˆë‹¤.

### (1) Back-Propagation

  Gradientë¥¼ êµ¬í–ˆë‹¤ë©´, DNNì˜ ì•ˆì— ìˆëŠ” parameterë“¤ì„ ì–´ë–»ê²Œ updateí• ê¹Œìš”? ë‹¤ì‹œ DNNì•ˆì—ì„œ dataê°€ ì „ë‹¬ë˜ì–´ê°€ëŠ” ê³¼ì •ì„ ìƒê°í•´ë´…ì‹œë‹¤. inputì´ ë“¤ì–´ê°€ë©´ layerë“¤ì„ ê±°ì³ê°€ë©° output layerì— ë„ë‹¬í•œ dataê°€ outputì´ ë˜ì–´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.

  parameterë¥¼ SGDë¡œ updateí•  ë•Œì—ëŠ” ê·¸ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê°€ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ê·¸ ì´ë¦„ì´ Back-Propagationì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ìŠµë‹ˆë‹¤. Tensorflowë“±ì˜ Deep learning frameworkë“¤ì„ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” ê·¸ëŸ¬í•œ ì‹ë“¤ì´ libraryí™” ë˜ì–´ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

***

# Deep Q Networks

  ì²« ë²ˆì§¸ chapterì—ì„œ Atari gameì˜ í•™ìŠµì— ëŒ€í•´ì„œ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì œëŠ” Playing atari with deep reinforcement learningì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ë§í¬ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ê°•í™”í•™ìŠµ + ë”¥ëŸ¬ë‹ìœ¼ë¡œ atarië¼ëŠ” ê³ ì „ ê²Œì„ì„ í•™ìŠµì‹œí‚´ìœ¼ë¡œì¨ deep reinforcement learningì˜ ì‹œëŒ€ë¥¼ ì—´ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤.
  https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf

  ì´ ë…¼ë¬¸ì˜ abstractëŠ” ë‹¤ìŒê³¼ ê°™ìœ¼ë©° ê°™ì´ ì½ì–´ ë´…ì‹œë‹¤.

    > We present the first deep learning model to successfully learn control policies directly from high-dimensional
    sensory input using reinforcement learning.

    The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and
    whose output is a value function estimating future rewards.

    We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the
    architecture or learning algorithm.

    We find that it outperforms all previous approaches on six of the games and surpasses a human expert on
    three of them.

  ì´ ë…¼ë¬¸ì˜ ì£¼ëª©í•  ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

  1. input dataë¡œ law pixelì„ ë°›ì•„ì˜¨ ì 
  2. ê°™ì€ agentë¡œ ì—¬ëŸ¬ ê°œì˜ ê²Œì„ì— ì ìš©ë˜ì–´ì„œ í•™ìŠµì´ ëœë‹¤ëŠ” ì 
  3. Convolutional neural networkë¥¼ function approximatorë¡œ ì‚¬ìš©í•œ ì 
  4. Experience Replay

  Deep Q Networkë¼ëŠ” ê°œë…ì´ ì—¬ê¸°ì„œ ì²˜ìŒ ì†Œê°œë˜ì—ˆëŠ”ë° ì•„ë˜ì™€ ê°™ì´ action value functionì„ approximate í•˜ëŠ” modelë¡œ deep learningì˜ modelì„ ë„ì…í–ˆëŠ”ë° ê·¸ ì¤‘ì—ì„œ convolutional networkë¥¼ ë„ì…í•´ì„œ networkë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì´ DQNì´ë¼ê³  ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

    > We refer to convolutional networks trained with our approach as Deep Q-Networks (DQN).

  Convolutional neural networks (CNN)ì€ ìµœê·¼ ë”¥ëŸ¬ë‹ ì—´í’ì„ ëª°ê³ ì˜¨ ì¥ë³¸ì¸ìœ¼ë¡œì¨ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•˜ê±°ë‚˜ ì‹œê³„ì—´ì„ í•™ìŠµì‹œí‚¤ëŠ”ë° ìµœì í™” ëœ Neural Network ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ í™”ë©´ ê²Œì„ í”½ì…€ ë°ì´í„° ê·¸ ìì²´ë¡œ í•™ìŠµì„ ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë”°ë¡œ ê²Œì„ë§ˆë‹¤ agent ì„¤ì •ì„ ë‹¬ë¦¬ í•´ì£¼ì§€ ì•Šì•„ë„ ì—¬ëŸ¬ ê²Œì„ì— ëŒ€í•´ í•˜ë‚˜ì˜ agentë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

  Neural Networkì— ë“¤ì–´ê°€ëŠ” input dataì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì–¸ê¸‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

    > Working directly with raw Atari frames, which are 210 * 160 pixel images with a 128 color palett, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality.
    The raw frames are preprocessed by first converting their RGB representation to gray-scale and down-sampling it to a 110 * 84 image.
    The final input representation is obtained by cropping an 84 * 84 region of the image that roughly captures the playing area

  ì´ ë‹¨ê³„ë¥¼ Deep mind íŒ€ì€ "Preprocessing"ì´ë¼ê³  í–ˆìŠµë‹ˆë‹¤. CNNìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•˜ê¸° ì „ì— ê²Œì„ì˜ í™”ë©´ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™”ì‹œì¼œ ì£¼ëŠ” ê²ƒìœ¼ë¡œì¨ ì¼ë‹¨ ìƒ‰ì„ ì—†ì• ê³ , ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ìœ„ ì•„ë˜ì˜ ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ì—†ì• ì£¼ë©° ì •ì‚¬ê°í˜•ì˜ ì´ë¯¸ì§€ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ë¯¸ì§€ë¥¼ 4ê°œì”© ë¬¶ì–´ì„œ CNNìœ¼ë¡œ ì§‘ì–´ë„£ê²Œ ë©ë‹ˆë‹¤.
  https://www.nervanasys.com/demystifying-deep-reinforcement-learning/

  stateê°€ ê°‘ìê¸° pixel dataê°€ ë˜ì–´ì„œ í—·ê°ˆë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ agentì˜ ì…ì¥ì—ì„œëŠ” ë‹¨ì§€ dataì˜ í˜•íƒœê°€ ë°”ë€Œì—ˆì„ ë¿ì´ê³  í™”ë©´ì„ í•˜ë‚˜ì˜ ìƒíƒœë¡œ ì¸ì‹í•˜ì—¬ ê·¸ ìƒíƒœì—ì„œ ì–´ë–¤ í–‰ë™ì„ í–ˆì„ ë•Œì˜ rewardë¥¼ ê¸°ì–µí•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

  Deep mindì˜ ê²½ìš° Chapter 8ì—ì„œ ì–¸ê¸ˆí–ˆë˜ experience replayë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬í•˜ì—¬ transition dataë“¤ì„ replay memoryì— ë„£ì–´ ë†“ê³  ë§¤ time stepë§ˆë‹¤ mini-batchë¥¼ ëœë¤ìœ¼ë¡œ memoryì—ì„œ êº¼ë‚´ì„œ updateë¥¼ í•©ë‹ˆë‹¤. learning ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” q-learningì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

  Replay memory : <S, A, R, S'>ì˜ transition batchë¥¼ ì¶œë ¥ìœ¼ë¡œ ë‚´ë³´ë‚´ì„œ Agentë¥¼update after each step

  Agent : Replay memoryë¡œë¶€í„° transition batchë¥¼ ì…ë ¥ë°›ê³ , Deep Q-Networkì—ì„œ Q-valueë¥¼ í†µí•´ Actionì„ Environmentì— í–‰í•¨.

  Environment : Agentë¡œë¶€í„° actionì„ ë°›ê³  Succesor stateì™€ Rewardë“±ì˜ ì •ë³´ë¥¼ Replay memoryë¡œ ë³´ë‚¸ë‹¤.

    > Algorithm of Deep Q - learning with Experience Replay

      Initialize replay memory D to capacity N
      Initialize action-value function Q with random weights

      for episode = 1 to M do
        Initialize sequence s_1 = {x_1} and preprocessed sequenced ğœ™_1 = ğœ™(s_1)

        for t = 1 to T do

          With probability epsilon select a ronadom action A_totherwise

          select a_t = {a}max [Q*(ğœ™(s_t), a; ğœƒ)]

          Excute action a_t in emulator and observe reward r_t and image x_(t+1)

          Set s_(t+1) = s_t, a_t, x_(t91) and preprocess ğœ™_(t+1) = ğœ™(s_(t+1))

          Store transition (ğœ™_t, a_t, r_t, ğœ™_(t+1)) in D

          Sample random minibatech of transition (ğœ™_j, a_j, r_j, ğœ™_(j+1)) from D
                      r_j                                 for terminal ğœ™_(j+1)
          Set y_j =
                      r_j + ğ›¾ * {a}max [Q*(ğœ™_j+1 a'; ğœƒ)]  for non-terminal ğœ™_(j+1)
          Perform a gradient descent step on (y_j - Q())^2
        end
      end

  Replay memoryëŠ” Nê°œì˜ episodeë¥¼ ê¸°ì–µí•˜ê³  ìˆì„ ìˆ˜ ìˆëŠ”ë° Nê°œê°€ ë„˜ì–´ê°€ë©´ ì˜¤ë˜ëœ episodeë¶€í„° ëºë‹ˆë‹¤.

  episodeë§ˆë‹¤ ì–´ë–»ê²Œ updatí• ê¹Œìš”? loss functionì„ ì •ì˜í•˜ê³  ê·¸ gradientë¥¼ ë”°ë¼ì„œ updateí•©ë‹ˆë‹¤. mini-batch dataì— ëŒ€í•´ì„œ bootstrapìœ¼ë¡œ q-learningì´ í–ˆì—ˆë˜ ê²ƒ ì²˜ëŸ¼ r + {a}max [Q*(s', a')]ì„ í˜„ì¬ Qê°€ updateê°€ ë˜ì–´ì•¼ í•  targetìœ¼ë¡œ ì¡ê³  ê·¸ errorë¥¼ quardraticí•˜ê²Œ ì¡ê³ ì„œ gradientë¥¼ ì·¨í•˜ë©´ ì•„ë˜ê³¼ ê°™ìŠµë‹ˆë‹¤.

  ~~~~~~ ì´ë¯¸ì§€ ì§ì ‘ë´ë¼ ê°œë¹¡ì¹œë‹¤ ~~~~~~~

  ì´ ë°©ë²•ì€ chapter8ì—ì„œ ë°°ì› ë˜ ë‚´ìš©ì„ í™œìš©í•œ ê²ƒìœ¼ë¡œì¨ ë‹¬ë¼ì§€ëŠ” ê²ƒì€ ~~~ë¥¼ ì–´ë–»ê²Œ êµ¬í•˜ëƒ ì…ë‹ˆë‹¤. ì‚¬ì‹¤ì€ ì´ ë¶€ë¶„ì€ ë”¥ëŸ¬ë‹ì— ëŒ€í•´ì„œ ê¹Šê²Œ ë“¤ì–´ê°€ì•¼ í•œëŠ ë¶€ë¶„ì¸ë° tensorflowê°™ì€ libraryë“¤ì´ ì˜ ë˜ì–´ ìˆì–´ì„œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ì•Œì•„ì„œ ê³„ì‚°í•´ ì¤ë‹ˆë‹¤.




















asdf
