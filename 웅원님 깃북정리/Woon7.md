/*ì´ì›…ì›ë‹˜ Git*/

# Temporal Difference Learning

  - 5~7ì¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

# TD Prediction

## 1. Tmeporal Difference

  ì´ì „ Chapterì—ì„œ ë°°ì› ë˜ Monte-Carlo Controlì€ Model-Free Controlì…ë‹ˆë‹¤. Model-Freeë¼ëŠ” ì ì— ê°•í™”í•™ìŠµì´ì§€ë§Œ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ onlineìœ¼ë¡œ ë°”ë¡œë°”ë¡œ í•™ìŠµí•  ìˆ˜ê°€ ì—†ê³  ê¼­ ëë‚˜ëŠ” episodeì—¬ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
  ëë‚˜ì§€ ì•Šë”ë¼ë„ eipsodeê°€ ê¸¸ ê²½ìš°ì—ëŠ”(ì˜ˆë¥¼ ë“¤ì–´ atari ê²Œì„ì´ ì•„ë‹ˆë¼ starcraftê°™ì€ ì—ê¹€) í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìì—°ìŠ¤ëŸ½ê²Œë„ ê¼­ episodeê°€ ëë‚˜ì§€ ì•Šë”ë¼ë„ DPì²˜ëŸ¼ time stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆì§€ ì•Šë‚˜?ë¼ëŠ” ìƒê°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ê²Œ ë°”ë¡œ Temporal Differenceì´ë©° Suttonêµìˆ˜ë‹˜ ì±…ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

    > Temporal Difference method
      If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.
      TD learning is a combination of Monte-Carlo ideas and dynamic programming (DP) ideas.
      Like Monte-Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics.
      Like DP, TD methods update estimates based in part on other learned estimates, without wating for a final outcome (they bootstrap)

  Temporal difference(TD)ëŠ” MCì™€ DPë¥¼ ì„ì€ ê²ƒìœ¼ë¡œì¨ MCì²˜ëŸ¼ raw experienceë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ DPì²˜ëŸ¼ time stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ì— "bootstrap"ì´ë¼ê³  í•˜ëŠ”ë° ì´ ë§ì€ ë¬´ì—‡ì„ ëœ»í• ê¹Œìš”?

  ëŒ€í•™ìƒí™œì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. MCëŠ” ëŒ€í•™êµì— ë“¤ì–´ì™€ì„œ ì¡¸ì—…ì„ í•œ ë‹¤ìŒì— ê·¸ ë™ì•ˆì„ ëŒì•„ë³´ë©° "ì´ê±´ ë” í–ˆì–´ì•¼ í–ˆê³  ìˆ ì€ ëœ ë§ˆì…”ì•¼í–ˆë‹¤"ë¼ê³  ìƒê°í•˜ë©° ë‹¤ì‹œ ëŒ€í•™êµë¥¼ ë“¤ì–´ê°€ì„œ ëŒ€í•™ìƒí™œì„ í•˜ë©´ì„œ ì¡¸ì—…í•  ë•Œê¹Œì§€ ë˜‘ê°™ì´ ì‚´ë‹¤ê°€ ë‹¤ì‹œ ì¡¸ì—…í•˜ê³  ìì‹ ì„ ëŒì•„ë³´ëŠ” ë°˜ë©´ì— TDê°™ì€ ê²½ìš°ëŠ” ê°™ì€ ëŒ€í•™êµë¥¼ ë‹¤ë‹ˆê³  ìˆëŠ” 2í•™ë…„ ì„ ë°°ê°€ 1í•™ë…„ì„ ì´ëŒì–´ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

  ì‚¬ì‹¤ì€ ë‘˜ ë‹¤ ì¡¸ì—…ì„ í•´ë³´ì§€ ì•Šì€ ìƒíƒœì—ì„œ (ì˜ ëª¨ë¥´ëŠ” ìƒí™©ì—ì„œ)ì´ëŒì–´ ì£¼ëŠ” ê²ƒì´ì§€ë§Œ ëŒ€í•™êµë¥¼ ë‹¤ë‹ˆë©´ì„œ ë°”ë¡œ ë°”ë¡œ ìì‹ ì„ ê³ ì³ë‚˜ê°€ê¸° ë•Œë¬¸ì— ì–´ì©Œë©´ ë” ì˜³ì€ ë°©ë²•ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤.

  TDëŠ” ë”°ë¼ì„œ í˜„ì¬ì˜ value functionì„ ê³„ì‚°í•˜ëŠ”ë° ì•ì„ (ì•ì„ ì´ë¼ê³  í‘œí˜„í•˜ê¸°ì—ëŠ” ì¡°ê¸ˆ ì• ë§¤í•œ ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤.) ì£¼ë³€ stateë“¤ì˜ value functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ê²ƒì€ ì´ì „ì— ë°°ì› ë˜ Bellman equationì´ë©°, ë”°ë¼ì„œ Bellman equationìì²´ê°€ Bootstrapí•˜ëŠ” ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

## 2. TD(0)

  Temporal Difference(TD)ëŠ” Monte-Carlo + DPë¼ê³  ë§í–ˆì—ˆìŠµë‹ˆë‹¤. ì´ì „ì— ë´¤ë˜ Monte-Carlo predictionì—ì„œ incremental meanì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ returnì„ ì‚¬ìš©í•´ì„œ updateí•©ë‹ˆë‹¤. TDì—ì„œëŠ” ì´ G_të¥¼ R_(t+1) + ğ›¾ * V_(t+1)(s)ë¡œ ë°”ê¿”ì„œ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ë©ë‹ˆë‹¤.
  Temporal Difference learning ë°©ë²•ì—ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë° ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ TD(0)ì´ê³  ë°©ê¸ˆ ë§í•œ ë°©ë²•ì´ ë°”ë¡œ TD(0)ì…ë‹ˆë‹¤.
  R_(t+1) + ğ›¾ * V_(t+1)(s)ë¥¼ TD targetì´ë¼ê³  ë¶€ë¥´ê³  ê·¸ targetê³¼ í˜„ì¬ì˜ value functionê³¼ì˜ ì°¨ì´ë¥¼ TD errorë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

    > Temporal Differenc TD(0)
      Goal : leaern v_ğœ‹ online from experience under policy ğœ‹
      Incremental every-visit Monte-Carlo
        Update value V(S_t) toward actual return G_t
          V(S_t) <- V(S_t) + ğ›¼ * (G_t - V(S_t))

      Simplest temporal-difference learning algorithm : __TD(0)__ !
        Update value V(S_t) toward estimated return R_(t+1) + ğ›¾ * V_(t+1)(s)
          V(S_t) <- V(S_t) + ğ›¼ * (R_(t+1) + ğ›¾ * V_(t+1)(s) - V(S_t))

      R_(t+1) + ğ›¾ * V_(t+1)(s) is called the TD target
      ğ›¿_t = R_(t+1) + ğ›¾ * V_(t+1)(s) - V(S_t) is called the TD error

      => V_(S_t) = V(S_t) + ğ›¼ * ğ›¿_t

  TD(0)ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê³  backup diagramì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

    > TD(0) algorithm
      Input : the policy ğœ‹ to be evaluted
      Initialize V(s) arbitrarily (e.g., V(s) = 0, âˆ€s âˆˆ S+)
      Repeat (for each episode):
        A <- action given by ğœ‹ for S
        Take action A; observe reward, R, and next state, S'
        V(S) <- V(S) + alpha[R + ğ›¾ * V(S') - V(S)]
        S <- S'
      until S is terminal

***

## 3. Monte-Carlo method vs Temporal Difference method

  MCì˜ ê²½ìš°ì—ëŠ” ë‹¤ ë„ì°©í•œ ë‹¤ìŒì— ê°ê°ì˜ stateì—ì„œ ì˜ˆì¸¡í–ˆë˜ value functionê³¼ ì‹¤ì œë¡œ ë°›ì€ returenì„ ë¹„êµí•´ì„œ updateí•˜ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ TDì—ì„œëŠ” í•œ step ì§„í–‰ì„ í•˜ë©´ ì•„ì§ ë„ì°©ì„ í•˜ì§€ ì•Šì•„ì„œ ì–¼ë§ˆë‚˜ ê±¸ë¦´ì§€ëŠ” ì •í™•íˆ ëª¨ë¥´ì§€ë§Œ í•œ step ë™ì•ˆ ì§€ë‚¬ë˜ ì‹œê°„ì„ í† ëŒ€ë¡œ value functionì„ updateí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œë¡œ ë„ì°©ì„ í•˜ì§€ ì•Šì•„ë„, final outcomeì„ ëª¨ë¥´ë”ë¼ë„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê²ƒì´ TDì˜ ì¥ì ì´ë©° ë§¤ stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒë„ ì¥ì ì…ë‹ˆë‹¤.

    > MC vs TD
      TD can learn before knowing the final outcome
        TD can learn online after every step
        MC must wait until end of episode before return is knwon

      TD can learn without the final outcome
        TD can learn from incomplete sequences
        MC can only learn from complete sequences
        TD works in continuing (non-terminating) environments
        MC only works for episodic (terminating) environments

  - Bias / Variation Trade-Off

  ë˜ í•œ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ì ì€ ë°”ë¡œ biasì™€ varianceì…ë‹ˆë‹¤.
  bias : ë°ì´í„°ê°€ ì‹¤ì œ ê°’ìœ¼ë¡œë¶€í„° ì „ì²´ì ìœ¼ë¡œ ë§ì´ ë²—ì–´ë‚˜ê²Œ ë˜ë©´ biasê°€ ë†’ë‹¤ í˜¹ì€ biasedë¬ë‹¤ê³  í‘œí˜„í•©ë‹ˆë‹¤.
  variance : ì‹¤ì œ ê°’ê³¼ ìƒê´€ì—†ì´, ì „ì²´ì ìœ¼ë¡œ ë°ì´í„°ê°€ ë§ì´ í¼ì ¸ìˆëŠ” ê²½ìš° varianceê°€ ë†’ë‹¤ê³  í•©ë‹ˆë‹¤.

  ë‘˜ ë‹¤ ë‚®ìœ¼ë©´ ì¢‹ê² ì§€ë§Œ ë³´í†µì€ ì´ ë‘˜ì´ Trade-off ê´€ê³„ì— ìˆì–´ì„œ í•˜ë‚˜ê°€ ë‚®ì•„ì§€ë©´ í•˜ë‚˜ê°€ ë†’ì•„ì§€ëŠ” ê´€ê³„ì— ìˆìŠµë‹ˆë‹¤. TDëŠ” biasê°€ ë†’ê³  MCëŠ” varianceê°€ ë†’ìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ í•™ìŠµì— ë°©í•´ê°€ ë˜ëŠ” ìš”ì†Œì…ë‹ˆë‹¤.

  TDëŠ” í•œ episodeì•ˆì—ì„œ ê³„ì† ì—…ë°ì´íŠ¸ë¥¼ í•˜ëŠ”ë° ë³´í†µì€ ê·¸ ì „ì˜ ìƒíƒœê°€ ê·¸ í›„ì˜ ìƒíƒœì— ì˜í–¥ì„ ë§ì´ ì£¼ê¸° ë•Œë¬¸ì— í•™ìŠµì´ í•œ ìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì§€ê²Œ ë©ë‹ˆë‹¤. ê³„ì† ê°™ì€ ë¶„ì•¼ ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°ë¥¼ í•˜ë©´ ìƒê°ì˜ í­ì´ ì¢ì•„ì§€ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ê²ƒì„ ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ìƒê°ì´ í•œ ìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ì§€ìš” ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ì‚¬ëŒë“¤ì˜ ì˜ê²¬ì„ ë“¤ì–´ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

  í•˜ì§€ë§Œ ë„ˆë¬´ ì—¬ëŸ¬ì‚¬ëŒì˜ ì˜ê²¬ì„ ë“£ë‹¤ë³´ë©´ ì´ë„ ì €ë„ ì•„ë‹ˆê²Œ ë˜ì„œ ê²°ì •ì„ ëª»í•˜ê²Œ ë˜ê³¤ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‹¤ì œ ê²½í—˜ë“¤ì´ ì‚¬ì‹¤ bias/variance trade-offì™€ ê´€ë ¨ì´ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. MCê°€ varianceê°€ ë†’ì€ ì´ìœ ëŠ” ì•ì—ì„œë„ ì„¤ëª…í–ˆì—ˆì§€ë§Œ ì—í”¼ì†Œë“œë§ˆë‹¤ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì²˜ìŒì— ì–´ë–»ê²Œ í–ˆëŠëƒì— ë”°ë¼ ì „í˜€ ë‹¤ë¥¸ experienceë¥¼ ê°€ì§ˆ ìˆ˜ê°€ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

    > TD methods
      TD target R_(t+1) + ğ›¾ * V_(t+1)(s) is biased estimate of v_ğœ‹(S_t)
      TD target is much lower variance than the return :
        Return depends on many random actions, transitions, rewards
        TD target depends on one random action, transition, reward

  ì•ìœ¼ë¡œë„ Biasì™€ VarianceëŠ” ë¨¸ë¦¬ì†ì— ê¸°ì–µí•´ë‘ì–´ì•¼ í•  í•„ìš”ê°€ ìˆëŠ” ê²ƒì´, ê°•í™”í•™ìŠµì„ ë°œì „ì‹œí‚¤ë ¤ëŠ” ë…¸ë ¥ë“¤ì´ ì•Œê³ ë¦¬ì¦˜ì˜ ê·¼ë³¸ì ì¸ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ëŠ” ê²ƒì—ë„ ìˆì§€ë§Œ, ê·¸ ì•Œê³ ë¦¬ì¦˜ì˜ biasì™€ varianceë¥¼ ë‚®ì¶”ë ¤ëŠ” ê²ƒì— ì§‘ì¤‘ë˜ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

  ***

# TD Control

## 1. Sansa

  TD(0)ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    > TD(0) algorithm
      Input : the policy ğœ‹ to be evaluted
      Initialize V(s) arbitrarily (e.g., V(s) = 0, âˆ€s âˆˆ S+)
      Repeat (for each episode):
        A <- action given by ğœ‹ for S
        Take action A; observe reward, R, and next state, S'
        V(S) <- V(S) + alpha[R + ğ›¾ * V(S') - V(S)]
        S <- S'
      until S is terminal

  í•˜ì§€ë§Œ model-free controlì´ ë˜ê¸° ìœ„í•´ì„œëŠ” action-value functionì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  ë§í–ˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìœ„ì˜ TD(0)ì˜ ì‹ì—ì„œ value functionì„ action-value functionìœ¼ë¡œ ë°”ê¾¸ì–´ì£¼ë©´ Sarsaê°€ ë©ë‹ˆë‹¤. SarsaëŠ” í˜„ì¬ state-action pairì—ì„œ ë‹¤ìŒ stateì™€ ë‹¤ìŒ actionê¹Œì§€ë¥¼ ë³´ê³  updateí•˜ê¸° ë•Œë¬¸ì— ë¶™ì€ ì´ë¦„ì…ë‹ˆë‹¤. TD(0)ë¥¼ ì´í•´í–ˆë‹¤ë©´ í¬ê²Œ ì–´ë ¤ìš´ ì ì´ ì—†ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

    > Sarsa
      Q(S,A) <- Q(S,A) + ğ›¼ * (R + ğ›¾ * Q(S',A') - Q(S,A))

  SarsaëŠ” ë”°ë¼ì„œ TD(0)ì„ ê°€ì§€ê³  action-value functionìœ¼ë¡œ ë°”ê¾¸ê³  Ïµ-greedy policy improvementë¥¼ í•œ ê²ƒì…ë‹ˆë‹¤.

    > On-Policy Control With Sarsa
      Every time-step:
        Policy evalutation Sarsa, Q â‰ˆ q_ğœ‹
        Policy improvement Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ policy improvement

  Sarsaì˜ algorithmì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. on-policy TD control algorithmìœ¼ë¡œì¨ ë§¤ time-setpë§ˆë‹¤ í˜„ì¬ì˜ Q valueë¥¼ imediate rewardì™€ ë‹¤ìŒ actionì˜ Q valueë¥¼ ê°€ì§€ê³  updateí•©ë‹ˆë‹¤. policyëŠ” ë”°ë¡œ ì •ì˜ë˜ì§€ëŠ” ì•Šê³  ì´ Q valueë¥¼ ë³´ê³  Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦í•˜ê²Œ ì›€ì§ì´ëŠ” ê²ƒ ìì²´ê°€ policyì…ë‹ˆë‹¤.

    > Sarsa algorithm
      Initialize Q(s,a), âˆ€s âˆˆ S, a âˆˆ A(s), arbitrarily, and Q(terminal-state,âˆ™) = 0
      Repeat (for each episode):
        Initialize S
        Choose A from S using policy derived from Q (e.g., Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
          Take action A, observe R, S'
          Choose A' from S' using policy derived from Q (e.g., Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
          Q(S,A) <- Q(S,A) + ğ›¼ * [R + ğ›¾ * Q(S',A') - Q(S,A)]
          S <- S'; A <- A';
        until S is terminal
