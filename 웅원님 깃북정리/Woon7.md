/*ì´ì›…ì›ë‹˜ Git*/

# Temporal Difference Learning

  - 5~7ì¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

# TD Prediction

## 1. Tmeporal Difference

  ì´ì „ Chapterì—ì„œ ë°°ì› ë˜ Monte-Carlo Controlì€ Model-Free Controlì…ë‹ˆë‹¤. Model-Freeë¼ëŠ” ì ì— ê°•í™”í•™ìŠµì´ì§€ë§Œ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ onlineìœ¼ë¡œ ë°”ë¡œë°”ë¡œ í•™ìŠµí•  ìˆ˜ê°€ ì—†ê³  ê¼­ ëë‚˜ëŠ” episodeì—¬ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
  ëë‚˜ì§€ ì•Šë”ë¼ë„ eipsodeê°€ ê¸¸ ê²½ìš°ì—ëŠ”(ì˜ˆë¥¼ ë“¤ì–´ atari ê²Œì„ì´ ì•„ë‹ˆë¼ starcraftê°™ì€ ì—ê¹€) í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìì—°ìŠ¤ëŸ½ê²Œë„ ê¼­ episodeê°€ ëë‚˜ì§€ ì•Šë”ë¼ë„ DPì²˜ëŸ¼ time stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆì§€ ì•Šë‚˜?ë¼ëŠ” ìƒê°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ê²Œ ë°”ë¡œ Temporal Differenceì´ë©° Suttonêµìˆ˜ë‹˜ ì±…ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

    > Temporal Difference method
      If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.
      TD learning is a combination of Monte-Carlo ideas and dynamic programming (DP) ideas.
      Like Monte-Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics.
      Like DP, TD methods update estimates based in part on other learned estimates, without wating for a final outcome (they bootstrap)

  Temporal difference(TD)ëŠ” MCì™€ DPë¥¼ ì„ì€ ê²ƒìœ¼ë¡œì¨ MCì²˜ëŸ¼ raw experienceë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ DPì²˜ëŸ¼ time stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ì— "bootstrap"ì´ë¼ê³  í•˜ëŠ”ë° ì´ ë§ì€ ë¬´ì—‡ì„ ëœ»í• ê¹Œìš”?

  ëŒ€í•™ìƒí™œì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. MCëŠ” ëŒ€í•™êµì— ë“¤ì–´ì™€ì„œ ì¡¸ì—…ì„ í•œ ë‹¤ìŒì— ê·¸ ë™ì•ˆì„ ëŒì•„ë³´ë©° "ì´ê±´ ë” í–ˆì–´ì•¼ í–ˆê³  ìˆ ì€ ëœ ë§ˆì…”ì•¼í–ˆë‹¤"ë¼ê³  ìƒê°í•˜ë©° ë‹¤ì‹œ ëŒ€í•™êµë¥¼ ë“¤ì–´ê°€ì„œ ëŒ€í•™ìƒí™œì„ í•˜ë©´ì„œ ì¡¸ì—…í•  ë•Œê¹Œì§€ ë˜‘ê°™ì´ ì‚´ë‹¤ê°€ ë‹¤ì‹œ ì¡¸ì—…í•˜ê³  ìì‹ ì„ ëŒì•„ë³´ëŠ” ë°˜ë©´ì— TDê°™ì€ ê²½ìš°ëŠ” ê°™ì€ ëŒ€í•™êµë¥¼ ë‹¤ë‹ˆê³  ìˆëŠ” 2í•™ë…„ ì„ ë°°ê°€ 1í•™ë…„ì„ ì´ëŒì–´ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

  ì‚¬ì‹¤ì€ ë‘˜ ë‹¤ ì¡¸ì—…ì„ í•´ë³´ì§€ ì•Šì€ ìƒíƒœì—ì„œ (ì˜ ëª¨ë¥´ëŠ” ìƒí™©ì—ì„œ)ì´ëŒì–´ ì£¼ëŠ” ê²ƒì´ì§€ë§Œ ëŒ€í•™êµë¥¼ ë‹¤ë‹ˆë©´ì„œ ë°”ë¡œ ë°”ë¡œ ìì‹ ì„ ê³ ì³ë‚˜ê°€ê¸° ë•Œë¬¸ì— ì–´ì©Œë©´ ë” ì˜³ì€ ë°©ë²•ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤.

  TDëŠ” ë”°ë¼ì„œ í˜„ì¬ì˜ value functionì„ ê³„ì‚°í•˜ëŠ”ë° ì•ì„ (ì•ì„ ì´ë¼ê³  í‘œí˜„í•˜ê¸°ì—ëŠ” ì¡°ê¸ˆ ì• ë§¤í•œ ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤.) ì£¼ë³€ stateë“¤ì˜ value functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ê²ƒì€ ì´ì „ì— ë°°ì› ë˜ Bellman equationì´ë©°, ë”°ë¼ì„œ Bellman equationìì²´ê°€ Bootstrapí•˜ëŠ” ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

## 2. TD(0)

  Temporal Difference(TD)ëŠ” Monte-Carlo + DPë¼ê³  ë§í–ˆì—ˆìŠµë‹ˆë‹¤. ì´ì „ì— ë´¤ë˜ Monte-Carlo predictionì—ì„œ incremental meanì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ returnì„ ì‚¬ìš©í•´ì„œ updateí•©ë‹ˆë‹¤. TDì—ì„œëŠ” ì´ G_të¥¼ R_(t+1) + ğ›¾ * V_(t+1)(s)ë¡œ ë°”ê¿”ì„œ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ë©ë‹ˆë‹¤.
  Temporal Difference learning ë°©ë²•ì—ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë° ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ TD(0)ì´ê³  ë°©ê¸ˆ ë§í•œ ë°©ë²•ì´ ë°”ë¡œ TD(0)ì…ë‹ˆë‹¤.
  R_(t+1) + ğ›¾ * V_(t+1)(s)ë¥¼ TD targetì´ë¼ê³  ë¶€ë¥´ê³  ê·¸ targetê³¼ í˜„ì¬ì˜ value functionê³¼ì˜ ì°¨ì´ë¥¼ TD errorë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

    > Temporal Differenc TD(0)
      Goal : leaern v_ğœ‹ online from experience under policy ğœ‹
      Incremental every-visit Monte-Carlo
        Update value V(S_t) toward actual return G_t
          V(S_t) <- V(S_t) + ğ›¼ * (G_t - V(S_t))

      Simplest temporal-difference learning algorithm : __TD(0)__ !
        Update value V(S_t) toward estimated return R_(t+1) + ğ›¾ * V_(t+1)(s)
          V(S_t) <- V(S_t) + ğ›¼ * (R_(t+1) + ğ›¾ * V_(t+1)(s) - V(S_t))

      R_(t+1) + ğ›¾ * V_(t+1)(s) is called the TD target
      ğ›¿_t = R_(t+1) + ğ›¾ * V_(t+1)(s) - V(S_t) is called the TD error

      => V_(S_t) = V(S_t) + ğ›¼ * ğ›¿_t

  TD(0)ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê³  backup diagramì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
