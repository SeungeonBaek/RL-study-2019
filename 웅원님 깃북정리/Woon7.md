ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content/value_function_approximation.html
/*ì´ì›…ì›ë‹˜ Git*/

# Temporal Difference Learning

  - 5~7ì¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

# TD Prediction

## 1. Tmeporal Difference

  ì´ì „ Chapterì—ì„œ ë°°ì› ë˜ Monte-Carlo Controlì€ Model-Free Controlì…ë‹ˆë‹¤. Model-Freeë¼ëŠ” ì ì— ê°•í™”í•™ìŠµì´ì§€ë§Œ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ onlineìœ¼ë¡œ ë°”ë¡œë°”ë¡œ í•™ìŠµí•  ìˆ˜ê°€ ì—†ê³  ê¼­ ëë‚˜ëŠ” episodeì—¬ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
  ëë‚˜ì§€ ì•Šë”ë¼ë„ eipsodeê°€ ê¸¸ ê²½ìš°ì—ëŠ”(ì˜ˆë¥¼ ë“¤ì–´ atari ê²Œì„ì´ ì•„ë‹ˆë¼ starcraftê°™ì€ ì—ê¹€) í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìì—°ìŠ¤ëŸ½ê²Œë„ ê¼­ episodeê°€ ëë‚˜ì§€ ì•Šë”ë¼ë„ DPì²˜ëŸ¼ time stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆì§€ ì•Šë‚˜?ë¼ëŠ” ìƒê°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ê²Œ ë°”ë¡œ Temporal Differenceì´ë©° Suttonêµìˆ˜ë‹˜ ì±…ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

    > Temporal Difference method
      If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.
      TD learning is a combination of Monte-Carlo ideas and dynamic programming (DP) ideas.
      Like Monte-Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics.
      Like DP, TD methods update estimates based in part on other learned estimates, without wating for a final outcome (they bootstrap)

  Temporal difference(TD)ëŠ” MCì™€ DPë¥¼ ì„ì€ ê²ƒìœ¼ë¡œì¨ MCì²˜ëŸ¼ raw experienceë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ DPì²˜ëŸ¼ time stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ì— "bootstrap"ì´ë¼ê³  í•˜ëŠ”ë° ì´ ë§ì€ ë¬´ì—‡ì„ ëœ»í• ê¹Œìš”?

  ëŒ€í•™ìƒí™œì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. MCëŠ” ëŒ€í•™êµì— ë“¤ì–´ì™€ì„œ ì¡¸ì—…ì„ í•œ ë‹¤ìŒì— ê·¸ ë™ì•ˆì„ ëŒì•„ë³´ë©° "ì´ê±´ ë” í–ˆì–´ì•¼ í–ˆê³  ìˆ ì€ ëœ ë§ˆì…”ì•¼í–ˆë‹¤"ë¼ê³  ìƒê°í•˜ë©° ë‹¤ì‹œ ëŒ€í•™êµë¥¼ ë“¤ì–´ê°€ì„œ ëŒ€í•™ìƒí™œì„ í•˜ë©´ì„œ ì¡¸ì—…í•  ë•Œê¹Œì§€ ë˜‘ê°™ì´ ì‚´ë‹¤ê°€ ë‹¤ì‹œ ì¡¸ì—…í•˜ê³  ìì‹ ì„ ëŒì•„ë³´ëŠ” ë°˜ë©´ì— TDê°™ì€ ê²½ìš°ëŠ” ê°™ì€ ëŒ€í•™êµë¥¼ ë‹¤ë‹ˆê³  ìˆëŠ” 2í•™ë…„ ì„ ë°°ê°€ 1í•™ë…„ì„ ì´ëŒì–´ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

  ì‚¬ì‹¤ì€ ë‘˜ ë‹¤ ì¡¸ì—…ì„ í•´ë³´ì§€ ì•Šì€ ìƒíƒœì—ì„œ (ì˜ ëª¨ë¥´ëŠ” ìƒí™©ì—ì„œ)ì´ëŒì–´ ì£¼ëŠ” ê²ƒì´ì§€ë§Œ ëŒ€í•™êµë¥¼ ë‹¤ë‹ˆë©´ì„œ ë°”ë¡œ ë°”ë¡œ ìì‹ ì„ ê³ ì³ë‚˜ê°€ê¸° ë•Œë¬¸ì— ì–´ì©Œë©´ ë” ì˜³ì€ ë°©ë²•ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤.

  TDëŠ” ë”°ë¼ì„œ í˜„ì¬ì˜ value functionì„ ê³„ì‚°í•˜ëŠ”ë° ì•ì„ (ì•ì„ ì´ë¼ê³  í‘œí˜„í•˜ê¸°ì—ëŠ” ì¡°ê¸ˆ ì• ë§¤í•œ ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤.) ì£¼ë³€ stateë“¤ì˜ value functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ê²ƒì€ ì´ì „ì— ë°°ì› ë˜ Bellman equationì´ë©°, ë”°ë¼ì„œ Bellman equationìì²´ê°€ Bootstrapí•˜ëŠ” ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

## 2. TD(0)

  Temporal Difference(TD)ëŠ” Monte-Carlo + DPë¼ê³  ë§í–ˆì—ˆìŠµë‹ˆë‹¤. ì´ì „ì— ë´¤ë˜ Monte-Carlo predictionì—ì„œ incremental meanì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ returnì„ ì‚¬ìš©í•´ì„œ updateí•©ë‹ˆë‹¤. TDì—ì„œëŠ” ì´ G_të¥¼ R_(t+1) + ğ›¾ * V_(t+1)(s)ë¡œ ë°”ê¿”ì„œ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ë©ë‹ˆë‹¤.
  Temporal Difference learning ë°©ë²•ì—ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë° ê·¸ ì¤‘ì—ì„œ ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ TD(0)ì´ê³  ë°©ê¸ˆ ë§í•œ ë°©ë²•ì´ ë°”ë¡œ TD(0)ì…ë‹ˆë‹¤.
  R_(t+1) + ğ›¾ * V_(t+1)(s)ë¥¼ TD targetì´ë¼ê³  ë¶€ë¥´ê³  ê·¸ targetê³¼ í˜„ì¬ì˜ value functionê³¼ì˜ ì°¨ì´ë¥¼ TD errorë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

    > Temporal Differenc TD(0)
      Goal : leaern v_ğœ‹ online from experience under policy ğœ‹
      Incremental every-visit Monte-Carlo
        Update value V(S_t) toward actual return G_t
          V(S_t) <- V(S_t) + ğ›¼ * (G_t - V(S_t))

      Simplest temporal-difference learning algorithm : __TD(0)__ !
        Update value V(S_t) toward estimated return R_(t+1) + ğ›¾ * V_(t+1)(s)
          V(S_t) <- V(S_t) + ğ›¼ * (R_(t+1) + ğ›¾ * V_(t+1)(s) - V(S_t))

      R_(t+1) + ğ›¾ * V_(t+1)(s) is called the TD target
      ğ›¿_t = R_(t+1) + ğ›¾ * V_(t+1)(s) - V(S_t) is called the TD error

      => V_(S_t) = V(S_t) + ğ›¼ * ğ›¿_t

  TD(0)ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê³  backup diagramì„ ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

    > TD(0) algorithm
      Input : the policy ğœ‹ to be evaluted
      Initialize V(s) arbitrarily (e.g., V(s) = 0, âˆ€s âˆˆ S+)
      Repeat (for each episode):
        A <- action given by ğœ‹ for S
        Take action A; observe reward, R, and next state, S'
        V(S) <- V(S) + alpha[R + ğ›¾ * V(S') - V(S)]
        S <- S'
      until S is terminal

***

## 3. Monte-Carlo method vs Temporal Difference method

  MCì˜ ê²½ìš°ì—ëŠ” ë‹¤ ë„ì°©í•œ ë‹¤ìŒì— ê°ê°ì˜ stateì—ì„œ ì˜ˆì¸¡í–ˆë˜ value functionê³¼ ì‹¤ì œë¡œ ë°›ì€ returenì„ ë¹„êµí•´ì„œ updateí•˜ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ TDì—ì„œëŠ” í•œ step ì§„í–‰ì„ í•˜ë©´ ì•„ì§ ë„ì°©ì„ í•˜ì§€ ì•Šì•„ì„œ ì–¼ë§ˆë‚˜ ê±¸ë¦´ì§€ëŠ” ì •í™•íˆ ëª¨ë¥´ì§€ë§Œ í•œ step ë™ì•ˆ ì§€ë‚¬ë˜ ì‹œê°„ì„ í† ëŒ€ë¡œ value functionì„ updateí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œë¡œ ë„ì°©ì„ í•˜ì§€ ì•Šì•„ë„, final outcomeì„ ëª¨ë¥´ë”ë¼ë„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê²ƒì´ TDì˜ ì¥ì ì´ë©° ë§¤ stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒë„ ì¥ì ì…ë‹ˆë‹¤.

    > MC vs TD
      TD can learn before knowing the final outcome
        TD can learn online after every step
        MC must wait until end of episode before return is knwon

      TD can learn without the final outcome
        TD can learn from incomplete sequences
        MC can only learn from complete sequences
        TD works in continuing (non-terminating) environments
        MC only works for episodic (terminating) environments

  - Bias / Variation Trade-Off

  ë˜ í•œ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ì ì€ ë°”ë¡œ biasì™€ varianceì…ë‹ˆë‹¤.
  bias : ë°ì´í„°ê°€ ì‹¤ì œ ê°’ìœ¼ë¡œë¶€í„° ì „ì²´ì ìœ¼ë¡œ ë§ì´ ë²—ì–´ë‚˜ê²Œ ë˜ë©´ biasê°€ ë†’ë‹¤ í˜¹ì€ biasedë¬ë‹¤ê³  í‘œí˜„í•©ë‹ˆë‹¤.
  variance : ì‹¤ì œ ê°’ê³¼ ìƒê´€ì—†ì´, ì „ì²´ì ìœ¼ë¡œ ë°ì´í„°ê°€ ë§ì´ í¼ì ¸ìˆëŠ” ê²½ìš° varianceê°€ ë†’ë‹¤ê³  í•©ë‹ˆë‹¤.

  ë‘˜ ë‹¤ ë‚®ìœ¼ë©´ ì¢‹ê² ì§€ë§Œ ë³´í†µì€ ì´ ë‘˜ì´ Trade-off ê´€ê³„ì— ìˆì–´ì„œ í•˜ë‚˜ê°€ ë‚®ì•„ì§€ë©´ í•˜ë‚˜ê°€ ë†’ì•„ì§€ëŠ” ê´€ê³„ì— ìˆìŠµë‹ˆë‹¤. TDëŠ” biasê°€ ë†’ê³  MCëŠ” varianceê°€ ë†’ìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ í•™ìŠµì— ë°©í•´ê°€ ë˜ëŠ” ìš”ì†Œì…ë‹ˆë‹¤.

  TDëŠ” í•œ episodeì•ˆì—ì„œ ê³„ì† ì—…ë°ì´íŠ¸ë¥¼ í•˜ëŠ”ë° ë³´í†µì€ ê·¸ ì „ì˜ ìƒíƒœê°€ ê·¸ í›„ì˜ ìƒíƒœì— ì˜í–¥ì„ ë§ì´ ì£¼ê¸° ë•Œë¬¸ì— í•™ìŠµì´ í•œ ìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì§€ê²Œ ë©ë‹ˆë‹¤. ê³„ì† ê°™ì€ ë¶„ì•¼ ì‚¬ëŒë“¤ê³¼ ì´ì•¼ê¸°ë¥¼ í•˜ë©´ ìƒê°ì˜ í­ì´ ì¢ì•„ì§€ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ê²ƒì„ ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ìƒê°ì´ í•œ ìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ì§€ìš” ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ì‚¬ëŒë“¤ì˜ ì˜ê²¬ì„ ë“¤ì–´ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

  í•˜ì§€ë§Œ ë„ˆë¬´ ì—¬ëŸ¬ì‚¬ëŒì˜ ì˜ê²¬ì„ ë“£ë‹¤ë³´ë©´ ì´ë„ ì €ë„ ì•„ë‹ˆê²Œ ë˜ì„œ ê²°ì •ì„ ëª»í•˜ê²Œ ë˜ê³¤ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‹¤ì œ ê²½í—˜ë“¤ì´ ì‚¬ì‹¤ bias/variance trade-offì™€ ê´€ë ¨ì´ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. MCê°€ varianceê°€ ë†’ì€ ì´ìœ ëŠ” ì•ì—ì„œë„ ì„¤ëª…í–ˆì—ˆì§€ë§Œ ì—í”¼ì†Œë“œë§ˆë‹¤ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì²˜ìŒì— ì–´ë–»ê²Œ í–ˆëŠëƒì— ë”°ë¼ ì „í˜€ ë‹¤ë¥¸ experienceë¥¼ ê°€ì§ˆ ìˆ˜ê°€ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

    > TD methods
      TD target R_(t+1) + ğ›¾ * V_(t+1)(s) is biased estimate of v_ğœ‹(S_t)
      TD target is much lower variance than the return :
        Return depends on many random actions, transitions, rewards
        TD target depends on one random action, transition, reward

  ì•ìœ¼ë¡œë„ Biasì™€ VarianceëŠ” ë¨¸ë¦¬ì†ì— ê¸°ì–µí•´ë‘ì–´ì•¼ í•  í•„ìš”ê°€ ìˆëŠ” ê²ƒì´, ê°•í™”í•™ìŠµì„ ë°œì „ì‹œí‚¤ë ¤ëŠ” ë…¸ë ¥ë“¤ì´ ì•Œê³ ë¦¬ì¦˜ì˜ ê·¼ë³¸ì ì¸ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ëŠ” ê²ƒì—ë„ ìˆì§€ë§Œ, ê·¸ ì•Œê³ ë¦¬ì¦˜ì˜ biasì™€ varianceë¥¼ ë‚®ì¶”ë ¤ëŠ” ê²ƒì— ì§‘ì¤‘ë˜ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

  ***

# TD Control

## 1. Sansa

  TD(0)ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    > TD(0) algorithm
      Input : the policy ğœ‹ to be evaluted
      Initialize V(s) arbitrarily (e.g., V(s) = 0, âˆ€s âˆˆ S+)
      Repeat (for each episode):
        A <- action given by ğœ‹ for S
        Take action A; observe reward, R, and next state, S'
        V(S) <- V(S) + alpha[R + ğ›¾ * V(S') - V(S)]
        S <- S'
      until S is terminal

  í•˜ì§€ë§Œ model-free controlì´ ë˜ê¸° ìœ„í•´ì„œëŠ” action-value functionì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  ë§í–ˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìœ„ì˜ TD(0)ì˜ ì‹ì—ì„œ value functionì„ action-value functionìœ¼ë¡œ ë°”ê¾¸ì–´ì£¼ë©´ Sarsaê°€ ë©ë‹ˆë‹¤. SarsaëŠ” í˜„ì¬ state-action pairì—ì„œ ë‹¤ìŒ stateì™€ ë‹¤ìŒ actionê¹Œì§€ë¥¼ ë³´ê³  updateí•˜ê¸° ë•Œë¬¸ì— ë¶™ì€ ì´ë¦„ì…ë‹ˆë‹¤. TD(0)ë¥¼ ì´í•´í–ˆë‹¤ë©´ í¬ê²Œ ì–´ë ¤ìš´ ì ì´ ì—†ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

    > Sarsa
      Q(S,A) <- Q(S,A) + ğ›¼ * (R + ğ›¾ * Q(S',A') - Q(S,A))

  SarsaëŠ” ë”°ë¼ì„œ TD(0)ì„ ê°€ì§€ê³  action-value functionìœ¼ë¡œ ë°”ê¾¸ê³  Ïµ-greedy policy improvementë¥¼ í•œ ê²ƒì…ë‹ˆë‹¤.

    > On-Policy Control With Sarsa
      Every time-step:
        Policy evalutation Sarsa, Q â‰ˆ q_ğœ‹
        Policy improvement Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ policy improvement

  Sarsaì˜ algorithmì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. on-policy TD control algorithmìœ¼ë¡œì¨ ë§¤ time-setpë§ˆë‹¤ í˜„ì¬ì˜ Q valueë¥¼ imediate rewardì™€ ë‹¤ìŒ actionì˜ Q valueë¥¼ ê°€ì§€ê³  updateí•©ë‹ˆë‹¤. policyëŠ” ë”°ë¡œ ì •ì˜ë˜ì§€ëŠ” ì•Šê³  ì´ Q valueë¥¼ ë³´ê³  Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦í•˜ê²Œ ì›€ì§ì´ëŠ” ê²ƒ ìì²´ê°€ policyì…ë‹ˆë‹¤.

    > Sarsa algorithm
      Initialize Q(s,a), âˆ€s âˆˆ S, a âˆˆ A(s), arbitrarily, and Q(terminal-state,âˆ™) = 0
      Repeat (for each episode):
        Initialize S
        Choose A from S using policy derived from Q (e.g., Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
          Take action A, observe R, S'
          Choose A' from S' using policy derived from Q (e.g., Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
          Q(S,A) <- Q(S,A) + ğ›¼ * [R + ğ›¾ * Q(S',A') - Q(S,A)]
          S <- S'; A <- A';
        until S is terminal

***

# Eligibility Traces

## 1. n-step TD

  TD learningì€ Monte-Carlo learningì— ë¹„í•´ì„œ ë§¤ time-stepë§ˆë‹¤ í•™ìŠµì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ TD Controlì¸ Sarsaë¥¼ ë´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ updateì‹ì„ ë³´ë©´ ê²°êµ­ update í•œ ë²ˆ í•  ë•Œ R í•˜ë‚˜ì˜ ì •ë³´ë°–ì— ì•Œ ìˆ˜ ì—†ì–´ì„œ í•™ìŠµí•˜ëŠ”ë° ì˜¤ë˜ ê±¸ë¦¬ê¸°ë„ í•˜ê³  biasê°€ ë†’ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ì€ TDì™€ MC ì´ ë‘˜ì˜ ì¥ì ì„ ë‹¤ ì‚´ë¦´ ìˆ˜ ìˆë‹¤ë©´ ì¢‹ìŠµë‹ˆë‹¤.

  ë”°ë¼ì„œ updateë¥¼ í•  ë•Œ one stepë§Œ ë³´ê³  updateë¥¼ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  n-stepì„ ì›€ì§ì¸ ë‹¤ìŒì— updateë¥¼ í•˜ìë¼ê³  ìƒê°í•˜ê²Œ ë©ë‹ˆë‹¤. ë§Œì•½ ì§€ê¸ˆì´ të¼ê³  í•˜ë©´ t+nì´ ë˜ë©´ ê·¸ ë•Œê¹Œì§€ ëª¨ì•˜ë˜ rewardë“¤ë¡œì¨ n-step returnì„ ê³„ì‚°í•˜ê³  ê·¸ ì‚¬ì´ì˜ ë°©ë¬¸í–ˆë˜ stateë“¤ì˜ value functionì„ updateí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ê²ƒì´ ë°”ë¡œ n-step TDì…ë‹ˆë‹¤. ëª‡ ê°œì˜ time-stepë§ˆë‹¤ updateë¥¼ í•  ê²ƒì¸ì§€ëŠ” ìì‹ ì´ ê²°ì •í•˜ë©´ ë©ë‹ˆë‹¤.

  ì´ n-stepì´ terminal stateê¹Œì§€ ê°„ë‹¤ë©´ ê·¸ê²Œ ë°”ë¡œ Monte-Carlo methodê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‘˜ì˜ ì¥ì ì„ ë‹¤ ì·¨í•˜ê¸° ìœ„í•´ì„œ ê·¸ ì‚¬ì´ì˜ ì ë‹¹í•œ n-stepì„ ì„ íƒí•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

    > n-Step Return
      Consider the following n-step returens for n = 1, 2, ..., âˆ;
        n = 1 (TD) G^1_t = R_(t+1) + ğ›¾ âˆ— V(S_(t+1))
        n = 2      G^2_t = R_(t+1) + ğ›¾ âˆ— R_(t+2) + ğ›¾^2 âˆ— V(S_(t+2))
          .
          .
          .
        n = âˆ (MC) G^âˆ_t = R_(t+1) + ğ›¾ âˆ— R_(t+2) + ğ›¾^2 âˆ— R_(t+3) + ... + ğ›¾^(T-1)) âˆ— R_T

      Define the n-step return
        G^n_t = R_(t+1) + ğ›¾ âˆ— R_(t+2) + ... + ğ›¾^(n-1) âˆ— R_(t+n) + ğ›¾^(n) âˆ— V(S_(t+n))

      n-step temporal-difference learning
        V(S_t) <- V(S_t) + ğ›¼ * (G^n_t - V(S_t))

  í•˜ì§€ë§Œ ì–´ë–¤ nì´ ì ë‹¹í•œ nì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ìˆëŠ”ì§€ì™€ ê·¸ ê¸°ì¤€ì´ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•œ ë¬¸ì œê°€ ë‚¨ìŠµë‹ˆë‹¤.

***

## 2. Forward-View of TD(ğœ†)

  ì˜ˆì œ ì„¤ëª…ì€ ìŠ¤í‚µí•˜ê² ìŠµë‹ˆë‹¤.

  ì–´ë– í•œ ë¬¸ì œì— n-step TD predictionì„ ì ìš©ì‹œì¼œë³¼ ê²½ìš°ì— nìœ¼ë¡œ ì ë‹¹í•œ ìˆ«ìê°€ ì–¼ë§ˆì¸ì§€ íŒë³„í•˜ê¸° ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤.
  ì‚¬ì‹¤ ğ›¼ì˜ ê°’ì— ë”°ë¼ì„œ ì–´ë–¤ n-stepì´ í•™ìŠµì— ì¢‹ì€ì§€ëŠ” ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ì€ ì—¬ëŸ¬ n-stepì„ í•©í•  ìˆ˜ë§Œ ìˆë‹¤ë©´ ê° n-stepì—ì„œì˜ ì¥ì ì„ ë‹¤ ì·¨í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

  ë”°ë¼ì„œ ì´ ëª¨ë“  n-step returnì„ ëª¨ë‘ ë”í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¨ìˆœíˆ ë”í•´ì„œ í‰ê· ì„ êµ¬í•˜ëŠ” ë°©ì‹ì´ ì•„ë‹ˆë¼, ì•„ë˜ì™€ ê°™ì´ ğœ†ë¼ëŠ” weightì„ ì‚¬ìš©í•´ì„œ geometrically weighted sumì„ ì´ìš©í•˜ê²Œ ë˜ë©´ ëª¨ë“  n-stepì„ ë‹¤ í¬í•¨í•˜ë©´ì„œë„ weightë“¤ì˜ í•©ì´ 1ì´ ë©ë‹ˆë‹¤. ì´ê²ƒì„ í†µí•´ì„œ êµ¬í•œ ğœ†-returnì„ ì›ë˜ MCì˜ return ìë¦¬ì— ë„£ì–´ì£¼ê²Œ ë˜ë©´, forward-view TD(ğœ†)ê°€ ë©ë‹ˆë‹¤.

      > ğœ†-return
        The ğœ†-return G^ğœ†_t combines all n-step returns G^ğœ†_t
        Using weight (1-ğœ†) * ğœ†^(n-1)

          G^ğœ†_t = (1-ğœ†) {n = 1 -> âˆ} Î£ ğœ†^(n-1) * G^n_t

        Forward-view TD(ğœ†)

          V(S_t) <- V(S_t) + ğ›¼ * (G^ğœ†_t - V(S_t))

  ë‹¤ì‹œ ì •ë¦¬ë¥¼ í•´ë³´ìë©´ TDëŠ” time-stepë§ˆë‹¤ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì¥ì ì€ ìˆì—ˆì§€ë§Œ ë˜í•œ biasê°€ ë†’ê³  í•™ìŠµì •ë³´ê°€ ë³„ë¡œ ì—†ê¸° ë•Œë¬¸ì— TDì™€ MCì˜ ì¥ì ì„ ë‘˜ ë‹¤ ì‚´ë¦¬ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ n-step TDê°€ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê° n-stepì´ ìƒí™©ë§ˆë‹¤ ë‹¤ë¥¸ ì¥ì ì´ ìˆì–´ì„œ ì´ ëª¨ë“  ì¥ì ì„ í¬í•¨í•˜ê¸° ìœ„í•´ì„œ ğœ†ë¼ëŠ” wiehgtì„ ë„ì…í•´ì„œ ğœ†-returnì„ ê³„ì‚°í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ forward-view TD(ğœ†)ì…ë‹ˆë‹¤.

  í•˜ì§€ë§Œ ì´ ë°©ë²•ì—ë„ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ MCì™€ ë˜‘ê°™ì´ episodeê°€ ëë‚˜ì•¼ updateë¥¼ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. (ëª¨ë“  n-stepì„ í¬í•¨í•˜ê¸° ë•Œë¬¸)

      > Forward-view TD(ğœ†)
        Update value function towards the ğœ†-return
        Forward-view looks into the future to compute G^ğœ†_t
        Like MC, can only be computed from complete episodes

***

## 3. Backward-View of TD(ğœ†)

  ë”°ë¼ì„œ ë³¸ë˜ TDì˜ ì¥ì ì´ì—ˆë˜ time-stepë§ˆë‹¤ updateí•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤. MCì˜ ì¥ì ì€ ì‚´ë¦¬ë©´ì„œë„ ë°”ë¡œ ë°”ë¡œ updateí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ì„ê¹Œìš”? ì—¬ê¸°ì„œ ë°”ë¡œ eligibility traceë¼ëŠ” ê°œë…ì´ ë‚˜ì˜µë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê³¼ê±°ì— ìˆì—ˆë˜ ì¼ë“¤ ì¤‘ì—ì„œ í˜„ì¬ ë‚´ê°€ ë°›ì€ rewardì— ê¸°ì—¬í•œ ê²ƒì´ ë¬´ì—‡ì¼ê¹Œ? ë¼ëŠ” credit assignmentë¬¸ì œì—ì„œ "ì–¼ë§ˆë‚˜ ìµœê·¼ì— ì¼ì–´ë‚¬ë˜ ì¼ì´ì—ˆë‚˜?"ì™€ "ì–¼ë§ˆë‚˜ ìì£¼ ë°œìƒí–ˆì—ˆë‚˜?"ë¼ëŠ” ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°ì˜ ì¼ë“¤ì„ ê¸°ì–µí•´ë†“ê³  í˜„ì¬ ë°›ì€ rewardë¥¼ ê³¼ê±°ì˜ stateë“¤ë¡œ ë¶„ë°°í•´ì£¼ê²Œ ë©ë‹ˆë‹¤.

      > Eligibility Traces

        ê·¸ë¦¼ : ë²¨ ë²¨ ë²¨ ì „êµ¬ ì‡¼í¬ ê°€ ê·¸ë ¤ì ¸ìˆìŒ.

        Credit assignment problem : did bell or light cause shock?
        Frequency heuristic : assign credit to most frequent states
        Recency heuristic   : assign credit to most recent states
        Eligibility traces combine both heuristics

          E_0(s) = 0
          E_t(s) = ğ›¾ * ğœ† * E_(t-1)(s) + I(S_t = s)

  ì¦‰, TD(0)ì²˜ëŸ¼ í˜„ì¬ updateí•  ğ›¿ë¥¼ ê³„ì‚°í•˜ë©´ í˜„ì¬ì˜ value functionë§Œ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê³¼ê±°ì— ì§€ë‚˜ì™”ë˜ ëª¨ë“  stateì— eligibility traceë¥¼ ê¸°ì–µí•´ë‘ì—ˆë‹¤ê°€ ê·¸ ë§Œí¼ ìì‹ ì„ updateí•˜ê²Œ ë©ë‹ˆë‹¤.

  ë”°ë¼ì„œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ í˜„ì¬ì˜ ê²½í—˜ì„ í†µí•´ í•œ ë²ˆì— ê³¼ê±°ì˜ ëª¨ë“  stateë“¤ì˜ value functionì„ updateí•˜ê²Œ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í˜„ì¬ì˜ ê²½í—˜ì´ ê³¼ê±°ì˜ value functionì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ê³  ì‹¶ì€ê°€ëŠ” ğœ†ë¥¼ í†µí•´ì„œ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ì´ëŸ¬í•œ updateë°©ì‹ì„ backward-view TD(ğœ†)ë¼ê³  í•©ë‹ˆë‹¤.

      > Backward-view TD(ğœ†)
        Keep an eligibility trace for every state s
        Update value V(s) for every state s
        In proportion to TD-error ğ›¿_t and eligibility trace E_t(s)
          ğ›¿_t = R_(t+1) + ğ›¾ * V(S_(t+1)) - V(S_t)
          V(s) <- V(s) + ğ›¼ * ğ›¿_t * E_t(s)

***

## 4. Sarsa(ğœ†)

  ìœ„ì—ì„œëŠ” TD predictionë§Œ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” TD Controlì— ëŒ€í•´ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Sarsaì—ë„ n-step Sarsaê°€ ìˆê³  forward-view Sarsa(ğœ†)ê°€ ìˆê³ , backward-view Sarsa(ğœ†)ê°€ ìˆìŠµë‹ˆë‹¤. ê°ê°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì„¤ëª…ì€ ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.

    > n-step Sarsa
    Consider the following n-step returens for n = 1, 2, ..., âˆ;
      n = 1 (Sarsa) q^1_t = R_(t+1) + ğ›¾ âˆ— q(S_(t+1))
      n = 2         q^2_t = R_(t+1) + ğ›¾ âˆ— R_(t+2) + ğ›¾^2 âˆ— q(S_(t+2))
        .
        .
        .
      n = âˆ (MC)    q^âˆ_t = R_(t+1) + ğ›¾ âˆ— R_(t+2) + ğ›¾^2 âˆ— R_(t+3) + ... + ğ›¾^(T-1)) âˆ— R_T

    Define the n-step Q-return
      q^n_t = R_(t+1) + ğ›¾ âˆ— R_(t+2) + ... + ğ›¾^(n-1) âˆ— R_(t+n) + ğ›¾^n âˆ— Q(S_(t+n))

    n-step Sarsa updates Q(s,a) towards the n-step Q-return
      Q(S_t,A_t) <- Q(S_t,A_t) + ğ›¼ * (q^n_t - Q(S_t,A_t))

***

## 5. Forward View Sarsa(ğœ†)

  The q^ğœ† return combines all n-step Q-returns q^ğœ†_t
  Using weight (1-ğœ†) * ğœ†^(n-1)
    q^ğœ†_t = (1-ğœ†) {n = 1 -> âˆ} Î£ (ğœ†^(n-1) * q^n_t)

  Forward-view Sarsa(ğœ†)
    Q(S_t,A_t) <- Q(S_t,A_t) + ğ›¼ * (q^ğœ†_t - Q(S_t,A_t))

***

## 5. Backward View Sarsa(ğœ†)

  Just like TD(ğœ†), we use eligibility traces in an online algorithm
  But Sarsa(ğœ†) has one eligibility trace for each stata-action pair
    E_0(s,a) = 0
    E_t(s,a) = ğ›¾ * E_(t-1)(s,a) + I(S_t = s, A_t = a)

  Q(s,a) is updated for every state s and action a
  In proportion to TD-error ğ›¿_t and eligibility trace E_t(s,a)
    ğ›¿_t = R_(t+1) + ğ›¾ * Q(S_(t+1),A_(t+1)) - Q(S_t,A_t)
    Q(s,a) <- Q(s,a) + ğ›¼ * ğ›¿_t * E_t(s,a)

  backward-view Sarsa(ğœ†) algorithmì˜ qseudo codeëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

      > Backward-veiw Sarsa(ğœ†) algorithm
        Initialize Q(s,a) arbitrarily, for all {s âˆˆ S ,ğ‘ âˆˆ ğ´}
        Repeat (for each episode) :
          E(s,a) = 0, for all {s âˆˆ S ,ğ‘ âˆˆ ğ´}
          Initilaze S, A
          Repeat(for each step of episode) :
            Take action A, observe R, S'
            Choose A' from S' using policy derived from Q (e.g. Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
            ğ›¿ <- R + ğ›¾ * Q(S',A') - Q(S,A)
            E(S,A) <- E(S,A) + 1
            For all {s âˆˆ S ,ğ‘ âˆˆ ğ´}:
              Q(s,a) <- Q(s,a) +  * ğ›¿_t * E_t(s,a)
              E(s,a) <- ğ›¾ * ğœ† * E(s,a)
            S <- S'; A <- A'
        until S is terminal
