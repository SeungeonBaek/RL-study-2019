ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/

# Policy Gradient

## Monte-Carlo Policy Gradient : REINFORCE

  ì•ì—ì„œ ì‚´í´ë´¤ë˜ Finite difference policy gradientëŠ” numericalí•œ ë°©ë²•ì´ê³  ì•ìœ¼ë¡œ ì‚´í´ë³¼ Monte-Carlo Policy Gradientì™€ Actor-Criticì€ analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•œë‹¤ëŠ” ê²ƒì€ objective functionì— ì§ì ‘ gradientë¥¼ ì·¨í•´ì¤€ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ, policyëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

  ì´ ë•Œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì„ episodeë§ˆë‹¤ í•´ì£¼ë©´ MC Policy Gradientì´ê³  time-stepë§ˆë‹¤ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©´ Actor-criticì…ë‹ˆë‹¤. ë°‘ì—ì„œ ë³´ë©´ score functionì´ë¼ëŠ” ê²ƒì´ ë‚˜ì˜µë‹ˆë‹¤. score functionì´ë€ ë¬´ì—‡ì¼ê¹Œìš”?

***

## 1. Score function

  analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ Objective functionì— ì§ì ‘ gradientë¥¼ ì·¨í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ìŠµë‹ˆë‹¤. objective functionìœ¼ë¡œ average reward formulationì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

  Gradientë¥¼ ğœƒì— ëŒ€í•´ì„œ ì·¨í•˜ê¸° ë•Œë¬¸ì— objective functionì˜ ì‹ ì¤‘ì—ì„œ policyì—ë§Œ gradientë¥¼ ì·¨í•˜ë©´ ë˜ì„œ ì•ˆìª½ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤.

    J(ğœƒ)    = E_ğœ‹ğœƒ[r] = {s âˆˆ S} Î£ d(s) {a âˆˆ A} Î£ ğœ‹_ğœƒ(s,a) * R^s_a

    âˆ‡ğœƒ J(ğœƒ) = {s âˆˆ S} Î£ d(s) {a âˆˆ A} Î£ ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)
            = E_ğœ‹ğœƒ [âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

  í•˜ì§€ë§Œ gradientê°€ ì•ˆìª½ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ì„œ logê°€ ê°‘ìê¸° ë‚˜ì˜¤ëŠ”ë° ê·¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) = ğœ‹_ğœƒ(s,a) * ( âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) / ğœ‹_ğœƒ(s,a) )
                = ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)

  ì™œ ì´ë ‡ê²Œ í•˜ëŠ” ê±¸ê¹Œìš”? ë§Œì•½ì— logì˜ í˜•íƒœë¡œ ë°”ê¾¸ì§€ ì•Šì•˜ë‹¤ê³  ìƒê°í•˜ë©´ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë©ë‹ˆë‹¤.

  $$\sum { s\in S }^{ }{ d(s) } \sum { a\in A }^{ }{ { { \nabla }{ \theta }\pi }{ \theta }(s,a)\quad{ R }_{ s,a }\quad } $$

  ì´ë ‡ê²Œ ë˜ë©´ ê²°êµ­ ğœ‹_ğœƒ(s,a)ê°€ ì‚¬ë¼ì¡Œê¸° ë•Œë¬¸ì— expectationì„ ì·¨í•  ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°êµ­ì€ expectationìœ¼ë¡œ ë¬¶ì–´ì„œ ê·¸ ì•ˆì„ samplingí•˜ê²Œ ë˜ì–´ì•¼ ê°•í™”í•™ìŠµì´ ë í…ë° ë”°ë¼ì„œ expectationì„ ì·¨í•˜ê¸° ìœ„í•´ì„œ policyë¥¼ ë‚˜ëˆ´ë‹¤ê°€ ê³±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ score functionì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ê°€ ë©ë‹ˆë‹¤.

    > Score function
      We now compute the policy gradient analytically
      Assume policy ğœ‹_ğœƒ is diffrentiable whenever it is non-zero
      and we know the gradient âˆ‡ğœƒ ğœ‹_ğœƒ(s,a)
      Likelihood ratios exploit the following identity

        âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) = ğœ‹_ğœƒ(s,a) * ( âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) / ğœ‹_ğœƒ(s,a) )
                    = ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)

      The socre function is âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)

  objective functionì˜ gradientëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

***

## 2. Policy Gradient Theorem

  E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

  ì´ ì‹ì˜ ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. p(x)ëŠ” policyë¼ê³  ë³´ì‹œë©´ ë˜ëŠ”ë° ~ëŠ” ì´ policyë¥¼ í‘œí˜„í•˜ëŠ” parameter spaceì—ì„œì˜ gradientì…ë‹ˆë‹¤. ì´ ë•Œ ì—¬ê¸°ì— scalarì¸ reward rì„ ê³±í•´ì¤Œìœ¼ë¡œì¨ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤˜ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê·¸ ë°©í–¥ìœ¼ë¡œ parameter spaceì—ì„œì˜ policyê°€ ì´ë™í•˜ê²Œ ë©ë‹ˆë‹¤.

  http://karpathy.github.io/2016/05/31/rl/  ì°¸ê³ í•˜ì„¸ìš© ^^

  ì´ë•Œ, policyê°€ ì–´ë””ë¡œ ì–¼ë§Œí¼ update ë  ê²ƒì¸ì§€ì˜ ì²™ë„ê°€ ë˜ëŠ” scalar functionìœ¼ë¡œ immediate rewardë§Œ ì‚¬ìš©í•˜ë©´ ê·¸ ìˆœê°„ì— ì˜í–ˆëƒ, ì˜ ëª»í–ˆëƒì˜ ì •ë³´ë°–ì— ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

  ì´ immediate rewardëŒ€ì‹ ì— ìì‹ ì´ í•œ í–‰ë™ì— ëŒ€í•œ long-term rewardì¸ action-value functionì„ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ê²ƒì´ policy gradient theoremì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì˜ ë§ˆì§€ë§‰ ì‹ì„ ë³´ê²Œë˜ë©´ r ëŒ€ì‹ ì— Q functionì´ ë“¤ì–´ê°„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œ ìˆœê°„ ìˆœê°„ì˜ rewardë¥¼ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì§€ê¸ˆê¹Œì§€ ê°•í™”í•™ìŠµì´ ê·¸ë˜ì™”ë“¯ì´ long-term valueë¥¼ ë³´ê² ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ Theoremì€ Suttonêµìˆ˜ë‹˜ì˜ "Policy Gradient Methods for Reinforcement Learning with Function Approximation"ë…¼ë¬¸ì— ì¦ëª…ì´ ë˜ì–´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

***

## 3. Stochastic Policy

  ìœ„ì˜ gradientë¥¼ í†µí•´ì„œ policyì˜ parameterë“¤ì„ ì—…ë°ì´íŠ¸ í•  ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ ì „ì˜ stochasticí•œ policyë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œìš”? ë³´í†µ ë”¥ëŸ¬ë‹ì—ì„œ output nodeì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” nonlinearí•¨ìˆ˜ì¸ Sigmoidí•¨ìˆ˜ì™€ Softmaxí•¨ìˆ˜ë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

  - Sigmoid
    Sigmoid í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤ê³  í•©ë‹ˆë‹¤.

      S(x) = 1 / (1 + e^-x) = e^x / (e^x + 1)

    ì´ í•¨ìˆ˜ëŠ” outputì´ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ stochastic ì¦‰ í™•ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ”ë°ì—ëŠ” ì¢‹ë‹¤ê³  í•©ë‹ˆë‹¤.

    discrete action spaceì˜ ê²½ìš° agentê°€ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆë‹¤ê³  í•˜ë©´(action = right or left) ì´ í•¨ìˆ˜ì—ì„œ ë‚˜ì˜¤ëŠ” ê°’ì´ "1ì— ê°€ê¹ë‹¤ë©´ ì™¼ìª½ìœ¼ë¡œ ê°ˆ í™•ë¥ ì´ ë†’ê³  0ì— ê°€ê¹ë‹¤ë©´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°ˆ í™•ë¥ ì´ ë†’ë‹¤"ë¼ëŠ” ì‹ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ stochastic í•œ policyë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ë˜ëŠ” continuous action spaceì¼ ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ì–´ë–¤ ë¡œë´‡ì˜ controllerì— 0ë¶€í„° 100ê¹Œì§€ control inputì„ ì¤„ ìˆ˜ ìˆë‹¤ë©´ sigmoidí•¨ìˆ˜ë¥¼ í†µí•´ 0ì´ ë‚˜ì˜¤ë©´ control inputì€ 0, 1ì´ outputìœ¼ë¡œ ë‚˜ì˜¤ë©´ control inputì€ 100ì„ ì£¼ëŠ” ì‹ìœ¼ë¡œ, sigmoid í•¨ìˆ˜ì˜ outputì„ normalized actionìœ¼ë¡œ ë³´ê³  continuous action ë˜í•œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - Softmax
    ë§Œì•½ discrete action spaceì—ì„œ actionì´ 3ê°œ ì´ìƒì´ ë˜ë©´ sigmoidí•¨ìˆ˜ë¡œ í‘œí˜„í•˜ê¸°ê°€ ì• ë§¤í•´ì§‘ë‹ˆë‹¤. ì´ëŸ´ ë•Œì—ëŠ” Softmaxí•¨ìˆ˜ë¥¼ ì“°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. Softmaxí•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

      P_t(a) = exp(q_t(a) / ğœ) / {i = 1 -> n} Î£ exp(q_t(i) / ğœ)

    softmax functionì€ valueë¥¼ action probabilityë¡œ ë³€í™˜í•´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    ì´ë•Œì˜ q_t(a)ëŠ” ì•Œë‹¤ì‹œí”¼ q valueì´ê³  ğœëŠ” temperature parameterë¼ê³  í•˜ì—¬ì„œ, high temperatureì¼ ë•ŒëŠ” í™•ë¥ ì´ ê±°ì˜ ê°™ì•„ì§€ê²Œ, low temperatureì—ëŠ” actionì„ ê³ ë¥¼ í™•ë¥ ì´ valueì— ì˜í–¥ì„ ë§ì´ ë°›ê²Œë” í•©ë‹ˆë‹¤.

    ì›…ì›ë‹˜ ì„¤ëª… : actionì´ i=1 ë¶€í„° nê¹Œì§€ ìˆì„ ë•Œ action probabilityë¥¼ ìœ„ì˜ í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ë‘ ê°€ì§€ ì´ìƒì˜ actionì— ëŒ€í•´ì„œ sigmoidê°€ ì•„ë‹Œ softmaxí•¨ìˆ˜ë¥¼ ì“°ëŠ” ì´ìœ ë¼ê³  í•˜ê³ , ì´ í•©ì€ 1ì…ë‹ˆë‹¹.

  ì´ë ‡ê²Œ stochasticí•œ policyë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•˜ëŠ” ì§€, sigmoidì™€ softmaxì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ì„¤ëª… í–ˆëŠ”ë° ì‚¬ì‹¤ ì´ë¡ ë³´ë‹¤ëŠ” ì‹¤ì œë¡œ ì½”ë“œë¡œ êµ¬í˜„í•  ë•Œ í•´ë³´ë©´ ë” ì˜ ì´í•´ê°€ ë  ê²ƒì´ë¼ê³  í•©ë‹ˆë‹¤.

***

## 4. Monte-Carlo Policy Gradient

  ì—¬ê¸°ê¹Œì§€ policy gradientë¥¼ í†µí•´ì„œ í•™ìŠµì„ í•  ì¤€ë¹„ëŠ” ëëƒˆë‹¤ê³  í•©ë‹ˆë‹¤. objective functionì„ ì •ì˜í–ˆê³  policyë¥¼ parameterë¥¼ í†µí•´ì„œ ë‚˜íƒ€ëƒˆì„ ë•Œ ê·¸ parameterë¥¼ update í•˜ê¸° ìœ„í•´ì„œ objective functionì˜ gradientë¥¼ êµ¬í•´ì•¼ í–ˆìŠµë‹ˆë‹¤.

  objective functionì˜ gradientëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ ëœë‹¤ê³  í•©ë‹ˆë‹¤.
  í•˜ì§€ë§Œ action value functionì˜ ê°’ì„ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œìš”? ì´ì „ì— ëª¨ë“  stateì— ëŒ€í•´ action value functionì„ ì•Œê¸° ì–´ë ¤ì›Œì„œ approximationì„ í–ˆì—ˆëŠ”ë° policyìì²´ë¥¼ updatí•˜ë ¤ë‹ˆ ê¸°ì¤€ì´ í•„ìš”í•˜ê³  ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ action value functionì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë° ì‚¬ì‹¤ ì´ ê°’ì„ ì•Œ ë°©ë²•ì´ ì• ë§¤í•©ë‹ˆë‹¤.

  í•˜ì§€ë§Œ ì•Œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆëŠ”ë° ê·¸ê²Œ ë°”ë¡œ Monte-Carloë°©ë²•ì…ë‹ˆë‹¤. episodeë¥¼ ê°€ë³´ê³  ë°›ì•˜ë˜ rewardë“¤ì„ ê¸°ì–µí•´ ë†“ê³  episodeê°€ ëë‚œ ë‹¤ìŒì— ê° stateì— ëŒ€í•œ returnì„ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤.

  returnìì²´ê°€ action-value functionì˜ unbiased estimationì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì€ REINFORCEë¼ê³  í•˜ë©° ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

    > Monte-Carlo Policy Gradient (REINFORCE)
      - Update parametres by stochastic gradient ascent
      - Using policy gradient theorem
      - Using return v_t as an unbiased sample of Q^(ğœ‹_ğœƒ)(s_t, a_t)
        âˆ†ğœƒ_t = ğ›¼ * âˆ‡ğœƒ log( ğœ‹_ğœƒ(s_t,a_t) * v_t)

    > function REINFORCE
      Initialise ğœƒ arbitrarily
      for each episode {s_1, a_1, r_2, ..., s_(T-1), a_(T-1), r_T}





































ã…ã„´ã…‡ã„¹
