ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/

# Policy Gradient

## Monte-Carlo Policy Gradient : REINFORCE

  ì•ì—ì„œ ì‚´í´ë´¤ë˜ Finite difference policy gradientëŠ” numericalí•œ ë°©ë²•ì´ê³  ì•ìœ¼ë¡œ ì‚´í´ë³¼ Monte-Carlo Policy Gradientì™€ Actor-Criticì€ analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•œë‹¤ëŠ” ê²ƒì€ objective functionì— ì§ì ‘ gradientë¥¼ ì·¨í•´ì¤€ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ, policyëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

  ì´ ë•Œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì„ episodeë§ˆë‹¤ í•´ì£¼ë©´ MC Policy Gradientì´ê³  time-stepë§ˆë‹¤ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©´ Actor-criticì…ë‹ˆë‹¤. ë°‘ì—ì„œ ë³´ë©´ score functionì´ë¼ëŠ” ê²ƒì´ ë‚˜ì˜µë‹ˆë‹¤. score functionì´ë€ ë¬´ì—‡ì¼ê¹Œìš”?

***

## 1. Score function

  analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ Objective functionì— ì§ì ‘ gradientë¥¼ ì·¨í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ìŠµë‹ˆë‹¤. objective functionìœ¼ë¡œ average reward formulationì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

  Gradientë¥¼ ğœƒì— ëŒ€í•´ì„œ ì·¨í•˜ê¸° ë•Œë¬¸ì— objective functionì˜ ì‹ ì¤‘ì—ì„œ policyì—ë§Œ gradientë¥¼ ì·¨í•˜ë©´ ë˜ì„œ ì•ˆìª½ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤.

    J(ğœƒ)    = E_ğœ‹ğœƒ[r] = {s âˆˆ S} Î£ d(s) {a âˆˆ A} Î£ ğœ‹_ğœƒ(s,a) * R^s_a

    âˆ‡ğœƒ J(ğœƒ) = {s âˆˆ S} Î£ d(s) {a âˆˆ A} Î£ ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * R^s_a
            = E_ğœ‹ğœƒ [âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * r]

  í•˜ì§€ë§Œ gradientê°€ ì•ˆìª½ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ì„œ logê°€ ê°‘ìê¸° ë‚˜ì˜¤ëŠ”ë° ê·¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) = ğœ‹_ğœƒ(s,a) * ( âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) / ğœ‹_ğœƒ(s,a) )
                = ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * R^s_a

  ì™œ ì´ë ‡ê²Œ í•˜ëŠ” ê±¸ê¹Œìš”? ë§Œì•½ì— logì˜ í˜•íƒœë¡œ ë°”ê¾¸ì§€ ì•Šì•˜ë‹¤ê³  ìƒê°í•˜ë©´ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë©ë‹ˆë‹¤.

    > {s âˆˆ S} Î£ d(s) {a âˆˆ A} Î£ âˆ‡ğœƒ(ğœ‹_ğœƒ(s,a) * R^s_a)

  ì´ë ‡ê²Œ ë˜ë©´ ê²°êµ­ ğœ‹_ğœƒ(s,a)ê°€ ì‚¬ë¼ì¡Œê¸° ë•Œë¬¸ì— expectationì„ ì·¨í•  ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°êµ­ì€ expectationìœ¼ë¡œ ë¬¶ì–´ì„œ ê·¸ ì•ˆì„ samplingí•˜ê²Œ ë˜ì–´ì•¼ ê°•í™”í•™ìŠµì´ ë í…ë° ë”°ë¼ì„œ expectationì„ ì·¨í•˜ê¸° ìœ„í•´ì„œ policyë¥¼ ë‚˜ëˆ´ë‹¤ê°€ ê³±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ score functionì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ê°€ ë©ë‹ˆë‹¤.

    > Score function
      We now compute the policy gradient analytically
      Assume policy ğœ‹_ğœƒ is diffrentiable whenever it is non-zero
      and we know the gradient âˆ‡ğœƒ ğœ‹_ğœƒ(s,a)
      Likelihood ratios exploit the following identity

        âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) = ğœ‹_ğœƒ(s,a) * ( âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) / ğœ‹_ğœƒ(s,a) )
                    = ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * R^s_a

      The socre function is âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a))

  objective functionì˜ gradientëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

***

## 2. Policy Gradient Theorem

  E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

  ì´ ì‹ì˜ ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. p(x)ëŠ” policyë¼ê³  ë³´ì‹œë©´ ë˜ëŠ”ë° âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)ëŠ” ì´ policyë¥¼ í‘œí˜„í•˜ëŠ” parameter spaceì—ì„œì˜ gradientì…ë‹ˆë‹¤. ì´ ë•Œ ì—¬ê¸°ì— scalarì¸ reward rì„ ê³±í•´ì¤Œìœ¼ë¡œì¨ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤˜ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê·¸ ë°©í–¥ìœ¼ë¡œ parameter spaceì—ì„œì˜ policyê°€ ì´ë™í•˜ê²Œ ë©ë‹ˆë‹¤.

  http://karpathy.github.io/2016/05/31/rl/  ì°¸ê³ í•˜ì„¸ìš© ^^

  ì´ë•Œ, policyê°€ ì–´ë””ë¡œ ì–¼ë§Œí¼ update ë  ê²ƒì¸ì§€ì˜ ì²™ë„ê°€ ë˜ëŠ” scalar functionìœ¼ë¡œ immediate rewardë§Œ ì‚¬ìš©í•˜ë©´ ê·¸ ìˆœê°„ì— ì˜í–ˆëƒ, ì˜ ëª»í–ˆëƒì˜ ì •ë³´ë°–ì— ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

  ì´ immediate rewardëŒ€ì‹ ì— ìì‹ ì´ í•œ í–‰ë™ì— ëŒ€í•œ long-term rewardì¸ action-value functionì„ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ê²ƒì´ policy gradient theoremì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì˜ ë§ˆì§€ë§‰ ì‹ì„ ë³´ê²Œë˜ë©´ r ëŒ€ì‹ ì— Q functionì´ ë“¤ì–´ê°„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œ ìˆœê°„ ìˆœê°„ì˜ rewardë¥¼ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì§€ê¸ˆê¹Œì§€ ê°•í™”í•™ìŠµì´ ê·¸ë˜ì™”ë“¯ì´ long-term valueë¥¼ ë³´ê² ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ Theoremì€ Suttonêµìˆ˜ë‹˜ì˜ "Policy Gradient Methods for Reinforcement Learning with Function Approximation"ë…¼ë¬¸ì— ì¦ëª…ì´ ë˜ì–´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

    > Policy Gradient Theorem
      - The policy gradient theorem generalises the likelihood ratio approach to multi-step MDPs
      - Replaces instanteneous reward r with long-term value Q^ğœ‹(s,a)
      - Policy gradient theorem applies to start state objective, average reward and average value objective

        For any differentiable policy ğœ‹_ğœƒ(s,a), for any of the policy objective functions
        J = J_1, J_avR, or (1/(1-ğ›¾)) * J_avV, the policy gradient is
        âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ [âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * Q^ğœ‹(s,a)]

***

## 3. Stochastic Policy

  ìœ„ì˜ gradientë¥¼ í†µí•´ì„œ policyì˜ parameterë“¤ì„ ì—…ë°ì´íŠ¸ í•  ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ ì „ì˜ stochasticí•œ policyë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œìš”? ë³´í†µ ë”¥ëŸ¬ë‹ì—ì„œ output nodeì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” nonlinearí•¨ìˆ˜ì¸ Sigmoidí•¨ìˆ˜ì™€ Softmaxí•¨ìˆ˜ë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

  - Sigmoid
    Sigmoid í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤ê³  í•©ë‹ˆë‹¤.

      S(x) = 1 / (1 + e^-x) = e^x / (e^x + 1)

    ì´ í•¨ìˆ˜ëŠ” outputì´ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ stochastic ì¦‰ í™•ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ”ë°ì—ëŠ” ì¢‹ë‹¤ê³  í•©ë‹ˆë‹¤.

    discrete action spaceì˜ ê²½ìš° agentê°€ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆë‹¤ê³  í•˜ë©´(action = right or left) ì´ í•¨ìˆ˜ì—ì„œ ë‚˜ì˜¤ëŠ” ê°’ì´ "1ì— ê°€ê¹ë‹¤ë©´ ì™¼ìª½ìœ¼ë¡œ ê°ˆ í™•ë¥ ì´ ë†’ê³  0ì— ê°€ê¹ë‹¤ë©´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°ˆ í™•ë¥ ì´ ë†’ë‹¤"ë¼ëŠ” ì‹ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ stochastic í•œ policyë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ë˜ëŠ” continuous action spaceì¼ ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ì–´ë–¤ ë¡œë´‡ì˜ controllerì— 0ë¶€í„° 100ê¹Œì§€ control inputì„ ì¤„ ìˆ˜ ìˆë‹¤ë©´ sigmoidí•¨ìˆ˜ë¥¼ í†µí•´ 0ì´ ë‚˜ì˜¤ë©´ control inputì€ 0, 1ì´ outputìœ¼ë¡œ ë‚˜ì˜¤ë©´ control inputì€ 100ì„ ì£¼ëŠ” ì‹ìœ¼ë¡œ, sigmoid í•¨ìˆ˜ì˜ outputì„ normalized actionìœ¼ë¡œ ë³´ê³  continuous action ë˜í•œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - Softmax
    ë§Œì•½ discrete action spaceì—ì„œ actionì´ 3ê°œ ì´ìƒì´ ë˜ë©´ sigmoidí•¨ìˆ˜ë¡œ í‘œí˜„í•˜ê¸°ê°€ ì• ë§¤í•´ì§‘ë‹ˆë‹¤. ì´ëŸ´ ë•Œì—ëŠ” Softmaxí•¨ìˆ˜ë¥¼ ì“°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. Softmaxí•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

      P_t(a) = exp(q_t(a) / ğœ) / {i = 1 -> n} Î£ exp(q_t(i) / ğœ)

    softmax functionì€ valueë¥¼ action probabilityë¡œ ë³€í™˜í•´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    ì´ë•Œì˜ q_t(a)ëŠ” ì•Œë‹¤ì‹œí”¼ q valueì´ê³  ğœëŠ” temperature parameterë¼ê³  í•˜ì—¬ì„œ, high temperatureì¼ ë•ŒëŠ” í™•ë¥ ì´ ê±°ì˜ ê°™ì•„ì§€ê²Œ, low temperatureì—ëŠ” actionì„ ê³ ë¥¼ í™•ë¥ ì´ valueì— ì˜í–¥ì„ ë§ì´ ë°›ê²Œë” í•©ë‹ˆë‹¤.

    ì›…ì›ë‹˜ ì„¤ëª… : actionì´ i=1 ë¶€í„° nê¹Œì§€ ìˆì„ ë•Œ action probabilityë¥¼ ìœ„ì˜ í•¨ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ë‘ ê°€ì§€ ì´ìƒì˜ actionì— ëŒ€í•´ì„œ sigmoidê°€ ì•„ë‹Œ softmaxí•¨ìˆ˜ë¥¼ ì“°ëŠ” ì´ìœ ë¼ê³  í•˜ê³ , ì´ í•©ì€ 1ì…ë‹ˆë‹¹.

  ì´ë ‡ê²Œ stochasticí•œ policyë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•˜ëŠ” ì§€, sigmoidì™€ softmaxì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ì„¤ëª… í–ˆëŠ”ë° ì‚¬ì‹¤ ì´ë¡ ë³´ë‹¤ëŠ” ì‹¤ì œë¡œ ì½”ë“œë¡œ êµ¬í˜„í•  ë•Œ í•´ë³´ë©´ ë” ì˜ ì´í•´ê°€ ë  ê²ƒì´ë¼ê³  í•©ë‹ˆë‹¤.

***

## 4. Monte-Carlo Policy Gradient

  ì—¬ê¸°ê¹Œì§€ policy gradientë¥¼ í†µí•´ì„œ í•™ìŠµì„ í•  ì¤€ë¹„ëŠ” ëëƒˆë‹¤ê³  í•©ë‹ˆë‹¤. objective functionì„ ì •ì˜í–ˆê³  policyë¥¼ parameterë¥¼ í†µí•´ì„œ ë‚˜íƒ€ëƒˆì„ ë•Œ ê·¸ parameterë¥¼ update í•˜ê¸° ìœ„í•´ì„œ objective functionì˜ gradientë¥¼ êµ¬í•´ì•¼ í–ˆìŠµë‹ˆë‹¤.

  objective functionì˜ gradientëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ ëœë‹¤ê³  í•©ë‹ˆë‹¤.
  í•˜ì§€ë§Œ action value functionì˜ ê°’ì„ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œìš”? ì´ì „ì— ëª¨ë“  stateì— ëŒ€í•´ action value functionì„ ì•Œê¸° ì–´ë ¤ì›Œì„œ approximationì„ í–ˆì—ˆëŠ”ë° policyìì²´ë¥¼ updatí•˜ë ¤ë‹ˆ ê¸°ì¤€ì´ í•„ìš”í•˜ê³  ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ action value functionì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë° ì‚¬ì‹¤ ì´ ê°’ì„ ì•Œ ë°©ë²•ì´ ì• ë§¤í•©ë‹ˆë‹¤.

  í•˜ì§€ë§Œ ì•Œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆëŠ”ë° ê·¸ê²Œ ë°”ë¡œ Monte-Carloë°©ë²•ì…ë‹ˆë‹¤. episodeë¥¼ ê°€ë³´ê³  ë°›ì•˜ë˜ rewardë“¤ì„ ê¸°ì–µí•´ ë†“ê³  episodeê°€ ëë‚œ ë‹¤ìŒì— ê° stateì— ëŒ€í•œ returnì„ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤.

  returnìì²´ê°€ action-value functionì˜ unbiased estimationì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì€ REINFORCEë¼ê³  í•˜ë©° ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

    > Monte-Carlo Policy Gradient (REINFORCE)
      - Update parameters by stochastic gradient ascent
      - Using policy gradient theorem
      - Using return v_t as an unbiased sample of Q^(ğœ‹_ğœƒ)(s_t, a_t)
        âˆ†ğœƒ_t = ğ›¼ * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s_t,a_t)) * v_t

    > function REINFORCE
      Initialise ğœƒ arbitrarily
      for each episode {s_1, a_1, r_2, ..., s_(T-1), a_(T-1), r_T} ~ ğœ‹_ğœƒ do
        for t = 1 to T-1 do
          ğœƒ <- ğœƒ + ğ›¼ * âˆ‡ğœƒ log( ğœ‹_ğœƒ(s_t,a_t) * v_t)
        end for
      end for
      return ğœƒ
      end function

  loopë¬¸ì„ ë³´ì‹œë©´ í•™ìŠµ, ì¦‰ parameterì˜ updateê°€ episodeë§ˆë‹¤ ì¼ì–´ë‚˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œ parameterë¥¼ regression ë°©ë²•ì´ ì•„ë‹ˆê³  stochastic gradient descent ë°©ë²•ì„ ì‚¬ìš©í•´ì„œ í•œ stepì”© update í•©ë‹ˆë‹¤.

***

## Actor-Critic Policy Gradient

  Monte-Carlo Policy Gradient ì•Œê³ ë¦¬ì¦˜ì„ ìœ„ì—ì„œ ë‹¤ì‹œ í•œ ë²ˆ ë³´ì‹­ì‹œì˜¤.

  REINFORCE ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” Returnì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— Monte-Carlo ê³ ìœ ì˜ ë¬¸ì œì¸ high varianceì˜ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ episode ìì²´ê°€ ê¸¸ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— í•™ìŠµí•˜ëŠ”ë° ê¹Œì§€ì˜ ì‹œê°„ì´ ìƒê°ë³´ë‹¤ ì˜¤ë˜ê±¸ë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì•„ì´ë””ì–´ë¥¼ ë‚¼ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

  Parameterë¥¼ í•˜ë‚˜ ë” ì‚¬ìš©í•´ì„œ action-value functionë„ approximationí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

***

## 1. Actor & Critic

  ê·¸ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì„ actor-criticì´ë¼ê³  ë¶€ë¥´ê³  ì•„ë˜ ê·¸ë¦¼ì„ í†µí•´ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Criticì€ action-value functionì„ approximationí•˜ëŠ” wë¥¼ updateí•˜ê³  ActorëŠ” approximateí•˜ëŠ” ğœƒë¥¼ update í•©ë‹ˆë‹¤. ë”°ë¼ì„œ wì™€ ğœƒë¼ëŠ” ë‘ ê°œì˜ weight parameterë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

  - Monte-Carlo policy gradient still has high variance

  - We use a critic to estimate the action-value function

    > Q_w(s,a) â‰ˆ Q^ğœ‹ğœƒ(s,a)

  - Actor-critic algorithms maintain two sets of parameters

    > Critic : Updates action-value function parameters w
      Actor  : Updates policy parameters ğœƒ, in direction suggested by critic

  - Actor-critic algorithms follow an approximate policy gradient

    > âˆ‡ğœƒ J(ğœƒ) â‰ˆ E_ğœ‹ğœƒ [âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * Q_w(s,a)]
           âˆ†ğœƒ = ğ›¼ * âˆ‡ğœƒ log( ğœ‹_ğœƒ(s_t,a_t) ) * Q_w(s,a)

  ì´ Criticì€ action-value functionì„ í†µí•´ í˜„ì¬ì˜ Policyë¥¼ í‰ê°€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. actionì„ í•´ë³´ê³  ê·¸ actionì˜ action value functionì´ ë†’ì•˜ìœ¼ë©´ ê·¸ actionì„ í•  í™•ë¥ ì„ ë†’ì´ë„ë¡ policyì˜ parameterë¥¼ updateí•˜ëŠ”ë° ê·¸ íŒë‹¨ì˜ ì²™ë„ê°€ ë˜ëŠ” action value functionë˜í•œ ì²˜ìŒì—ëŠ” ì˜ ëª¨ë¥´ê¸° ë•Œë¬¸ì— í•™ìŠµì„ í•´ì£¼ì–´ì•¼ í•˜ê³  ê·¸ë˜ì„œ criticì´ í•„ìš”í•©ë‹ˆë‹¤.

  action-value functionì„ updateí•˜ëŠ” ê²ƒì€ chapter 8ì—ì„œ ë´¤ë˜ ê²ƒ ì²˜ëŸ¼ TD(0)ì„ ì‚¬ìš©í•˜ì—¬ updateí•©ë‹ˆë‹¤. ì•„ë˜ì—ëŠ” action-value functionì„ linearí•˜ê²Œ approximationí–ˆì„ ê²½ìš°ì…ë‹ˆë‹¤.

  DNNì„ ì‚¬ìš©í•  ë•Œì—ëŠ” ì´ì „ì— ë°°ì› ë˜ ë°©ë²•ìœ¼ë¡œ ë°”ê¾¸ì–´ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. TD(0)ì„ ì‚¬ìš©í•œ Actor-Critic ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

  Monte-Carlo PGë•Œì™€ëŠ” ë‹¤ë¥´ê²Œ ë§¤ time stepë§ˆë‹¤ updateë¥¼ í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ updateë¥¼ í•  ë•ŒëŠ” policyì˜ parameterì™€ action value functionì˜ parameterë¥¼ ë™ì‹œì— updateí•´ì¤ë‹ˆë‹¤.

    > Action-Value Actor-Critic

    - Simple actor-critic algorithm based on action-value critic

    - Using lenear value function approximator Q_w(s,a) = ğœ™(s,a)^T * w
      Critic : Updates w by linear TD(0)
      Actor  : Updates ğœƒ by policy gradient

    function QAC
      Initialise s, ğœƒ
      Sample a ~ ğœ‹_ğœƒ

      for each step do
        Sample reward r = R^a_s; sample transition s' ~ P^a_s.
        Sample action a' ~ ğœ‹_ğœƒ(s',a')
        ğ›¿ = r + ğ›¾ * Q_w(s',a') - Q_w(s,a)
        ğœƒ = ğœƒ + ğ›¼ * âˆ‡ğœƒ log( ğœ‹_ğœƒ(s_t,a_t) * Q_w(s,a) )

        w <- w + Î² * ğ›¿ * ğœ™(s,a)
        a <- a', s <- s'
      end for
    end function

***

## 2. Baseline

  ì—¬ê¸°ê¹Œì§€ ê¸°ë³¸ì ì¸ Policy Gradientì˜ ê°œë…ì— ëŒ€í•´ì„œ ì‚´í´ë³´ì•˜ëŠ”ë° Actor-Criticë§ê³  ë‹¤ë¥´ê²Œ Variance ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì´ Baselineì…ë‹ˆë‹¤. Q function ì´í›„ë¡œ ì‚¬ìš©í•˜ê³  ìˆì§€ ì•Šë˜ State-value functionì„ ì¼ì¢…ì˜ í‰ê· ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ì˜ í–‰ë™ì´ í‰ê· ì ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” valueë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¢‹ë‚˜ ë¼ëŠ” ê²ƒì„ ê³„ì‚°í•˜ë„ë¡ í•´ì„œ varianceë¥¼ ì¤„ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ì¦‰, ì§€ê¸ˆê¹Œì§€ í•´ì™”ë˜ ê²ƒë³´ë‹¤ ì¢‹ìœ¼ë©´ ê·¸ ë°©í–¥ìœ¼ë¡œ updateë¥¼ í•˜ê³ , ì•„ë‹ˆë©´ ê·¸ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ê°€ê² ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

    > Reducing Variance Using a Baseline

      - We subtract a baseline function B(s) from the policy gradient
      - This can reduce variance, without changing expectation
        E_ğœ‹ğœƒ[âˆ‡ğœƒ log( ğœ‹_ğœƒ(s_t,a_t)) * B(s)]
               = {s âˆˆ S} Î£ d^ğœ‹ğœƒ(s) * {a âˆˆ A} Î£ âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) * B(s)
               = {s âˆˆ S} Î£ d^ğœ‹ğœƒ(s) B(s) * âˆ‡ğœƒ * {a âˆˆ A} Î£ ğœ‹_ğœƒ(s,a)
               = 0

      - A good baseline B(s) is the state value function V^ğœ‹ğœƒ(s)
        B(s) = V^ğœ‹ğœƒ(s)

      - So we can rewrithe the policy gradient using the advantage function A^ğœ‹ğœƒ(s,a)
        A^ğœ‹ğœƒ(s,a) = Q^ğœ‹ğœƒ(s,a) - V^ğœ‹ğœƒ(s)
        âˆ‡ğœƒ J(ğœƒ)   = E_ğœ‹ğœƒ [âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * A^ğœ‹ğœƒ(s,a)]

  ì´ëŸ¬í•œ advantage functionì˜ ì‚¬ìš©ì€ varianceë¥¼ ìƒë‹¹íˆ ê°œì„ ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì•„ë˜ì™€ ê°™ì´ state-value functionê³¼ action-value functionì„ ë‘˜ ë‹¤ approximationí•´ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

  - The advantage function can significantly reduce variance of policy gradient
  - So the critic should really estimate the advantage function
  - For example, by estimating both V^ğœ‹ğœƒ(s) and Q^ğœ‹ğœƒ(s,a)
  - Using two function approximators and two parameter vectors, __w, v__

    > V_v(s) â‰ˆ V^ğœ‹ğœƒ(s) // Q_w(s,a) â‰ˆ Q^ğœ‹ğœƒ(s) // A(s,a) = Q_w(s,a) - V_v(s)

  - And updating both value functions by e.g. TD learning

  í•˜ì§€ë§Œ ë‹¤ì‹œ action-value functionì´ immediate reward + value functionì´ë¼ëŠ” ê²ƒì„ ìƒê°í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ê²°êµ­ value function í•˜ë‚˜ë§Œ approximateí•´ë„ ë˜ì„œ criticì— parameterë¥¼ ë‘ ê°œ ì‚¬ìš©í•˜ëŠ” ë¹„ íš¨ìœ¨ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - For the true value function V^ğœ‹ğœƒ(s), the TD error ğ›¿^ğœ‹ğœƒ
    > ğ›¿^ğœ‹ğœƒ = r + ğ›¾ * V^ğœ‹ğœƒ(s') - V^ğœ‹ğœƒ(s)

  - is an unbiased estimate of the advantage function

    > E_ğœ‹ğœƒ[ğ›¿^ğœ‹ğœƒ | s,a] = E_ğœ‹ğœƒ[r + ğ›¾ * V^ğœ‹ğœƒ(s') | s,a] - V^ğœ‹ğœƒ(s)

                       = Q^ğœ‹ğœƒ(s,a) - V^ğœ‹ğœƒ(s)

                       = A^ğœ‹ğœƒ(s,a)

  - So we can use the TD error to compute the policy gradient
    âˆ‡ğœƒ J(ğœƒ)   = E_ğœ‹ğœƒ [âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a)) * ğ›¿^ğœ‹ğœƒ]

  - In practice we can use an approximate TD error
    ğ›¿_v = r + ğ›¾ * V_v(s') - V_v(s)

  - This approach only requires one set of critic parameters v

  ì§€ê¸ˆê¹Œì§€ëŠ” evaluationìœ¼ë¡œ TD(0)ì„ ì‚¬ìš©í–ˆì§€ë§Œ ì´ì „ì—ë„ ë°°ì› ë“¯ì´ ì´ ìë¦¬ëŠ” TD(ğœ†)ê°€ ë“¤ì–´ê°ˆ ìˆ˜ë„ ìˆê³ , eligibility traceê°€ ë“¤ì–´ê°ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

  ìœ„ ë°©ë²•ì€ varianceê°€ ë‚®ì€ ëŒ€ì‹ ì— one stepë§Œì˜ ì •ë³´ë¡œ updateí•˜ë¯€ë¡œ biasê°€ ë†’ìŠµë‹ˆë‹¤. ì´ ë¬¸ì œì— ëŒ€í•œ ëŒ€ì±…ìœ¼ë¡œ TDì™€ MC ì‚¬ì´ì˜ ë°©ë²•ì¸ TD(ğœ†)ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
