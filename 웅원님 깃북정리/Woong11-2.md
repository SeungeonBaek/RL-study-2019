ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/

# Policy Gradient

## Monte-Carlo Policy Gradient : REINFORCE

  ì•ì—ì„œ ì‚´í´ë´¤ë˜ Finite difference policy gradientëŠ” numericalí•œ ë°©ë²•ì´ê³  ì•ìœ¼ë¡œ ì‚´í´ë³¼ Monte-Carlo Policy Gradientì™€ Actor-Criticì€ analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•œë‹¤ëŠ” ê²ƒì€ objective functionì— ì§ì ‘ gradientë¥¼ ì·¨í•´ì¤€ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ, policyëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

  ì´ ë•Œ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì„ episodeë§ˆë‹¤ í•´ì£¼ë©´ MC Policy Gradientì´ê³  time-stepë§ˆë‹¤ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©´ Actor-criticì…ë‹ˆë‹¤. ë°‘ì—ì„œ ë³´ë©´ score functionì´ë¼ëŠ” ê²ƒì´ ë‚˜ì˜µë‹ˆë‹¤. score functionì´ë€ ë¬´ì—‡ì¼ê¹Œìš”?

***

## 1. Score function

  analyticí•˜ê²Œ gradientë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ Objective functionì— ì§ì ‘ gradientë¥¼ ì·¨í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ìŠµë‹ˆë‹¤. objective functionìœ¼ë¡œ average reward formulationì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

  Gradientë¥¼ ğœƒì— ëŒ€í•´ì„œ ì·¨í•˜ê¸° ë•Œë¬¸ì— objective functionì˜ ì‹ ì¤‘ì—ì„œ policyì—ë§Œ gradientë¥¼ ì·¨í•˜ë©´ ë˜ì„œ ì•ˆìª½ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤.

    J(ğœƒ)    = E_ğœ‹ğœƒ[r] = {s âˆˆ S} Î£ d(s) {a âˆˆ A} Î£ ğœ‹_ğœƒ(s,a) * R^s_a

    âˆ‡ğœƒ J(ğœƒ) = {s âˆˆ S} Î£ d(s) {a âˆˆ A} Î£ ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)
            = E_ğœ‹ğœƒ [âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

  í•˜ì§€ë§Œ gradientê°€ ì•ˆìª½ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ì„œ logê°€ ê°‘ìê¸° ë‚˜ì˜¤ëŠ”ë° ê·¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) = ğœ‹_ğœƒ(s,a) * ( âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) / ğœ‹_ğœƒ(s,a) )
                = ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)

  ì™œ ì´ë ‡ê²Œ í•˜ëŠ” ê±¸ê¹Œìš”? ë§Œì•½ì— logì˜ í˜•íƒœë¡œ ë°”ê¾¸ì§€ ì•Šì•˜ë‹¤ê³  ìƒê°í•˜ë©´ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë©ë‹ˆë‹¤.

  $$\sum { s\in S }^{ }{ d(s) } \sum { a\in A }^{ }{ { { \nabla }{ \theta }\pi }{ \theta }(s,a)\quad{ R }_{ s,a }\quad } $$

  ì´ë ‡ê²Œ ë˜ë©´ ê²°êµ­ ğœ‹_ğœƒ(s,a)ê°€ ì‚¬ë¼ì¡Œê¸° ë•Œë¬¸ì— expectationì„ ì·¨í•  ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°êµ­ì€ expectationìœ¼ë¡œ ë¬¶ì–´ì„œ ê·¸ ì•ˆì„ samplingí•˜ê²Œ ë˜ì–´ì•¼ ê°•í™”í•™ìŠµì´ ë í…ë° ë”°ë¼ì„œ expectationì„ ì·¨í•˜ê¸° ìœ„í•´ì„œ policyë¥¼ ë‚˜ëˆ´ë‹¤ê°€ ê³±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ score functionì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ê°€ ë©ë‹ˆë‹¤.

    > Score function
      We now compute the policy gradient analytically
      Assume policy ğœ‹_ğœƒ is diffrentiable whenever it is non-zero
      and we know the gradient âˆ‡ğœƒ ğœ‹_ğœƒ(s,a)
      Likelihood ratios exploit the following identity

        âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) = ğœ‹_ğœƒ(s,a) * ( âˆ‡ğœƒ ğœ‹_ğœƒ(s,a) / ğœ‹_ğœƒ(s,a) )
                    = ğœ‹_ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)

      The socre function is âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * R^s_a)

  objective functionì˜ gradientëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

***

## 2. Policy Gradient Theorem

  E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹_ğœƒ(s,a) * r)]

  ì´ ì‹ì˜ ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. p(x)ëŠ” policyë¼ê³  ë³´ì‹œë©´ ë˜ëŠ”ë° ~ëŠ” ì´ policyë¥¼ í‘œí˜„í•˜ëŠ” parameter spaceì—ì„œì˜ gradientì…ë‹ˆë‹¤. ì´ ë•Œ ì—¬ê¸°ì— scalarì¸ reward rì„ ê³±í•´ì¤Œìœ¼ë¡œì¨ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤˜ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê·¸ ë°©í–¥ìœ¼ë¡œ parameter spaceì—ì„œì˜ policyê°€ ì´ë™í•˜ê²Œ ë©ë‹ˆë‹¤.

  http://karpathy.github.io/2016/05/31/rl/  ì°¸ê³ í•˜ì„¸ìš© ^^

  ì´ë•Œ, policyê°€ ì–´ë””ë¡œ ì–¼ë§Œí¼ update ë  ê²ƒì¸ì§€ì˜ ì²™ë„ê°€ ë˜ëŠ” scalar functionìœ¼ë¡œ immediate rewardë§Œ ì‚¬ìš©í•˜ë©´ ê·¸ ìˆœê°„ì— ì˜í–ˆëƒ, ì˜ ëª»í–ˆëƒì˜ ì •ë³´ë°–ì— ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

  ì´ immediate rewardëŒ€ì‹ ì— ìì‹ ì´ í•œ í–‰ë™ì— ëŒ€í•œ long-term rewardì¸ action-value functionì„ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ê²ƒì´ policy gradient theoremì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì˜ ë§ˆì§€ë§‰ ì‹ì„ ë³´ê²Œë˜ë©´ r ëŒ€ì‹ ì— Q functionì´ ë“¤ì–´ê°„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œ ìˆœê°„ ìˆœê°„ì˜ rewardë¥¼ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì§€ê¸ˆê¹Œì§€ ê°•í™”í•™ìŠµì´ ê·¸ë˜ì™”ë“¯ì´ long-term valueë¥¼ ë³´ê² ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ Theoremì€ Suttonêµìˆ˜ë‹˜ì˜ "Policy Gradient Methods for Reinforcement Learning with Function Approximation"ë…¼ë¬¸ì— ì¦ëª…ì´ ë˜ì–´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

***

## 3. Stochastic Policy

  ìœ„ì˜ gradientë¥¼ í†µí•´ì„œ policyì˜ parameterë“¤ì„ ì—…ë°ì´íŠ¸ í•  ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ ì „ì˜ stochasticí•œ policyë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œìš”? ë³´í†µ ë”¥ëŸ¬ë‹ì—ì„œ output nodeì—ì„œ ë§ì´ ì‚¬ìš©ë˜ëŠ” nonlinearí•¨ìˆ˜ì¸ Sigmoidí•¨ìˆ˜ì™€ Softmaxí•¨ìˆ˜ë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

  - Sigmoid
      Sigmoid í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤ê³  í•©ë‹ˆë‹¤.
      S(x) = 1 / (1 + e^-x) = e^x / (e^x + 1)










































ã…ã„´ã…‡ã„¹
