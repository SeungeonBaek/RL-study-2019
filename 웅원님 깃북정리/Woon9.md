ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/

# Value function Approximation

## 1. Tabular Methods

  ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ê°•í™”í•™ìŠµì€ action value funcitonì„ Tableë¡œ ë§Œë“¤ì–´ì„œ í‘¸ëŠ” Tabular Methodsì…ë‹ˆë‹¤. ì´ì— ëŒ€í•´ì„œ Suttonêµìˆ˜ë‹˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ë§í•©ë‹ˆë‹¤. ì¦‰, í˜„ì¬ì˜ ë°©ë²•ì€ stateë‚˜ actionì´ ë§ì§€ ì•Šì„ ê²½ìš°ì—ë§Œ ì ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ Tableì´ ì ì  ë” ì»¤ì§€ë©´ ì´ ê°’ë“¤ì„ ë‹¤ ê¸°ì–µí•  ë©”ëª¨ë¦¬ë„ ë¬¸ì œì§€ë§Œ í•™ìŠµì— ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì•ì—ì„œ ë‹¤ë¤˜ë˜ ì˜ˆì œë“¤ë„ ë‹¤ gridworldê°™ì´ ì‘ì€ ì˜ˆì œì˜€ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > We haver so far assumed that our esimates of value functions are represented as a table with one entry for each
    state of for each state,action pair.
    This is particularly clear and instructive case, but of course it is limited to tasks with small numbers of
    states and actions.
    The problem is not just the memory needed for large tables, but the time and data needed to fill them accurately.
    In other words, the key issue is that of generalization!

  ê·¸ëŸ¬ë‚˜ í˜„ì‹¤ì€ gridworldê°™ì´ ì‘ì€ ì˜ˆì œê°€ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— í˜„ì¬ì˜ ë°©ì‹ì€ ì‹¤ìš©ì ì´ì§€ ëª»í•©ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ë©´, ë°‘ì˜ ë¬¸ì œë“¤ì„ ê°•í™”í•™ìŠµì€ í’€ì§€ ëª»í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— "Generalization"ì´ ê°€ëŠ¥í•´ì§€ê¸° ìœ„í•´ì„œëŠ” ìƒˆë¡œìš´ ideaê°€ í•„ìš”í•©ë‹ˆë‹¤.

  íŠ¹íˆë‚˜ ê°•í™”í•™ìŠµì„ ì‹¤ì œ ì„¸ìƒì— ì ìš©ì‹œí‚¤ê³  ì‹¶ë‹¤ê³  í•  ê²½ìš°, ì‹¤ì œ ì„¸ìƒì€ continuous state spaceì´ë¯€ë¡œ ì‚¬ì‹¤ìƒ stateê°€ ë¬´í•œëŒ€ì´ê¸° ë•Œë¬¸ì— ìƒˆë¡œìš´ ë°©ë²•ì´ ì—†ë‹¤ë©´ ë¡œë´‡ì´ ê°•í™”í•™ìŠµìœ¼ë¡œ í•™ìŠµí•˜ê¸°ëŠ” ì‰½ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.
    > Large-Scale Reinforcement Learning
      Reinforcement learning can be used to solve large problems, e.g.
        Backgammon  : 10^20  states
        Computer GO : 10^170 states
        Helicopter  : continuous state space

      How can we scale up the model-free method for prediction and control from the last two lectures?

***

## 2. Parameterizing value function

  ì´ì— ëŒ€í•œ í•´ë‹µì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. tableë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  wë¼ëŠ” ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ value functionì„ í•¨ìˆ˜í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ì‰½ê²Œ ë§í•´ í•¨ìˆ˜ì˜ inputìœ¼ë¡œ stateê°€ ë“¤ì–´ê°€ë©´, action value functionì´ outputìœ¼ë¡œ ë‚˜ì˜¤ê³ , ì´ë•Œ í•¨ìˆ˜ì˜ parameterê°€ wì¸ ê²ƒì…ë‹ˆë‹¤.

  ì´ì œëŠ” í•™ìŠµì„ í†µí•´ì„œ Q functionì„ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , wë¼ëŠ” parameterë¥¼ ì—…ë°ì´íŠ¸ í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ functdion approximation ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

    > We consider differentiable function approximators, e.g.
      Linear combinations of features
      Neural network
      Decision tree
      Nearst neighbour
      Fourier / wavelet bases

  ì´ ì¤‘ì—ì„œ ì €í¬ëŠ” ìœ„ì˜ ë‘ ê°€ì§€ë¥¼ ë³¼í…ë° (1) Linear combinations of features (2) Neural networkë¥¼ ë³¼ ê²ƒì…ë‹ˆë‹¤. (2)ëŠ” íŠ¹íˆ ìµœê·¼ì— ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ê°ê´‘ë°›ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ìµœê·¼ì˜ ê°•í™”í•™ìŠµì€ ë‹¤ ë”¥ëŸ¬ë‹ì„ approximatorë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë³´í†µ Deep Reinforcement Learning (DRL)ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

***

# Stochastic Gradient Descent

## 1. Gradient Descent

  ì´ì „ê¹Œì§€ ë‹¤ë£¨ì—ˆë˜ Tabular Reinforcement learningì˜ ë‹¨ì  ë•Œë¬¸ì— function approximatorë¥¼ ë„ì…í•˜ê²Œ ë˜ì—ˆê³  value functionì„ wë¼ëŠ” parameterë¥¼ í†µí•´ì„œ approximateí•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ ì´ì œ í•™ìŠµì´ë¼ëŠ” ê²ƒì€ ì´ parameterë¥¼ updateí•˜ëŠ” ê²ƒì´ë¼ê³  í–ˆì—ˆìŠµë‹ˆë‹¤.

  ê·¸ë ‡ë‹¤ë©´ parameterë¥¼ updateí•˜ëŠ” ê²ƒì€ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì„ê¹Œìš”? ì´ì „ì— machine learningì— ëŒ€í•´ì„œ ì ‘í•´ë³¸ ë¶„ì´ë©´ ì˜ ì•„ëŠ” Stochastic Gradient Descentë°©ë²•ì„ í™œìš©í•˜ì—¬ value functionì˜ parameterë¥¼ updateí•˜ê²Œ ë©ë‹ˆë‹¤.

  ì´ ë°©ë²•ì€ ê°„ë‹¨í•˜ë©´ì„œë„ ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì— í”„ë¡œê·¸ë¨ ìƒìœ¼ë¡œ ê°•ë ¥í•œ updateë°©ë²•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹œì‘ì ì´ ì¡°ê¸ˆë§Œ ë‹¬ë¼ì ¸ë„ ë„ì°©í•˜ëŠ” local optimumì´ ë°”ë€ŒëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ë„ì°©í•œ ê·¹ì ì´ global optimumì´ ì•„ë‹ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

  ì´ëŸ¬í•œ ë‹¨ì ì„ ê·¹ë³µí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ì²˜ìŒ parameterë¥¼ updateí•˜ëŠ” ê²ƒì„ ë°°ìš°ëŠ” ì…ì¥ì—ì„œëŠ” ê°„ë‹¨í•œ ê°œë…ì„ ì•Œê³  ë‚˜ì¤‘ì— í™œìšœí•  ë•Œ ê·¸ëŸ¬í•œ ê¸°ë²•ë“¤ì„ ë„ì…í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

  J(w)ë¡œ í‘œí˜„ë˜ëŠ” objective í•¨ìˆ˜ëŠ” ë‚´ê°€ ì›í•˜ëŠ” ëŒ€ìƒê³¼ ìì‹ ì˜ errorë¡œ ë³´í†µ ì„¤ì •í•˜ì—¬ ê·¸ errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. updateë¥¼ í•˜ë ¤ë©´ ì–´ëŠë°©í–¥ìœ¼ë¡œ ê°€ì•¼ ê·¸ errorê°€ ì¤„ì–´ë“œëŠ”ì§€ ì•Œì•„ì•¼ í•˜ëŠ”ë° ê·¸ê²ƒì„ í•¨ìˆ˜ì˜ ë¯¸ë¶„(gradient)ì„ ì·¨í•´ì„œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. gradientìì²´ëŠ” ê²½ì‚¬ì´ê¸° ë•Œë¬¸ì— ê³¡ë©´ì—ì„œ ë³´ìë©´ ìœ„ë¡œ ì˜¬ë¼ê°€ëŠ” ë°©í–¥ì´ë¯€ë¡œ -ë¥¼ ê³±í•´ì„œ ê·¸ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë‚´ë ¤ê°ìœ¼ë¡œì„œ(descent) ì¡°ê¸ˆì”© errorë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤.

    > Gradient Descent
      Let J(w) be a differentiable function of parameter vector w
      Define the gardient of J(w) to be
        âˆ‡w J(w) = (ğœ•J(w)/ğœ•w1, ğœ•J(w)/ğœ•w2, ... , ğœ•J(w)/ğœ•wn)'

      To find a local minumum of J(w)
      Adjust w in direction of -ve gradient
        âˆ†w = -(1/2) *


















  ã…ã„´ã…‡ã„¹
