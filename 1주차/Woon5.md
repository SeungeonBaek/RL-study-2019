/*ì´ì›…ì›ë‹˜ Git*/

# Dynamic programming

  - 5~7ì¥ì€ Dyanmic programming, Monte Carlo methods, Temporal-difference methodsì— ëŒ€í•´ì„œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

  ìœ„ì˜ ì„¸ê°€ì§€ ë°©ë²•ì€ ê°ê° ì¥ì ê³¼ ë‹¨ì ì„ ë¶„ëª…íˆ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

  Dynamic programmingì˜ ê²½ìš°, ìˆ˜í•™ì ìœ¼ë¡œ ì˜ ì„¤ê³„ëœ ëª¨ë¸ì´ê¸´ í•˜ì§€ë§Œ, ì™„ì „í•˜ê³  ì •í™•í•œ ëª¨ë¸ì´ í•„ìˆ˜ì ì´ë¼ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

  Monte Carlo methodsì˜ ê²½ìš° simpleí•œ ì»¨ì…‰ì„ ì§€ë‹ˆê³  ìˆê³ , ëª¨ë¸ì„ í•„ìš”ë¡œ í•˜ì§€ëŠ” ì•Šì§€ë§Œ, step-by-step í•™ìŠµì—ëŠ” ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

  Temporal-difference methodsì˜ ê²½ìš°, modelì´ í•„ìš”í•˜ì§€ ì•Šê³ , fully incrementalí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ë³µì¡í•˜ê³  ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.

  ê°ê°ì˜ ë°©ë²•ë“¤ì€ íš¨ìœ¨ê³¼ ìˆ˜ë ´ì†ë„ ì¸¡ë©´ì—ì„œ ì—¬ëŸ¬ê°€ì§€ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.

  ë˜í•œ Dynamic programmingì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    > The term dynamic programming (DP) refers to a collection of algorithm that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP)


1.Policy iteration

  - Planning vs Learning

  Planningê³¼ Learningì˜ ì°¨ì´ë¥¼ ë¨¼ì € ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
  Planningì´ë€ environmentì˜ modelì„ ì•Œê³ ì„œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì´ê³ , Learningì´ë€ environmentì˜ modelì„ ëª¨ë¥´ì§€ë§Œ ìƒí˜¸ì‘ìš©ì„ í†µí•´ì„œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.

  Dynamic programmingì€ planningìœ¼ë¡œì„œ Environmentì˜ model(reward, state transition matrix)ì— ëŒ€í•´ì„œ ì•ˆë‹¤ëŠ” ì „ì œì—ì„œ, Bellman equationì„ í†µí•´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì„ ë§í•©ë‹ˆë‹¤.


    > Two fundamental problems in sequential decision making
      - Reinforcement Learning
        The environment is initially unknown
        The agent interacts with the environment
        The agent improves its policy
      - Planning
        A model of the environment is known
        The agent performs computations with its model (without any external interaction)
        The agent improves its policy
        a.k.a deliberation, reasoning, introspection, pondering, thought, search

  - Prediction & control

  Dynamic programmingì€ ë‹¤ìŒì˜ ë‘ step (1) Prediction (2) Control ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

  Predictionì´ë€ í˜„ì¬ optimal í•˜ì§€ ì•Šì€ ì–´ë–¤ policyì— ëŒ€í•´ì„œ value functionì„ êµ¬í•˜ëŠ” ê³¼ì •ì´ë©°, í˜„ì¬ì˜ value functionì„ í† ëŒ€ë¡œ ë” ë‚˜ì€ policyë¥¼ êµ¬í•˜ê³  ì´ì™€ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ optimal policyë¥¼ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  - Policy evaluation

    > Problem : evaluate a given policy ğœ‹
      - Solution : iterative application of Bellman expectation backup
      - v_1 -> v_2 -> ... -> v_ğœ‹
      - Using synchronous backup,
        At each iteration k+1
        For all states
        Update v_k+1(s) from v_k(s')
        where s' is a successor state of s

  Policy evaluationì€ perediction ë¬¸ì œë¥¼



***
