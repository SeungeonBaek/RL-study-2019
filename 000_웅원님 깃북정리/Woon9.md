ì¶œì²˜ : https://dnddnjs.gitbooks.io/rl/content
/*ì´ì›…ì›ë‹˜ Git*/

# Value function Approximation

## 1. Tabular Methods

  ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ê°•í™”í•™ìŠµì€ action value funcitonì„ Tableë¡œ ë§Œë“¤ì–´ì„œ í‘¸ëŠ” Tabular Methodsì…ë‹ˆë‹¤. ì´ì— ëŒ€í•´ì„œ Suttonêµìˆ˜ë‹˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ë§í•©ë‹ˆë‹¤. ì¦‰, í˜„ì¬ì˜ ë°©ë²•ì€ stateë‚˜ actionì´ ë§ì§€ ì•Šì„ ê²½ìš°ì—ë§Œ ì ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ Tableì´ ì ì  ë” ì»¤ì§€ë©´ ì´ ê°’ë“¤ì„ ë‹¤ ê¸°ì–µí•  ë©”ëª¨ë¦¬ë„ ë¬¸ì œì§€ë§Œ í•™ìŠµì— ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì•ì—ì„œ ë‹¤ë¤˜ë˜ ì˜ˆì œë“¤ë„ ë‹¤ gridworldê°™ì´ ì‘ì€ ì˜ˆì œì˜€ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    > We have so far assumed that our esimates of value functions are represented as a table with one entry for each
    state of for each state,action pair.
    This is particularly clear and instructive case, but of course it is limited to tasks with small numbers of
    states and actions.
    The problem is not just the memory needed for large tables, but the time and data needed to fill them accurately.
    In other words, the key issue is that of generalization!

  ê·¸ëŸ¬ë‚˜ í˜„ì‹¤ì€ gridworldê°™ì´ ì‘ì€ ì˜ˆì œê°€ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— í˜„ì¬ì˜ ë°©ì‹ì€ ì‹¤ìš©ì ì´ì§€ ëª»í•©ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ë©´, ë°‘ì˜ ë¬¸ì œë“¤ì„ ê°•í™”í•™ìŠµì€ í’€ì§€ ëª»í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— "Generalization"ì´ ê°€ëŠ¥í•´ì§€ê¸° ìœ„í•´ì„œëŠ” ìƒˆë¡œìš´ ideaê°€ í•„ìš”í•©ë‹ˆë‹¤.

  íŠ¹íˆë‚˜ ê°•í™”í•™ìŠµì„ ì‹¤ì œ ì„¸ìƒì— ì ìš©ì‹œí‚¤ê³  ì‹¶ë‹¤ê³  í•  ê²½ìš°, ì‹¤ì œ ì„¸ìƒì€ continuous state spaceì´ë¯€ë¡œ ì‚¬ì‹¤ìƒ stateê°€ ë¬´í•œëŒ€ì´ê¸° ë•Œë¬¸ì— ìƒˆë¡œìš´ ë°©ë²•ì´ ì—†ë‹¤ë©´ ë¡œë´‡ì´ ê°•í™”í•™ìŠµìœ¼ë¡œ í•™ìŠµí•˜ê¸°ëŠ” ì‰½ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.

    > Large-Scale Reinforcement Learning
      Reinforcement learning can be used to solve large problems, e.g.
        Backgammon  : 10^20  states
        Computer GO : 10^170 states
        Helicopter  : continuous state space

      How can we scale up the model-free method for prediction and control from the last two lectures?

***

## 2. Parameterizing value function

  ì´ì— ëŒ€í•œ í•´ë‹µì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. tableë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  wë¼ëŠ” ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ value functionì„ í•¨ìˆ˜í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  ì‰½ê²Œ ë§í•´ í•¨ìˆ˜ì˜ inputìœ¼ë¡œ stateê°€ ë“¤ì–´ê°€ë©´, action value functionì´ outputìœ¼ë¡œ ë‚˜ì˜¤ê³ , ì´ë•Œ í•¨ìˆ˜ì˜ parameterê°€ wì¸ ê²ƒì…ë‹ˆë‹¤.

  ì´ì œëŠ” í•™ìŠµì„ í†µí•´ì„œ Q functionì„ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ , wë¼ëŠ” parameterë¥¼ ì—…ë°ì´íŠ¸ í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ functdion approximation ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

    > We consider differentiable function approximators, e.g.
      Linear combinations of features
      Neural network
      Decision tree
      Nearst neighbour
      Fourier / wavelet bases

  ì´ ì¤‘ì—ì„œ ì €í¬ëŠ” ìœ„ì˜ ë‘ ê°€ì§€ë¥¼ ë³¼í…ë° (1) Linear combinations of features (2) Neural networkë¥¼ ë³¼ ê²ƒì…ë‹ˆë‹¤. (2)ëŠ” íŠ¹íˆ ìµœê·¼ì— ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ê°ê´‘ë°›ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ìµœê·¼ì˜ ê°•í™”í•™ìŠµì€ ë‹¤ ë”¥ëŸ¬ë‹ì„ approximatorë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë³´í†µ Deep Reinforcement Learning (DRL)ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

***

# Stochastic Gradient Descent

## 1. Gradient Descent

  ì´ì „ê¹Œì§€ ë‹¤ë£¨ì—ˆë˜ Tabular Reinforcement learningì˜ ë‹¨ì  ë•Œë¬¸ì— function approximatorë¥¼ ë„ì…í•˜ê²Œ ë˜ì—ˆê³  value functionì„ wë¼ëŠ” parameterë¥¼ í†µí•´ì„œ approximateí•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ ì´ì œ í•™ìŠµì´ë¼ëŠ” ê²ƒì€ ì´ parameterë¥¼ updateí•˜ëŠ” ê²ƒì´ë¼ê³  í–ˆì—ˆìŠµë‹ˆë‹¤.

  ê·¸ë ‡ë‹¤ë©´ parameterë¥¼ updateí•˜ëŠ” ê²ƒì€ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì„ê¹Œìš”? ì´ì „ì— machine learningì— ëŒ€í•´ì„œ ì ‘í•´ë³¸ ë¶„ì´ë©´ ì˜ ì•„ëŠ” Stochastic Gradient Descentë°©ë²•ì„ í™œìš©í•˜ì—¬ value functionì˜ parameterë¥¼ updateí•˜ê²Œ ë©ë‹ˆë‹¤.

  ì´ ë°©ë²•ì€ ê°„ë‹¨í•˜ë©´ì„œë„ ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì— í”„ë¡œê·¸ë¨ ìƒìœ¼ë¡œ ê°•ë ¥í•œ updateë°©ë²•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹œì‘ì ì´ ì¡°ê¸ˆë§Œ ë‹¬ë¼ì ¸ë„ ë„ì°©í•˜ëŠ” local optimumì´ ë°”ë€ŒëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ë„ì°©í•œ ê·¹ì ì´ global optimumì´ ì•„ë‹ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

  ì´ëŸ¬í•œ ë‹¨ì ì„ ê·¹ë³µí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ì²˜ìŒ parameterë¥¼ updateí•˜ëŠ” ê²ƒì„ ë°°ìš°ëŠ” ì…ì¥ì—ì„œëŠ” ê°„ë‹¨í•œ ê°œë…ì„ ì•Œê³  ë‚˜ì¤‘ì— í™œìš©í•  ë•Œ ê·¸ëŸ¬í•œ ê¸°ë²•ë“¤ì„ ë„ì…í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

  J(w)ë¡œ í‘œí˜„ë˜ëŠ” objective í•¨ìˆ˜ëŠ” ë‚´ê°€ ì›í•˜ëŠ” ëŒ€ìƒê³¼ ìì‹ ì˜ errorë¡œ ë³´í†µ ì„¤ì •í•˜ì—¬ ê·¸ errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. updateë¥¼ í•˜ë ¤ë©´ ì–´ëŠë°©í–¥ìœ¼ë¡œ ê°€ì•¼ ê·¸ errorê°€ ì¤„ì–´ë“œëŠ”ì§€ ì•Œì•„ì•¼ í•˜ëŠ”ë° ê·¸ê²ƒì„ í•¨ìˆ˜ì˜ ë¯¸ë¶„(gradient)ì„ ì·¨í•´ì„œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. gradientìì²´ëŠ” ê²½ì‚¬ì´ê¸° ë•Œë¬¸ì— ê³¡ë©´ì—ì„œ ë³´ìë©´ ìœ„ë¡œ ì˜¬ë¼ê°€ëŠ” ë°©í–¥ì´ë¯€ë¡œ -ë¥¼ ê³±í•´ì„œ ê·¸ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë‚´ë ¤ê°ìœ¼ë¡œì„œ(descent) ì¡°ê¸ˆì”© errorë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤.

    > Gradient Descent
      Let J(w) be a differentiable function of parameter vector w
      Define the gardient of J(w) to be
        âˆ‡w J(w) = (ğœ•J(w)/ğœ•w1, ğœ•J(w)/ğœ•w2, ... , ğœ•J(w)/ğœ•wn)'

      To find a local minumum of J(w)
      Adjust w in direction of -ve gradient
        âˆ†w = -(1/2) * ğ›¼ * âˆ‡w J(w)

***

## 2. Gradient Descent on RL

  Gradientì˜ ê°œë…ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ ê°œë…ì„ ê°•í™”í•™ìŠµì— ì ìš©ì‹œì¼œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œëŠ” J(w)ë¥¼ ì–´ë–»ê²Œ ì •ì˜í• ê¹Œìš”? ë°”ë¡œ true value functionê³¼ approximate value vhat(s,w)ì™€ì˜ errorë¡œ ì¡ìŠµë‹ˆë‹¤.

    > Value Function Approximation by Stochastic Gradient Descent
      Goal : find parameter vector w minimising mean-squared error between approximate value function vhat(s,w)
      and true value function v_ğœ‹(s)
        J(w) = E_ğœ‹[{v_ğœ‹(s)-vhat(s,w)}^2]

      Gradient descent finds a local minimum
        âˆ†w = -(1/2) * ğ›¼ * âˆ‡w J(w)
           = ğ›¼ * E_ğœ‹[{v_ğœ‹(s)-vhat(s,w)} * âˆ‡w vhat(s,w)]

      Stochastic gardient descent samples the gradient
        âˆ†w = ğ›¼ * {v_ğœ‹(s)-vhat(s,w)} * âˆ‡w vhat(s,w)

      Expedted update is equal to full gradient update

  Gradient Descentë°©ë²•ë„ (1) Stochastic Gradient Descent(SGD)ì™€ (2) Batch ë°©ë²•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ”ë° ìœ„ì™€ ê°™ì´ ëª¨ë“  stateì—ì„œ true value functionê³¼ì˜ errorë¥¼ í•œ ë²ˆì— í•¨ìˆ˜ë¡œ ì¡ì•„ì„œ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì€ Batchì˜ ë°©ì‹ì„ í™œìš©í•œ ê²ƒìœ¼ë¡œì„œ step by stepìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  í•œ ë²ˆì— ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

  Mean-Squared errorë¥¼ gradientë°©ì‹ì— ì§‘ì–´ë„£ì–´ì„œ gradientë¥¼ ì·¨í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    > Gradient descent finds a local minimum
        âˆ†w = -(1/2) * ğ›¼ * âˆ‡w J(w)
           = ğ›¼ * E_ğœ‹[{v_ğœ‹(s)-vhat(s,w)} * âˆ‡w vhat(s,w)]

  í•˜ì§€ë§Œ DPì—ì„œ ê°•í™”í•™ìŠµìœ¼ë¡œ ë„˜ì–´ê°ˆ ë•Œ ì²˜ëŸ¼ expectationì„ ì—†ì• ê³  samplingìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ ì•„ë˜ì™€ ê°™ì•„ì§‘ë‹ˆë‹¤.

    > Stochastic gradient descent samples the gradient
      âˆ†w = ğ›¼ * {v_ğœ‹(s)-vhat(s,w)} * âˆ‡w vhat(s,w)

  ì´ì „ì— MCì™€ TD Learningì—ì„œ í–ˆë“¯ì´ True value function ë¶€ë¶„ì„ ì—¬ëŸ¬ê°€ì§€ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Sample returnì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆê³  TD targetì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

    > Have assumed true value function v_ğœ‹(s) given by superviser
      But in RL there is no supervisor, only rewards
      In practice, we substitute a target for v_ğœ‹(s)
        For Mc, the target is the return G_t
          âˆ†w = ğ›¼ * {G_t-vhat(s,w)} * âˆ‡w vhat(s,w)

        For TD(0), the target is the TD target R_(t+1) + ğ›¾ * vhat(s,w)
          âˆ†w = ğ›¼ * {R_(t+1) + ğ›¾ * vhat(s,w) - vhat(s,w)} * âˆ‡w vhat(s,w)
        For TD(ğœ†), the target is the TD target R_(t+1) + ğ›¾ * vhat(s,w)
          âˆ†w = ğ›¼ * {G^ğœ†_t - vhat(s,w)} * âˆ‡w vhat(s,w)

***

# Learning with Function Approximator

## 1. Action-value function approximation

  ì•ì—ì„œëŠ” state-value functionì„ ì‚¬ìš©í–ˆì§€ë§Œ model-freeê°€ ë˜ê¸° ìœ„í•´ì„œëŠ” action-value functionì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì„ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ìë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

  policy evaluationì€ parameterì˜ updateë¡œ ì§„í–‰í•˜ë©° policy improvementëŠ” ê·¸ë ‡ê²Œ updateëœ action value functionì— Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ í•œ actionì„ ì·¨í•¨ìœ¼ë¡œì¨ improveê°€ ë©ë‹ˆë‹¤.

    > Control with Value Function Approximation
      Policy evalutation : Approximate policy evaluation, qhat(âˆ™,âˆ™,w) â‰ˆ q_ğœ‹
      Policy improvement : Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ policy improvement

  ì•ì—ì„œ value functionìœ¼ë¡œ í–ˆë˜ ë‚´ìš©ì„ ë°˜ë³µí•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

    > Action-Value Function Approximation
      Approximate the action-value function
        qhat(S,A,w) â‰ˆ q_ğœ‹(S,A)

      Minimise mean-squared error between approximate action-value function qhat(S,A,w)
      and true action-value function q_ğœ‹(S,A)
        J(w) = E_ğœ‹[{q_ğœ‹(S,A) - qhat(S,A,w)}^2]

      Use stochastic gradient descent to find a local minimum
        -(1/2) * âˆ‡w J(w) =     {q_ğœ‹(S,A) - qhat(S,A,w)} * âˆ‡w qhat(S,A,w)
        âˆ†w               = ğ›¼ * {q_ğœ‹(S,A) - qhat(S,A,w)} * âˆ‡w qhat(S,A,w)

      True value functionì„ ëŒ€ì²´í•˜ëŠ” ê²ƒë„ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

      Like prediction, we muist substitute a target for q_ğœ‹(S,A)
        For MC, the target is the return G_t
          âˆ†w = ğ›¼ * {G_t - qhat(S_t, A_t, w)} * âˆ‡w qhat(S_t, A_t, w)

        For TD(0), the target is the TD target R(t+1) + ğ›¾ * Q(S_(t+1), A_(t+1))
          âˆ†w = ğ›¼ * {R_(t+1) + ğ›¾ * qhat(S_(t+1), A_(t+1), w) - qhat(S_t, A_t, w)} * âˆ‡w qhat(S_t, A_t, w)

        For forward-view TD(ğœ†), target is the action-value ğœ†-return
          âˆ†w = ğ›¼ * {q^ğœ†_t - qhat(S_t, A_t, w)} * âˆ‡w qhat(S_t, A_t, w)

        For backward-view TD(ğœ†), equivalent update is
          ğ›¿_t = R_(t+1) + ğ›¾ * qhat(S_(t+1), A_(t+1), w) - qhat(S_t, A_t, w)
          E_t = ğ›¾ * ğœ† * E_(t-1) + âˆ‡w qhat(S_t, A_t, w)
          âˆ†w  = ğ›¼ * ğ›¿_t * E_t

***

## 2. Batch Methods

  ì§€ê¸ˆê¹Œì§€ SGD(Stochastic Gradient Descent)ë¥¼ í†µí•´ì„œ parameterë¥¼ updateí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.

    > Batch Reinforcement Learning
      Gradient descent is simple and appealing
      But it is not sample efficient
      Batch methods seek to find the best fitting value function
      Given the agents's experience ("training data")

  SGDì²˜ëŸ¼ gradientë¥¼ ë”°ë¼ì„œ parameterë¥¼ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  training data(agentê°€ ê²½í—˜í•œ ê²ƒ)ë“¤ì„ ëª¨ì•„ì„œ í•œêº¼ë²ˆì— updateí•˜ëŠ” ê²ƒì´ "Batch Methods"ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ Batchë°©ë²•ì€ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ í•˜ëŠ” ë§Œí¼ ê·¸ ë§ì€ ë°ì´í„°ë“¤ì— ê°€ì¥ ì˜ ë§ëŠ” value functionì„ ì°¾ê¸°ê°€ ì–´ë µê¸° ë•Œë¬¸ì— SGDì™€ Batchë°©ë²•ì˜ ì¤‘ê°„ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤.

  ì˜ˆë¥¼ ë“¤ë©´, step-by-stepìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  100ê°œì˜ ë°ì´í„°ê°€ ëª¨ì¼ ë–„ê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ 100ë²ˆì— í•œ ë²ˆì”© ì—…ë°ì´íŠ¸í•˜ëŠ” "mini-batch"ë°©ë²•ë„ ìˆìŠµë‹ˆë‹¤.

  ìœ„ì—ì„œ ë§í•˜ëŠ” SGDì˜ ë¬¸ì œì ì¸ experience dataë¥¼ í•œ ë²ˆë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë¹„ íš¨ìœ¨ì ì´ë‹¤ë¼ê³  ë§í•˜ëŠ” ì ì— ëŒ€í•´ì„œëŠ” í•œ ë²ˆë§Œ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì—¬ëŸ¬ë²ˆ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ experience dataë¥¼ ì—¬ëŸ¬ë²ˆ í™œìš©í•  ê²ƒì¸ê°€ì— ëŒ€í•´ì„œ experience replayê°€ ê·¸ ë‹µì„ ë§í•´ì¤ë‹ˆë‹¤.

***

## 3. Experience Replay

  Experience ReplayëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ë’¤ì—ì„œ ì„¤ëª…í•˜ê² ì§€ë§Œ Deepmindì—ì„œ Atrai Gameì— ì‚¬ìš©í–ˆë˜ ì•Œê³ ë¦¬ì¦˜ì´ê³  ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. replay memoryë¼ëŠ” ê²ƒì„ ë§Œë“¤ì–´ ë†“ê³ ì„œ agentê°€ ê²½í—˜í–ˆë˜ ê²ƒë“¤ì„ (S_t, A_t, R_(t+1), S_(t+1))ë¡œ time-stepë§ˆë‹¤ ëŠì–´ì„œ ì €ì¥í•´ ë†“ìŠµë‹ˆë‹¤.

  ê·¸ í›„, action-value funtionì˜ parameterë¥¼ updateí•˜ëŠ” ê²ƒì€ time-stepë§ˆë‹¤ í•˜ì§€ë§Œ í•˜ë‚˜ì˜ transitionì— ëŒ€í•´ì„œë§Œ í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë¼ ëª¨ì•„ë†“ì•˜ë˜ transitionì„ replay memoryì—ì„œ 100ê°œ í˜¹ì€ 200ê°œì”© êº¼ë‚´ì„œ(mini-batch) ê·¸ mini-batchì— ëŒ€í•´ updateë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

    > Experience Replay in Deep Q-Networks (DQN)
      DQN uses experience replay and fixed Q-targets
      - Take action a_t according to Ïµâˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ policy
      - Store transition (s_t, a_t, r_(t+1), s_(t+1)) in replay memory D
      - Sample random mini-batch of transitions (s,a,r,s') from D
      - Compute Q-learning targets w.r.t. old, fixed parameter w-
      - Optimise MSE between Q-network and Q-learning targets
          L_i(w_i) = E_(s,a,r,s'~ D_i) [(r + ğ›¾ * {a'}max( Q(s', a'; w-_i) - Q(s, a; w_i) ))^2]

      - Using variant of stochastic gradient descent

  ì´ë ‡ê²Œ í•  ê²½ìš°ì— sample efficientí•  ìˆ˜ë„ ìˆì§€ë§Œ ë˜í•œ episodeë‚´ì—ì„œ step-by-stepìœ¼ë¡œ updateë¥¼ í•˜ë©´ ê·¸ ë°ì´í„°ë“¤ ì‚¬ì´ì˜ correlation ë•Œë¬¸ì— í•™ìŠµì´ ì˜ ì•ˆë˜ëŠ” ë¬¸ì œë„ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
